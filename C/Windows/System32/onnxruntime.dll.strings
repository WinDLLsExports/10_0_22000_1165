      
            
              
                    [
                    [0.0, 0.0, 1.0, 1.2],
                    [0.0, 0.0, 2.3, 3.4],
                    [0.0, 0.0, 4.5, 5.7],
                    [1.0, 1.2],
                    [2.3, 3.4],
                    [4.5, 5.7],
                    ]
                    ],
                input_x = [2, 1, 1, 3, 4, 3]
                output_counts = [1, 2, 2, 1]
                output_idx = [0, 1, 1, 2, 3, 2]
                output_uniques = [2, 1, 3, 4]
              Example:
              Finds all the unique values (deduped list) present in the given input tensor.
              of each value of the input in 'uniques'.
              sorted in the same order that they occur in the input.
              The first output tensor 'uniques' contains all of the unique elements of the input,
              The second output tensor 'idx' is the same size as the input and it contains the index
              The third output tensor 'counts' contains the count of each element of 'uniques' in the input.
              This operator returns 3 outputs.
            data = [
            Example:
            Given `data` tensor, pads, mode, and value.
            Insert 0 pads to the beginning of the second dimension.
            output = [
            pads = [0, 2, 0, 0]
        (possibly with aspect ratio change) to a common output size specified by crop_height and crop_width.
        <requestedExecutionLevel level='asInvoker' uiAccess='false' />
        a fixed size = [crop_height, crop_width]. The result is a 4-D tensor [num_boxes, crop_height, crop_width, depth].
        Extracts crops from the input image tensor and resizes them using bilinear sampling or nearest neighbor sampling
        Returns a tensor with crops from the input image at positions defined at the bounding box locations in boxes.
        The cropped boxes are all resized (with bilinear or nearest neighbor interpolation) to
        The resizing is corner aligned.
      [1, 2, 3, 4],
      [2, 3, 4],
      [5, 6, 7, 8],
      [5, 6, 7],
      </requestedPrivileges>
      <requestedPrivileges>
      All other elements in the matrix are set to zero.
      If k = 0, the triangular part on and above/below the main diagonal is retained.
      If upper is set to false, a positive k retains the lower triangular matrix including k diagonals above
      If upper is set to true, a positive k retains the upper triangular matrix excluding k diagonals above
      of the elements on and above the given diagonal (k). The lower triangular part consists of elements on and below the diagonal.
      Returns the upper or lower triangular part of a 2-D matrix, or batches of 2-D matrices. If the attribute "upper" is set to true,
      the main diagonal. A negative k value excludes as many diagonals below the main diagonal.
      the main diagonal. A negative k value includes as many diagonals below the main diagonal.
      the upper triangular matrix is retained. Lower triangular matrix is retained otherwise. Default value for upper is true.
      Trilu takes one input tensor of shape [*, N, M], where * is zero or more batch dimensions. The upper triangular part consists
    </security>
    <security>
   ' 0 8 ; > A C G Q S S U ^ 
  !~09AZ__az ~09az09
  "(&*(Y
  "separators" is a list of strings which are regular expressions. "tokenexp" is a single regular expression.
  ("*$
  ["Hello World", "I love computer science !"] whose shape is [2],
  </trustInfo>
  <trustInfo xmlns="urn:schemas-microsoft-com:asm.v3">
 ' 0 C E Q S ^ } ~ 
  09AFafAZAZaz
  09AZaz
  axes = [0, 1]
  data    = [[[0,1],[2,3]],[[4,5],[6,7]]]
  data    = [[0,1],[2,3]]
  data = [
  ends = [-1, 1000]
  ends = [2, 3]
  If input is
  If input is ["Hello", "World"],
  If the maximum number of tokens found per input string is D, the output shape would be [N, C, D] when input shape is [N, C].
  indices = [[[0,1]],[[1,0]]]
  indices = [[0,0],[1,1]]
  indices = [[0,1],[1,0]]
  indices = [[1],[0]]
  Let's assume "separators" is [" "] and consider an example.
  Let's consider another example to illustrate the effect of setting "mark" to true.
  output  = [[[2,3]],[[4,5]]]
  output  = [[2,3],[0,1]]
  output  = [[2,3],[4,5]]
  output  = [0,3]
  result = [
  shape(A) = (2, 3, 4, 5), shape(B) = (,), i.e. B is a scalar tensor
  shape(A) = (2, 3, 4, 5), shape(B) = (1, 1), i.e. B is an 1-element tensor
  shape(A) = (2, 3, 4, 5), shape(B) = (2), with axis=0
  shape(A) = (2, 3, 4, 5), shape(B) = (3, 4), with axis=1
  shape(A) = (2, 3, 4, 5), shape(B) = (4, 5)
  shape(A) = (2, 3, 4, 5), shape(B) = (5,)
  Similarly, if input shape is [C] then the output should be [C, D]. Tokenizer has two different operation modes.
  starts = [0, 1]
  starts = [1, 0]
  The first mode is selected when "tokenexp" is not set and "separators" is set. If "tokenexp" is set and "separators" is not set,
  the second mode will be used. The first mode breaks each input string into tokens by matching and removing separators.
  then the corresponding output would be [0x02, "Hello", "World", 0x03].
  then the output would be
  This implies that if mark is true, [C]/[N, C] - input's output shape becomes [C, D+2]/[N, C, D+2].
  Tokenizer divides each string in X into a vector of strings along the last axis. Allowed input shapes are [C] and [N, C].
 != mat_h:
 """$f
 """$r
 "$2&.$`&.$: 
 ( ) / / _ _ 
 (0>T
 (08@H%
 (08@HPX`hpx
 (08@HPX`jL>
 (08@HRL>
 (domain: 
 (falsenode).
 (node 
 (node_version: 
 (truenode).
 (weights).
 ) is different from what is supplied (
 * . ` d f o 
 *!+!2!2!N!N!`!
 *0-0
 ,"D$
 / / _ _ 
 : : 
 :L$T
 ;t$`
 @33{@33{@33{@33{@
 ["I", "love", "computer", "science", "!"]]
 [["Hello", "World", padvalue, padvalue, padvalue],
 [seqno=
 [truncated]
 \"H$B&b(<*b,V", 8
 |$(&
 |,},o-o-/./.
 0"b$`&`$H(
 9 9 
 A^_^
 A^_^][
 A^A\_^]
 A^A]A\_^
 A^A]A\_^[]
 A_A^_
 A_A^_^[
 A_A^_^]
 A_A^A\
 A_A^A\_^
 A_A^A\_^][
 A_A^A]
 A_A^A]A\]
 A_A^A]A\^][
 A_A^A]A\_
 A_A^A]A\_^]
 already exist.
 and 
 and function is trying to import opset version 
 and the number of 
 and then launches another search starting from the first remained character after the first matched token.
 and type (
' appeared multiple times.
 appears in graph inputs and will not be treated as constant value/weight. 
 are '0' and '1'. 
 are '0' and '1'. The environment variable contained the value: 
 arena_extend_strategy 
 arg 
 as it still has output edges.
 at line 
 at pos=
 attribtues in LabelEncoder 
' attribute.
 Attribute:
 Axis is 
 axis value 
 Axis=
 BackUp() can only be called after Next().
 Base Class Array'
 Base Class Descriptor at (
 bins of max chunk size 
 but 
 but different TensorProto.
 but expected 
 but has 
 but has rank 
 but input '
 but is of type: 
 but ngram_indexes size: 
 but subgraphs produce 
 but the actually size is: 
 but the node in the model has the following type (
 but usage of initializer in graph expects 
 bytes for 
 bytes were able to be read.
 bytes.
 cannot be safely updated to 
 Can't back up over more bytes than were returned by the last call to Next().
 Char embedding size: 
 char_embedding_size attribute: 
 Class Hierarchy Descriptor'
 Complete Object Locator'
 conv filter size: 
 conv kernal size 1: 
 Conv kernal size 2 : 
 conv_window_size attribute: 
 d f p t ~ 
 delete
 delete[]
 DeviceId:
 did not match batch size of 
 did not return correct number of compiled functions
 did not.
' dimension 
 dimension != 
 Dimension=
 dimensions or more but input had shape of 
 dimensions.
 does not align with rank of input data: 
 does not contain a graph.
 does not match existing output type of 
 does not match the equation indices.
 does not match. 
 does not specify a valid type.
 does not.
' doesn't support memcpy 
 E E } } 
 elements.
 else=
 embedding_size attribute: 
 Encountered following errors: (
 entries which doesn't match the number of fetches the frame was initialized with of 
 exceeded maximum protobuf size of 2GB: 
 Expected 
 Expected DENSE or SPARSE
 Expected std::map<int64_t, float> or std::map<int64_t, std::string>
 expected to be a registered ONNX type
 expected to be of type: 
 expected to have rank 
 expected to have rank >
 expected to have sequence type
 expected to have tensor type
 expected to have type but instead is null
 Expected TO_FLOAT, TO_STRING or TO_INT64
 Expected:
 Expected: 
 ExplicitInputs:
 fail, errcode = 
 fail: unexpected end
 failed
' failed
 failed.
 failed. Error:
 failed. File doesn't exist
 failed. Only 
 failed:
 failed: 
 filter_number: 
 for attribute 
 for domain 
 For each input string, the second mode searches matches of "tokenexp" and each match will be a token in Y.
 for input shape 
 for operator 
 for SizeFromDimension. Tensor has 
 for the following indices
 for the same domain
 found!
 Found:
 Got:
 Got: 
 got: 
 Graph may not conform to the ONNX spec and contain initializers that are not graph inputs.
 group: 
 H"( 
 H;s(u
 H;T$@
 H;W 
 H3E H3E
 has already been loaded.
 has already been registered.
 has batch size of 
' has been deprecated since version 
' has been used as graph input names multiple times.
' has been used as output names multiple times.
 has Compile error: 
' has element type 
 has length of 
 has mismatched dimensions of 
 has unknown expected type
 Hc|$p3
 Hct$p3
 Hiy @!
 However the types are incompatible.
 I;^pu
 I;p0|
 id: 
 If "separators" contains a single empty string, the Tokenizer will enter into character tokenezation mode. This means all strings
 If no match found, this operator will remove the first character from the remained string and do another search.
 Implicit input name 
 ImplicitInputs:
 imports opset version 
 in AddToThreadq
 in ComputeFirstByte
 in 'Constant' node '
' in custom op '
 in initializer but not in graph input
 in KernelRegistryManager
 in node 
 in node (
 in one of the subgraphs.
 in step
 in the same dimension
 in the supported version range
 in tree 
 Index:
 index: 
 inferred output shape:
 inferred=
 initializer name is not unique
 Input dim value: 
 input dimensions instead
 Input shape=
 input with name 
 Input=
 inputs and requires 
 inputs but 
 inputs but Scan was only given 
 inputs but subgraph has 
 inputs. Either provide all subgraph inputs, or just the required inputs.
 inputs. Found:
' instead of '
 into softmax(input + bias)
 is already there.
 is defined.
 is deprecated in domain_version of 
' is expected to have field 'floats'
' is expected to have field 'g'
' is expected to have field 'graphs'
' is expected to have field 'ints'
' is expected to have field 'sparse_tensor'
' is expected to have field 'strings'
' is expected to have field 't'
' is expected to have field 'tensors'
 is expected to have type: 
 is greater than input dim=
 is incompatible in the dimension 
 is invalid for a tensor of rank 
 is invalid.
 is marked single but has an empty string in the graph
 is missing type info.
' is missing.
 is missing. Invalid ORT format model.
 is NaN
 is not a registered function/op
 is not a valid date
 is not a valid day
 is not a valid year
 is not compatible with 
 is not currently registered or supported
 is not expected to be of type sparse tensor.
 is not expected to be of type tensor sequence.
 is not expected to be of type tensor.
 is not found
' is not found
 is not implemented
 is not in (0, 
 is not in valid range [-
 is not output of any previous nodes.
 is not supported
 is not supported currently
 is not supported in this function
 is not supported yet
 is not the same as this node's index:
 is not used by any node.
 is null. Invalid ORT format model.
 is null. Type info is expected.
 is out of bounds
 is outside range.
 is repeated.
 is required but missing.
 is required to be non-empty.
 is smaller than requested bytes of 
 is till opset 
 is under development and support for this is limited. The operator schemas and or other functionality could possibly change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain 
 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain 
 is unrecognized, acceptable values are TF,IDF,TFIDF
 is used by node 
 J":  $
 J"2  $5
 kernel channels: 
 kernel is not supported in 
 kernel not found in 
 kernel start version: 
 kernel_end_version: 
 kernel_shape: 
 known by the checker.
 l$(&
 Lc\$pE3
 Left shape override: 
 line 
 Max:
 max_dead_bytes_per_chunk: 
 memory limit: 
 MemoryType:
 message of type "
 Microsoft Corporation. All rights reserved.
' Model is invalid.
 model may run depending upon legacy support of some older opset version operators.
 model uses the deprecated attribute
 models with experimental operators: 
 must be 1 instead of 
 must be either specified in graph inputs or graph initializers.
 must be of equal size
 must be within the inclusive range [
 must have shape {
 N"X$
 new[]
 No matching hash for 
 node '
 node. Name:'
 node_version: 
 noexcept
' not found
 not found.
 not in allowed input sizes.
 not in allowed output sizes.
 not in range [min=
 not specified
 Note that the input at most can have two axes, so 3-D and higher dimension are not supported.
 Num entries in 'split' (must equal number of outputs) was 
 num_input_channels: 
 NumOutputs=
' of 
' of input parameter (
 of node 
' of node: 
 Operating System
' OpType:
 OrtAllocatorType:
 OrtMemType:
 Output dim value: 
 outputs but Scan expects 
 outputs so the subgraph requires 
 outputs which doesn't match the subgraph's 
 outputs.
 outputs. Expected 
 P!_!
 Parameter to BackUp() can't be negative.
 Please fix either the inputs or the model.
 Provider: [
 Requested shape:
 returned nullptr
 Right shape override: 
 Right shape: 
 rounded_bytes:
 row[
 rows[
 should be of integer type and specify a type.
' should be stored in field '
 should only appear once.
 should specify a shape
 size=
' source:
 source=
 sparse initializer name is not unique across initializers and sparse_initializers
 specified. It should be either avg or max
 specified. It should be either bilinear or nearest
' Status Message: 
 Sum of sizes in 'split' (must equal size of selected axis) was 
 sum of split values=
 t^D9
 t>I;
 target:
 target=
 Target=
 tDE3
 Tensor=
 The matching of "tokenexp" is conducted greedily (i.e., a match should be as long as possible).
' the model will use the latest encountered initializer
 The production MUST never overflow. The accumulation may overflow if and only if in 32 bits.
 then=
 This op has been implemented only for the following types (
 This operator searches for the first match starting from the beginning of the considered string,
 This procedure will be repeated until reaching the end of the considered string.
 Type Descriptor'
 unknown
 v.H+
 Value=
 values, but NNZ is 
 Version mismatch.
 version: 
 volatile
 vs. 
 was 
' was 
 was false.
 was not
 was not a tensor.
 was not found. Defaulting to a rank 1 shape of {0}.
 were provided
 where as the model imports opset version 
 whose shape is [2, 5] because you can find at most 5 tokens per input string.
 will be broken part into individual characters.
 Windows
 with domain_version of 
 with following configs: initial_chunk_size_bytes: 
!#!%!%!'!'!)!)!.!.!:!;!@!D!J!M!O!O!
!#!%!%!'!'!)!)!.!.!:!;!J!J!L!M!O!O!
!#%'**,,./:;?@\\
!#%*,/:;?@[]__{{}}
!$!$!&!&!(!(!*!-!/!9!<!?!E!I!N!N!
!$!$!&!&!(!(!*!-!0!3!>!?!E!E!
!%!'!)!,!1!3!M!O!_!
!&$@$J$`$
!(it.GetName().empty())
!/!/!4!4!9!9!<!=!F!I!N!N!
!/:@[`{~
!@!D!K!K!
!0,^,a,a,e,f,h,h,j,j,l,l,q,q,s,t,v,{,
!ba|H
!c->in_use() && (c->bin_num != kInvalidBinNum)
!c->in_use() && (c->bin_num == kInvalidBinNum)
!c1->in_use() && !c2->in_use()
!char_tokenezation_ || mincharnum_ < 2
!chunk->in_use()
!coefficients_.empty()
!current_parallel_section
!D$PD
!D$XL
!found
!graph.GetInitializedTensor(new_initializer.name(), existing)
!has_axes || attr_axes_.size() == attr_starts_.size()
!helper.HaveTwoTensorInputs()
!input_tensor.IsDataType<std::string>()
!is_concrete_shape_
!L$`L
!mask || mask->Shape() == X_shape
!normalize_
!pool_strings.empty()
!scale_.empty()
!separators.empty()
!sw.empty()
!T$(H!T$ 
!This program cannot be run in DOS mode.
!tokenexp.empty()
!using_counters_
!utils::HasExternalData(t_proto)
" #!#|#|#
" : "
" because it is missing required fields: 
"""""""
"@$("A
"`&@($
"<r>J<
"2n4$6r8P:
"8^<4>2FLHPT.V,F(H JPL.N,FHxP
"args" : {
"dur" :
"F$x"u
"IcO H
"j$J&^(j*b,Z0A
"L$4"
"N$2" &
"n6684<&>^@6B4
"name" :"
"ph" : "X",
"pid" :
"tid" :
"ts" :
"X$N(-
#"#(#+#{#}#
#&$@$J$
#(#+#&$@$J$
#)#)#h'h'j'j'l'l'n'n'p'p'r'r't't'
#)#*#h'u'
#*#*#i'i'k'k'm'm'o'o'q'q's's'u'u'
#L$8;
#L$8D;
#T$8D;
#wht^
$"&z(Z*
$$++<>^^``||~~
$$bAmX
$$bQ}X
$$M9A@
$;tNE3
$@bA~H
$@ba~H
$@bA~H
$@ba~H
$@bA~H
$@ba~H
$`(N*(,
$< t6<$t,<+t"<vt
$<"x&d(i
$b&H((&@$
$bQ}X
$bR}H|
$f(.*,
$H&((
$H&^*
$H*J,R.
$H;0u
$I;@8t}M
$I;H8tFH
$I+D$
$Iba|H
$IbB}H
$J(J,V0L4y
$L;T$
$M;J(
$Nba|H
$NF(H!
$p&"$
$P&f*B,^.D$8
$v&&(6&
-%-'-'-----
%.0Lf
%~3a*
-%-'-'-----0-g-o-o-
-%-'-'-----A
%b %d %H : %M : %S %Y
%d / %m / %y
%H : %M
%H : %M : %S
%hs!%p: 
%hs(%d) tid(%x) %08X %ws
%hs(%u)\%hs!%p: 
%I : %M : %S %p
%I64u
%m / %d / %y
%o&o&
%Y-%m-%d_%H-%M-%S
& (>&}
& 0"4 
&!&!e
&"P$.&,
&(4.f,)
&,(R*
&.(`"
&.(b"
&.(f"
&.(L*b, ..$
&^(h*d.@0d4>6`:d<~@^BFD
&`,..
&<( ,
&9x(t
&b(t*
&H((&
&H(r,R.
&n&p&g'
&N(2&
&R(R*B,P(D.L&2
&x(.*%
&X(b*r,d.b0z2r4b6N:
&Z">2-
(  "($e
( ( p
("*(,**5
(%<t8
(([[{{
((P4.6,
((T*.,,
(){}[]*+?|.^$\
(:D$Uu
(;Qh|
(?HaveMatch:%d)
(?-m:$)
(?-m:^)
(\$PD
(]PH+
(_^][
(|$ H
(|$ I
(|$ L
(|$@A
(|$@D
(|$@H
(|$`D
(|$`H
(|$`I
(|$0D
(|$0H
(|$0I
(|$PD
(|$pE
(|$PH
(|$pI
(|$PI
(|$pL
(08@HPXf
(08@HPXf"-M
(08@HPXf2-M
(08@HPXfb-M
(08@HPXfB-M
(08@HPXfr-M
(08@HPXfz*M
(2>4 8
(2D>FFH
(6c,M
(6K.M
(6R-M
(6s,M
(6S,M
(6W}K
(A^_^[
(A_A^_^[]
(A_A^_^][
(A_A^A]A\_^][
(caller: %p) 
(cannot determine missing fields for lite message)
(D$ B
(D$ D
(D$ f
(D$ H
(D$@A
(D$@f
(D$@H
(d$`D
(D$`D
(d$`D
(D$`D
(d$`D
(D$`D
(d$`D
(D$`f
(D$`fA
(d$`H
(D$`H
(D$`L
(D$0D
(D$0f
(d$0f
(D$0f
(D$0H
(D$PA
(D$PD
(D$Pf
(D$pf
(D$Pf
(D$pf
(D$Pf
(D$pf
(D$Pf
(D$pf
(D$Pf
(D$pfA
(D$pH
(D$PH
(D$pH
(D$pL
(default) or 
(F*8(
(F*8( ,
(float, default 0.5) the ratio of random dropout
(fmod == 0) || (fmod == 1)
(H*(.
(H2(4
(HhJ"HV
(inputs_.size() - 1) == i
(int, default 0) if nonzero, run dropout in test mode where the output is simply Y = X.
(items % ngram_size == 0)
(j*|.
(L$ D
(L$ H
(L$@D
(L$@f
(L$`f
(L$0D
(l$0f
(L$0H
(l$pD
(L$pD
(l$pD
(L$Pf
(L$pf
(L$Pf
(L$pf
(L$Pf
(L$pf
(L$Pf
(L$pf
(L$Pf
(L$pH
(local_source >= source) && (local_source < source + num_blocks * blocksize)
(local_source >= source) && (local_source < source + num_blocks * num_elts_in_block)
(local_source >= source) && (local_source < source + num_blocks)
(local_source >= source) && (local_source < source + sizeof(T) * num_blocks)
(name: 
(null)
(op_type:
(Optional) A scalar or rank 1 tensor containing a single value to be filled if the mode chosen is `constant` (by default it is 0.0).
(Optional) Axis along which one-hot representation in added. Default: axis=-1. axis=-1 means that the additional dimension will be inserted as the innermost/last dimension in the output tensor.
(Optional) Axis along which one-hot representation in added. Default: axis=-1. axis=-1 means that the additional dimension will be inserted as the innermost/last dimension in the output tensor. Negative value means counting dimensions from the back. Accepted range is [-r-1, r] where r = rank(indices).
(Optional) Axis along which to take slices. If not specified, input is flattened before elements being selected.
(Optional) Axis along which to take slices. If not specified, input is flattened before elements being selected. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
(Optional) Index of the diagonal to be populated with ones. Default is 0. If T2 is the output, this op sets T2[i, i+k] = 1. k = 0 populates the main diagonal, k > 0 populates an upper diagonal,  and k < 0 populates a lower diagonal.
(Optional) Seed to the random generator, if not specified we will auto generate one.
(Optional) Specify which axis is batch axis. Must be one of 1 (default), or 0.
(Optional) Specify which axis is time axis. Must be one of 0 (default), or 1.
(Optional) The axis of the dequantizing dimension of the input tensor. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input)
(Optional) The axis of the quantization dimension of the input tensor. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input)
(Optional) The data type for the elements of the output tensor, if not specified, we will use int32.
(Optional) The data type for the elements of the output tensor, if not specified, we will use the data type of the input tensor.
(Optional) The data type for the elements of the output tensor. If not specified,the data type of the input tensor T1 is used. If input tensor T1 is also notspecified, then type defaults to 'float'.
(Optional) The data type of the tensors in the output sequence. The default type is 'float'.
(Optional) The dimension to apply unique. If not specified, the unique elements of the flattened input are returned. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
(Optional) The value of the output elements.Should be a one-element tensor. If not specified, it defaults to a tensor of value 0 and datatype float32
(Optional) Whether map negative infinity to true. Default to 1 so that negative infinity induces true. Set this attribute to 0 if negative infinity should be mapped to false.
(Optional) Whether map positive infinity to true. Default to 1 so that positive infinity induces true. Set this attribute to 0 if positive infinity should be mapped to false.
(Optional) Whether to sort the unique elements in ascending order before returning as output. Must be one of 0, or 1 (default).
(outputs_.size() - 1) == i
(p0X2h4
(static_cast<size_t>(X_shape[1]) < nchwc_block_size) || ((X_shape[1] % nchwc_block_size) == 0)
(t$ H
(t$ L
(T$@D
(t$@H
(T$@H
(t$@I
(t$@L
(t$`D
(t$`H
(t$`I
(t$`L
(t$0H
(T$0H
(t$0H
(T$0H
(t$0H
(t$0L
(t$PD
(t$PH
(t$PI
(t$pL
(Tensor<T>) where the affine function, y = alpha * x + beta,
(Tensor<T>) where the softplus function, y = alpha * ln(exp(beta * x) + 1), is applied to
(X_shape[1] % MlasNchwcGetBlockSize()) == 0
(z*j$
) != (
) != new size (
) != split_dim_size (
) ) !
) + (
) + bottom_border (
) + bottomBorder (
) + right_border (
) + rightBorder (
) + scale[0] (
) + scale[1] (
) + scale_[0] (
) + scale_[1] (
) and node 
) and outputs (
) and the split dimension of the input (
) are not at boundary of span with size:
) attribute (
) because the ORT planned memory location device 
) bound to different types (
) dimensions are not positive.
) does not exist in the graph.
) does not have type information set by parent node.
) does not have type information.
) does not match expected type (
) does not match number of inputdimensions values (
) does not match the data size(
) does not match the number of channels (
) first dimension size does not equal NNZ.
) for attribute 'axis'
) for tensor of length:
) from file 
) has 
) has input size 
) has more inputs (
) has more outputs (
) has no index values.
) has output size 
) has zero input and zero output.
) in node (
) in op definition.
) in proto
) index value at position [
) input arg (
) is 0-element but contains data!
) is invalid.
) is not equal to number of scan inputs (
) is not equal to number of scan outputs (
) is not equal to the existing dim value (
) is not equal to the existing rank value (
) is required but not specified.
) is stored externally and should not have data field.
) is stored externally but doesn't have a location.
) must have a dense-rank > 0
) must have INT64 type.
) must have rank 1 or 2.
) must have rank 1.
) must have the same length. 
) needs to be greater than or equal to the left_border (
) needs to be greater than or equal to the leftBorder (
) needs to be greater than or equal to the top_border (
) needs to be greater than or equal to the topBorder (
) of node (
) of operator (
) of output arg (
) Op (
) or 1
) output arg (
) second dimension size does not match rank of tensor.
) should be stored in 
) should contain one and only one value field.
) should not be stored in raw_data field
) should not contain more than one value field.
) should refer to attribute in parent node.
) specified for sequence of size (
) than declared (
) to UNDEFINED is not allowed
) type inference failed
) vs (
)".".$.$.&.&.(.(.B.B.
)#.#.%.%.'.'.).).
)) , expected: (
))]]}}
), but the current device does not support 16-bit float.
), input tensor data type (
)\$@A
)\$@D
)\$@H
)\$0H
)\$PD
)\$pD
)\$PD
)\$PH
)]0fD
)|$ H;
)|$ I
)|$`E
)|$`H
)|$0H
)|$PD
)|$PE
)|$PH
)D$ D
)D$ f
)D$ H
)D$ H9i
)D$@H
)d$`D
)D$`D
)d$`D
)D$`D
)d$`D
)D$`D
)d$`D
)D$`D
)d$`D
)D$`E3
)D$`H
)d$`H
)D$`H
)D$`H+
)D$`M
)d$0D
)D$0E
)D$0f
)d$0H
)D$0L
)D$pE3
)D$PfH
)D$pH
)d$PH
)D$pH
)D$pH+
)D$pM
)E7Mc
)I;Vh
)l$ D
)L$0A
)L$0D
)L$0f
)L$0L
)l$pD
)L$pD
)l$pD
)l$PD
)l$pD
)L$pH
)'s input 
)'s output 
)s+v+
)t$ H
)t$ I
)t$ M
)T$@D
)T$@H
)t$@H
)t$@I
)t$`D
)t$`H
)t$`I
)t$`M
)t$0D
)t$0H
)T$0H
)t$0I
)t$0L
)t$0M
)T$PA
)T$PD
)t$pD
)t$PH
)t$PI
)T$pM
)t$pM
)u%D8)t
* p"Y
*".X(
*$F&"(
*&,v*
*&N("*
*@*B-
*\,n.d0P2`.L6.8~:\<t>
*^,b.r0d2b4r6d8b:r<v>b@rBPDbFTJ
*0,H.@0" ^$.&,*n,N.@0
-*0/0
*0+D+G+L+)
-*0-0
*D,l0 2p4V6
*H,(*V(h*H,(*
*H+1H
*H+9H
*j0b2v4p6b8j:p<b>`@:D|FnHVJjLdNbPNTI
*L$0I
*L$0M
*l$hb
*L$HI
*out_size >= 0
*X Z`^.`
*X,N.R0b2B4`6V,2*8
*X,p.X0B2T.L6,8
*Z,l.d0B2`.L6,8~:Z<l>
*Z,N.8, 0$2M
, am_attn_size}, Got:
, aw_attn_size}. Got:
, block in memory pattern size is: 
, but it doesn't exist or is not accessible.
, but it is already registered from file 
, but it its domain is not
, but it its version is higher
, but it its version is not 
, column 
, data shape: 
, error code: 
, external_data.length: 
, fall back to default allocation behavior
, Got 
, got 
, indices shape: 
', location: 
, max=
, node name: 
, requested shape:
, T".$,
, type: 
, Z"4$2
,"V$P&@(
,$F& (
,$F&"(
,$I;ppsrH
,&N( *
,&N("*
,*Z,4.20
,.,`,`,b,d,g,g,i,i,k,k,m,p,r,r,u,u,~,
,.,0,^,`
,.,0,^,`,
,`0.2
,|. 0
,<ellipsis>
,b.l0
,b.P0^.
,D$ f
,F$x$zP|.~,D(rPt.v,
,H.(,a
,H.N0^4V6f4
,L$ H
,N.2, 0}
,P..0
,p-p-
,T0<.Z0..
,V.@0$4
. *"T$.&,
. ,B&DPF.H,B&DPF.H
. . .
'. 0 == forward. 1 == reverse.
. batch_size=
. bin_num:
. Can't constant fold 
. Dimension 0 is 
. Do you have duplicated calls to SessionState::AddInitializedTensor function?
. Execution Provider must generate unique names across the entire model.
. Falling back to lenient merge.
. Ignoring allocator from 
. Index:
. Input rank=
. Input tensor rank was 
. Invalid ORT format model.
. It can only be 
'. It is no longer used by any node.
'. It is not used by any node and should be removed from the model.
. Must be 0 or 1
'. Must be one of 'forward', 'reverse', or 'bidirectional'.
. No schema registered for this operator.
. Num args is 
. Output tensor rank was 
. Please, fix your model.
. Shape:
. shape=
. The shape is: 
'. Valid values are 'LEFT' or 'RIGHT'.
. Validate usage of dim_value (values should be > 0) and dim_param (all values with the same string should equate to the same size) in shapes in the model.
.!.!.0
.(,.T0.2,
.*...0.9.<.?.A.A.C.N.
...0.N.
.:.;.@.@.
.:0^E
.?AU?$Abs@_J@functors@onnxruntime@@
.?AU?$Abs@_K@functors@onnxruntime@@
.?AU?$Abs@C@functors@onnxruntime@@
.?AU?$Abs@E@functors@onnxruntime@@
.?AU?$Abs@F@functors@onnxruntime@@
.?AU?$Abs@G@functors@onnxruntime@@
.?AU?$Abs@H@functors@onnxruntime@@
.?AU?$Abs@I@functors@onnxruntime@@
.?AU?$Abs@M@functors@onnxruntime@@
.?AU?$Abs@N@functors@onnxruntime@@
.?AU?$Ceil@M@functors@onnxruntime@@
.?AU?$ChainInterfaces@UIMLOperatorKernelCreationContextPrivate@@UIMLOperatorKernelCreationContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@WRL@Microsoft@@
.?AU?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@WRL@Microsoft@@
.?AU?$ChainInterfaces@UIMLOperatorSupportQueryContextPrivate@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@VNil@Details@WRL@Microsoft@@V4567@V4567@V4567@V4567@V4567@V4567@@WRL@Microsoft@@
.?AU?$default_delete@VBFCArena@onnxruntime@@@std@@
.?AU?$default_delete@VIAllocator@onnxruntime@@@std@@
.?AU?$default_delete@VModel@onnxruntime@@@std@@
.?AU?$ElementWiseRangedTransform@_J@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@_K@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@C@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@E@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@F@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@G@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@H@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@I@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@M@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@N@functors@onnxruntime@@
.?AU?$Elu@M@functors@onnxruntime@@
.?AU?$Exp@M@functors@onnxruntime@@
.?AU?$Exp@N@functors@onnxruntime@@
.?AU?$Floor@M@functors@onnxruntime@@
.?AU?$HardSigmoid@M@functors@onnxruntime@@
.?AU?$heap_implements@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@@impl@winrt@@
.?AU?$heap_implements@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@@impl@winrt@@
.?AU?$heap_implements@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@@impl@winrt@@
.?AU?$implements@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@3@U?$IMapView@Uhstring@winrt@@I@5673@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@5673@@winrt@@
.?AU?$implements@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@@winrt@@
.?AU?$implements@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@@winrt@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$$V@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00UIMLOperatorRegistryPrivate@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00UIMLOperatorTensorShapeDescription@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00UIMLOperatorTypeInferenceContext@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00UIWinmlExecutionProvider@Adapter@MachineLearning@AI@Windows@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@U?$ChainInterfaces@UIMLOperatorKernelCreationContextPrivate@@UIMLOperatorKernelCreationContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTensorShapeDescription@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTypeInferenceContext@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@U?$ChainInterfaces@UIMLOperatorSupportQueryContextPrivate@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@VNil@Details@WRL@Microsoft@@V4567@V4567@V4567@V4567@V4567@V4567@@23@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIExecutionProvider@Dml@@UIWinmlExecutionProvider@Adapter@MachineLearning@AI@Windows@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIMLOperatorKernel@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIMLOperatorKernelContext@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIMLOperatorKernelFactory@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIMLOperatorRegistry@@UIMLOperatorRegistryPrivate@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIMLOperatorShapeInferrer@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIMLOperatorSupportQueryPrivate@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIMLOperatorTensor@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIUnknown@@@Details@WRL@Microsoft@@
.?AU?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@
.?AU?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@
.?AU?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@
.?AU?$LeakyRelu@M@functors@onnxruntime@@
.?AU?$Log@M@functors@onnxruntime@@
.?AU?$Log@N@functors@onnxruntime@@
.?AU?$map_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@Uhstring@3@I@winrt@@
.?AU?$map_view_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@Uhstring@3@IUcollection_version@23@@winrt@@
.?AU?$MaxPool1DTask@C@onnxruntime@@
.?AU?$MaxPool1DTask@E@onnxruntime@@
.?AU?$MaxPool1DTask@M@onnxruntime@@
.?AU?$MaxPool1DTask@N@onnxruntime@@
.?AU?$MaxPool2DTask@C@onnxruntime@@
.?AU?$MaxPool2DTask@E@onnxruntime@@
.?AU?$MaxPool2DTask@M@onnxruntime@@
.?AU?$MaxPool2DTask@N@onnxruntime@@
.?AU?$MaxPool3DTask@C@onnxruntime@@
.?AU?$MaxPool3DTask@E@onnxruntime@@
.?AU?$MaxPool3DTask@M@onnxruntime@@
.?AU?$MaxPool3DTask@N@onnxruntime@@
.?AU?$MaxpoolWithMask1DTask@M@contrib@onnxruntime@@
.?AU?$MaxpoolWithMask2DTask@M@contrib@onnxruntime@@
.?AU?$MaxpoolWithMask3DTask@M@contrib@onnxruntime@@
.?AU?$Neg@_J@functors@onnxruntime@@
.?AU?$Neg@C@functors@onnxruntime@@
.?AU?$Neg@H@functors@onnxruntime@@
.?AU?$Neg@M@functors@onnxruntime@@
.?AU?$Neg@N@functors@onnxruntime@@
.?AU?$ParametricSoftplus@M@functors@onnxruntime@@
.?AU?$Pool1DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AU?$Pool2DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AU?$Pool3DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AU?$Powx@M@functors@onnxruntime@@
.?AU?$produce@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@@impl@winrt@@
.?AU?$produce@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@3@@impl@winrt@@
.?AU?$produce@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMapView@Uhstring@winrt@@I@Collections@Foundation@Windows@3@@impl@winrt@@
.?AU?$produce@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@@impl@winrt@@
.?AU?$produce@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@@impl@winrt@@
.?AU?$produce_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$produce_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$produce_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMapView@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$produce_base@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$produce_base@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMapView@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer_convert@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer_convert@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer_convert@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMapView@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer_convert@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer_convert@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producers_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@V?$tuple@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@U?$IMapView@Uhstring@winrt@@I@2345@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@2345@@std@@@impl@winrt@@
.?AU?$producers_base@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@V?$tuple@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@std@@@impl@winrt@@
.?AU?$producers_base@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@V?$tuple@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@winrt@@@std@@@impl@winrt@@
.?AU?$Reciprocal@M@functors@onnxruntime@@
.?AU?$Reciprocal@N@functors@onnxruntime@@
.?AU?$Relu@M@functors@onnxruntime@@
.?AU?$Relu@N@functors@onnxruntime@@
.?AU?$root_implements@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@3@U?$IMapView@Uhstring@winrt@@I@5673@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@5673@@impl@winrt@@
.?AU?$root_implements@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@@impl@winrt@@
.?AU?$root_implements@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@@impl@winrt@@
.?AU?$root_implements_composable_inner@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@$0A@@impl@winrt@@
.?AU?$root_implements_composable_inner@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@$0A@@impl@winrt@@
.?AU?$root_implements_composable_inner@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@$0A@@impl@winrt@@
.?AU?$root_implements_composing_outer@$0A@@impl@winrt@@
.?AU?$RuntimeClassFlags@$03@WRL@Microsoft@@
.?AU?$ScaledTanh@M@functors@onnxruntime@@
.?AU?$Selu@M@functors@onnxruntime@@
.?AU?$Sigmoid@M@functors@onnxruntime@@
.?AU?$Sigmoid@N@functors@onnxruntime@@
.?AU?$Softplus@M@functors@onnxruntime@@
.?AU?$Softsign@M@functors@onnxruntime@@
.?AU?$Sqrt@M@functors@onnxruntime@@
.?AU?$Sqrt@N@functors@onnxruntime@@
.?AU?$Tanh@M@functors@onnxruntime@@
.?AU?$Tanh@N@functors@onnxruntime@@
.?AU?$ThresholdedRelu@M@functors@onnxruntime@@
.?AU?$weak_ref@$00@impl@winrt@@
.?AU?$weak_source@$00@impl@winrt@@
.?AU?$weak_source_producer@$00@impl@winrt@@
.?AU_Crt_new_delete@std@@
.?AUBFloat16@onnxruntime@@
.?AUcollection_version@impl@winrt@@
.?AUContainer@?$InternalMetadataWithArenaBase@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VInternalMetadataWithArenaLite@internal@protobuf@google@@@internal@protobuf@google@@
.?AUCpuProviderFactory@onnxruntime@@
.?AUctype_base@std@@
.?AUCustomOpKernel@onnxruntime@@
.?AUDMLProviderFactory@onnxruntime@@
.?AUException@Ort@@
.?AUFunctionBodyBuildContext@onnx@@
.?AUFunctionBodyBuildContextImpl@onnx@@
.?AUhresult_access_denied@winrt@@
.?AUhresult_canceled@winrt@@
.?AUhresult_changed_state@winrt@@
.?AUhresult_class_not_available@winrt@@
.?AUhresult_error@winrt@@
.?AUhresult_illegal_delegate_assignment@winrt@@
.?AUhresult_illegal_method_call@winrt@@
.?AUhresult_illegal_state_change@winrt@@
.?AUhresult_invalid_argument@winrt@@
.?AUhresult_no_interface@winrt@@
.?AUhresult_not_implemented@winrt@@
.?AUhresult_out_of_bounds@winrt@@
.?AUhresult_wrong_thread@winrt@@
.?AUIExecutionProvider@Dml@@
.?AUIExecutionProviderFactory@onnxruntime@@
.?AUIMarshal@impl@winrt@@
.?AUIMLOperatorAttributes@@
.?AUIMLOperatorAttributes1@@
.?AUIMLOperatorKernel@@
.?AUIMLOperatorKernelContext@@
.?AUIMLOperatorKernelCreationContext@@
.?AUIMLOperatorKernelCreationContextPrivate@@
.?AUIMLOperatorKernelFactory@@
.?AUIMLOperatorRegistry@@
.?AUIMLOperatorRegistryPrivate@@
.?AUIMLOperatorShapeInferenceContext@@
.?AUIMLOperatorShapeInferenceContextPrivate@@
.?AUIMLOperatorShapeInferrer@@
.?AUIMLOperatorSupportQueryContextPrivate@@
.?AUIMLOperatorSupportQueryPrivate@@
.?AUIMLOperatorTensor@@
.?AUIMLOperatorTensorShapeDescription@@
.?AUIMLOperatorTypeInferenceContext@@
.?AUInferenceContext@onnx@@
.?AUInferenceContextImpl@shape_inference@onnx@@
.?AUinput_adapter_protocol@detail@nlohmann@@
.?AUiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@
.?AUiterator_type@collection_version@impl@winrt@@
.?AUIUnknown@@
.?AUIWeakReference@impl@winrt@@
.?AUIWeakReferenceSource@impl@winrt@@
.?AUIWinmlExecutionProvider@Adapter@MachineLearning@AI@Windows@@
.?AUmarshaler@?1??make_marshaler@impl@winrt@@YAHPEAUtype@?$abi@UIUnknown@Foundation@Windows@winrt@@X@23@PEAPEAX@Z@
.?AUmessages_base@std@@
.?AUMLFloat16@onnxruntime@@
.?AUmoney_base@std@@
.?AUNode__EdgeIterator@onnxruntime@@
.?AUNode__EdgeIterator_Impl@onnxruntime@@
.?AUNode__NodeIterator@onnxruntime@@
.?AUNode__NodeIterator_Impl@onnxruntime@@
.?AUNodeAttributes_Iterator@onnxruntime@@
.?AUNodeAttributes_Iterator_Impl@onnxruntime@@
.?AUNodeCompare@onnxruntime@@
.?AUnull_type@onnxruntime@@
.?AUOpKernel_Translator@onnxruntime@@
.?AUOrtAllocator@@
.?AUOrtAllocatorImpl@@
.?AUOrtDefaultAllocator@@
.?AUPad@onnxruntime@@
.?AUPriorityNodeCompare@onnxruntime@@
.?AUProvider_TensorShapeProto_Dimension_Iterator@onnxruntime@@
.?AUProvider_TensorShapeProto_Dimension_Iterator_Impl@onnxruntime@@
.?AUProviderHost@onnxruntime@@
.?AUProviderHostImpl@onnxruntime@@
.?AUSequentialExecutionPlan@onnxruntime@@
.?AUSlice1@onnxruntime@@
.?AUSlice10@onnxruntime@@
.?AUTile@onnxruntime@@
.?AUtime_base@std@@
.?AUtype@?$abi@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@winrt@@X@impl@winrt@@
.?AUtype@?$abi@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@winrt@@X@impl@winrt@@
.?AUtype@?$abi@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@X@impl@winrt@@
.?AUtype@?$abi@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@X@impl@winrt@@
.?AUtype@?$abi@U?$IMapView@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@X@impl@winrt@@
.?AUtype@?$abi@UIInspectable@Foundation@Windows@winrt@@X@impl@winrt@@
.?AUtype@?$abi@UIUnknown@Foundation@Windows@winrt@@X@impl@winrt@@
.?AV?$_ExceptionPtr_static@Vbad_alloc@std@@@?A0x327206fc@@
.?AV?$_ExceptionPtr_static@Vbad_exception@std@@@?A0x327206fc@@
.?AV?$_Func_base@_JM_N@std@@
.?AV?$_Func_base@_N$$V@std@@
.?AV?$_Func_base@_N_K@std@@
.?AV?$_Func_base@_NAEBUFunctionBodyBuildContext@onnx@@AEBVOpSchema@2@AEAVFunctionProto@2@@std@@
.?AV?$_Func_base@_NAEBVNode@onnxruntime@@@std@@
.?AV?$_Func_base@_NH@std@@
.?AV?$_Func_base@_NPEBVNode@onnxruntime@@PEBV12@@std@@
.?AV?$_Func_base@HPEAUComputeContext@onnxruntime@@PEAPEAX@std@@
.?AV?$_Func_base@MM@std@@
.?AV?$_Func_base@MMMM@std@@
.?AV?$_Func_base@MMMMMMM@std@@
.?AV?$_Func_base@PEAVOpKernel@onnxruntime@@AEBVOpKernelInfo@2@@std@@
.?AV?$_Func_base@V?$ComPtr@UIMLOperatorTensor@@@WRL@Microsoft@@I@std@@
.?AV?$_Func_base@V?$OrtValueTensorSlicer@$$CBUOrtValue@@@onnxruntime@@AEBUOrtValue@@_J_J@std@@
.?AV?$_Func_base@V?$OrtValueTensorSlicer@UOrtValue@@@onnxruntime@@AEAUOrtValue@@_J_J@std@@
.?AV?$_Func_base@V?$shared_ptr@VIAllocator@onnxruntime@@@std@@HW4OrtMemType@@@std@@
.?AV?$_Func_base@V?$unique_ptr@VIAllocator@onnxruntime@@U?$default_delete@VIAllocator@onnxruntime@@@std@@@std@@F@std@@
.?AV?$_Func_base@V?$unique_ptr@VTensor@onnxruntime@@U?$default_delete@VTensor@onnxruntime@@@std@@@std@@AEBVTensor@onnxruntime@@_J_JV?$shared_ptr@VIAllocator@onnxruntime@@@2@PEAX@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@$$V@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@AEAV?$shared_ptr@VModel@onnxruntime@@@std@@@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@AEAVGraph@3@@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@AEBV?$vector@_KV?$allocator@_K@std@@@std@@AEBVTensor@3@AEAV63@@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@AEBV?$vector@_KV?$allocator@_K@std@@@std@@AEBVTensor@3@AEAV63@PEBVTensorShape@3@PEAX@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@AEBVNode@3@AEAVGraph@3@AEBV?$vector@PEBVTypeProto@onnx@@V?$allocator@PEBVTypeProto@onnx@@@std@@@std@@AEAV67@AEBUResolveOptions@53@@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@AEBVNodeArg@3@_K@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@AEBVTensor@3@AEAV43@PEAX@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@AEBVTensorShape@3@AEBUOrtMemoryInfo@@AEAUOrtValue@@AEA_N@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@HAEBUOrtValue@@AEBUOrtCallback@3@_N@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@PEAX_K@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@PEAXAEAV?$vector@UOrtValue@@V?$allocator@UOrtValue@@@std@@@std@@PEAX_K@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@PEAXPEBUOrtApi@@PEAUOrtKernelContext@@@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@PEB_JPEB_JPEA_J_K_K_K_K_K_K_KPEAVThreadPool@concurrency@3@PEAX@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@PEBHPEBHPEAH_K_K_K_K_K_K_KPEAVThreadPool@concurrency@3@PEAX@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@PEBMPEBMPEAM_K_K_K_K_K_K_KPEAVThreadPool@concurrency@3@PEAX@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@PEBNPEBNPEAN_K_K_K_K_K_K_KPEAVThreadPool@concurrency@3@PEAX@std@@
.?AV?$_Func_base@VTensor@onnxruntime@@AEBV12@AEBV?$vector@_JV?$allocator@_J@std@@@std@@_NV?$shared_ptr@VIAllocator@onnxruntime@@@4@PEBVTensorShape@2@PEAVThreadPool@concurrency@2@PEAX@std@@
.?AV?$_Func_base@X$$QEAVOpSchema@onnx@@@std@@
.?AV?$_Func_base@X$$V@std@@
.?AV?$_Func_base@X_J@std@@
.?AV?$_Func_base@X_J_J@std@@
.?AV?$_Func_base@X_K_K@std@@
.?AV?$_Func_base@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_base@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_base@XAEBVNode@onnxruntime@@AEAV?$function@$$A6A?AV?$ComPtr@UIMLOperatorTensor@@@WRL@Microsoft@@I@Z@std@@PEBXPEAUDmlGraphNodeCreateInfo@Adapter@MachineLearning@AI@Windows@@@std@@
.?AV?$_Func_base@XAEBVNodeArg@onnxruntime@@_N@std@@
.?AV?$_Func_base@XI@std@@
.?AV?$_Func_base@XMPEAM@std@@
.?AV?$_Func_base@XPEA_K@std@@
.?AV?$_Func_base@XPEAD@std@@
.?AV?$_Func_base@XPEAH@std@@
.?AV?$_Func_base@XPEAM@std@@
.?AV?$_Func_base@XPEAPEAUOrtValue@@@std@@
.?AV?$_Func_base@XPEAX@std@@
.?AV?$_Func_base@XPEBMPEAM_K@std@@
.?AV?$_Func_base@XPEBVDataTypeImpl@onnxruntime@@@std@@
.?AV?$_Func_base@XPEBVNode@onnxruntime@@@std@@
.?AV?$_Func_impl_no_alloc@P6A?AV?$OrtValueTensorSlicer@$$CBUOrtValue@@@onnxruntime@@AEBUOrtValue@@_J1@ZV12@AEBU3@_J_J@std@@
.?AV?$_Func_impl_no_alloc@P6A?AV?$OrtValueTensorSlicer@UOrtValue@@@onnxruntime@@AEAUOrtValue@@_J1@ZV12@AEAU3@_J_J@std@@
.?AV?$_Func_impl_no_alloc@P6A?AV?$unique_ptr@VTensor@onnxruntime@@U?$default_delete@VTensor@onnxruntime@@@std@@@std@@AEBVTensor@onnxruntime@@_J1V?$shared_ptr@VIAllocator@onnxruntime@@@2@PEAX@ZV12@AEBV34@_J_JV52@PEAX@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@AEBV?$vector@_KV?$allocator@_K@std@@@std@@AEBVTensor@3@AEAV63@PEBVTensorShape@3@PEAX@ZV123@AEBV45@AEBV63@AEAV63@PEBV73@PEAX@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@AEBVNode@3@AEAVGraph@3@AEBV?$vector@PEBVTypeProto@onnx@@V?$allocator@PEBVTypeProto@onnx@@@std@@@std@@AEAV67@AEBUResolveOptions@53@@ZV123@AEBV43@AEAV53@AEBV67@AEAV67@AEBU853@@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@AEBVTensor@3@AEAV43@PEAX@ZV123@AEBV43@AEAV43@PEAX@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@PEAXAEAV?$vector@UOrtValue@@V?$allocator@UOrtValue@@@std@@@std@@0_K@ZV123@PEAXAEAV45@PEAX_K@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@PEB_J0PEA_J_K222222PEAVThreadPool@concurrency@3@PEAX@ZV123@PEB_JPEB_JPEA_J_K_K_K_K_K_K_KPEAV453@PEAX@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@PEBH0PEAH_K222222PEAVThreadPool@concurrency@3@PEAX@ZV123@PEBHPEBHPEAH_K_K_K_K_K_K_KPEAV453@PEAX@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@PEBM0PEAM_K222222PEAVThreadPool@concurrency@3@PEAX@ZV123@PEBMPEBMPEAM_K_K_K_K_K_K_KPEAV453@PEAX@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@PEBN0PEAN_K222222PEAVThreadPool@concurrency@3@PEAX@ZV123@PEBNPEBNPEAN_K_K_K_K_K_K_KPEAV453@PEAX@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVTensor@onnxruntime@@AEBV12@AEBV?$vector@_JV?$allocator@_J@std@@@std@@_NV?$shared_ptr@VIAllocator@onnxruntime@@@4@PEBVTensorShape@2@PEAVThreadPool@concurrency@2@PEAX@ZV12@AEBV12@AEBV34@_NV54@PEBV62@PEAV782@PEAX@std@@
.?AV?$_Func_impl_no_alloc@P6A_NAEBUFunctionBodyBuildContext@onnx@@AEBVOpSchema@2@AEAVFunctionProto@2@@Z_NAEBU12@AEBV32@AEAV42@@std@@
.?AV?$_Func_impl_no_alloc@P6AMMMM@ZMMMM@std@@
.?AV?$_Func_impl_no_alloc@P6APEAVOpKernel@onnxruntime@@AEBVOpKernelInfo@2@@ZPEAV12@AEBV32@@std@@
.?AV?$_Func_impl_no_alloc@P6AX$$QEAVOpSchema@onnx@@@ZX$$QEAV12@@std@@
.?AV?$_Func_impl_no_alloc@P6AXAEAUInferenceContext@onnx@@@ZXAEAU12@@std@@
.?AV?$_Func_impl_no_alloc@P6AXAEAVOpSchema@onnx@@@ZXAEAV12@@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@_J@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@_K@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@C@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@E@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@F@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@G@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@H@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@I@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@N@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Ceil@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Elu@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Exp@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Exp@N@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Floor@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$HardSigmoid@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$LeakyRelu@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Log@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Log@N@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool1DTask@C@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool1DTask@E@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool1DTask@M@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool1DTask@N@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool2DTask@C@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool2DTask@E@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool2DTask@M@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool2DTask@N@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool3DTask@C@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool3DTask@E@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool3DTask@M@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool3DTask@N@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxpoolWithMask1DTask@M@contrib@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxpoolWithMask2DTask@M@contrib@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxpoolWithMask3DTask@M@contrib@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Neg@_J@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Neg@C@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Neg@H@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Neg@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Neg@N@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$ParametricSoftplus@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Pool1DTask@MVLpPool@onnxruntime@@@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Pool2DTask@MVLpPool@onnxruntime@@@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Pool3DTask@MVLpPool@onnxruntime@@@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Powx@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Reciprocal@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Reciprocal@N@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Relu@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Relu@N@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$ScaledTanh@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Selu@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Sigmoid@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Sigmoid@N@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Softplus@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Softsign@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Sqrt@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Sqrt@N@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Tanh@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$Tanh@N@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@U?$ThresholdedRelu@M@functors@onnxruntime@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@UNodeCompare@onnxruntime@@_NPEBVNode@2@PEBV32@@std@@
.?AV?$_Func_impl_no_alloc@UPriorityNodeCompare@onnxruntime@@_NPEBVNode@2@PEBV32@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_006c5dfae8d5efe14aa8dd98266e3df1>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_006f042491572090b778a6887556c541>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_007d356207fb2206257759d3648591e7>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_00f913cc4b69ebb86f45ae3c7b5157d6>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_019248982c19876b36bd9d56cbf00fcf>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0251dfdaad61cfa5b2a46c99b78d27e8>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0275254af342f4b71991ea84a162d515>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0311f58d27b0724e7aff1c8a092d73e4>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_031e8dfd8d6a123502d9ec2194443e60>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_038eb253b7f4e93118566e43dda81530>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_03e7406b29b220fa131a6585ce48dcd2>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_040c82e16435d8fa2645bdc3a6feb222>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0443539308e5b1117b55e6de227ed40c>@@XPEBVNode@onnxruntime@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0457fc55cb15f22bdcba75f54e0aa1c1>@@XPEBVNode@onnxruntime@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_04d93a296e8e245be45f1b71871c71c4>@@VStatus@common@onnxruntime@@AEBVNodeArg@4@_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_05416e20ce8aa4e6f975518b44ddcde1>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_05b00e57ae6a5f779503e7606fdbb129>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_05c8e068d08f49052da87d86b6e09131>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_05d4d8e8ce6819b16e6fba2d5b9fcbb4>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_06244d6f86a0e04dd264c197f436ab90>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0634e9236de63d479eb0b5769cc5905c>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_06471b7954288b9dda9f9d99a72bb7c6>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_066c50938341a8bc4180c44723e9e561>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_06c5d09c41dfd075b909f65599f357bc>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_07b5ec0d67fe9f3a2b9eff732577e516>@@XPEBMPEAM_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_07c38659a483ec71b80a34d988a12e67>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_07cc7db52a628b03f17e7b1fd5adf33e>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_084d58d9727b25a791fe4a65cdec3cd9>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_087f7d76293e9eae88f0507abf2d5be6>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_09052ceb1bf3b577c728dec9fe3eab9b>@@_JM_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0920e8c792b09e0b2fbd0035321589a5>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_09804657b1d4ece14dbea3419a326007>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_099bb0573d1376fbed683cceb180bb81>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_099f8dd4d33b6b91bc0ee51d86ed6c1a>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0a3d98e003a8310444f50c395f4ee870>@@_N_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0a786afdc494302a03a8347211af4f5e>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0aa140aa205a0a79b623f0b6cf867084>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0b3e445014abdc00ff4fcef82b0ddb16>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0b4501e0c9b2f4b8bd34960a35816f56>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0b4cc153b058489c05500d0b394c47a0>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0c69e806d9e2fad559bbd11cd911ec3e>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0c6b04fd5867b05b63eee5705dade40a>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0d27c011736e5be9787a1d0ac63fe07a>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0d57079c5084d18bf7b57a312b9c6ff1>@@V?$ComPtr@UIMLOperatorTensor@@@WRL@Microsoft@@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0e9a1a7b6965f6b0c97b479e18664de1>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0ef38e5b4dd40a551bcbede7fadd1472>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1009fe77501925cd48ec95dce1652db1>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1038cf6b5b3a06f7836c03470cc73cfe>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_104e5d4f3c346a5807a7db11389af990>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_115059020db09aa13bab825ad518034e>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_120fe39f3f94f79844c85c347c36a663>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1221b4097effa32a8dd388b4455dd2c1>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1270656b5748c7df835592233d045234>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_12732c75ed9773d86371326e53f563fc>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_12dd4c4dcfd6c2d8effe6071c823bd43>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_13046178b359e9c81742cfc406e97a28>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_13c0d49a798e8cd7f3691f8cf3d13739>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_14407bcf8cfc66db9a85fd2745a31d94>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1472d44ac2293296de1d1858a76d2083>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1476157c6a284e4f379191854345eea5>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_166f6feb25832b61746c79ccdee9d41a>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_16be1790a9760642272e826be4697ed1>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_170eb7415bead306baf0c6b0a2038992>@@VStatus@common@onnxruntime@@AEBVNodeArg@4@_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_173e0a40a6995e7b7babfb2fbad06d73>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_17829500fdcde99eb4798ae93821b557>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_182f1a27d8844e569a31380c256cc9bb>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_18a3b6757625880e23566c26c3f789bc>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_18c8bc7d6e1564eeb9ad39be83abfb9f>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1a27d0f30cf9efcc6916e583d192ed19>@@_N$$V@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1a6960ebca2811a967a9cbfd5069c842>@@X$$V@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1a8b8132ed3c0ab63c63224ad4161527>@@_NAEBUFunctionBodyBuildContext@onnx@@AEBVOpSchema@3@AEAVFunctionProto@3@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1aa88cfbcf551c26b6ab6d047872e0e0>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1b259c1c37867f7117af2630fb7a6d3a>@@MMMMMMM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1bd85f327df3bce7adb1be01960673ad>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1be1cbcaffa122d29a4666a385d05e9a>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1cd5cfe6b76bb3ba1c53203d58a4eedd>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1cdda74d195f4dc7bbdbaed3ee174bf7>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1cfb91e6d30b05211dfbd68ad9e022a5>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1ea0a0832a4cf660ea6a57031389b3fe>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1f048742a5a93eb61d2d12e7e445b475>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1f1cad28f5d50542c82767124ccbbeb8>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1f71f65e3a443fc2082168237f5b3822>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1f8675d15ee5efaa8f5579945c6d118a>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1fd04ff7ed408ca4ebed6e8107f19670>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_21332c666c4782640bb01cbce240ea30>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_21a762d3684c6831db7ddce449ffa123>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_221457049dd6e599b1e592be3898266a>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_22478b9deeaf5ef5a9976c43d80db60d>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_22485ec429c0920bef998442ed4b0141>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_226d842a4bcdc8605120f4a2efdc3985>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2344ddb4306265b449b2e4f4d9e8b0c6>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_236692037ce4d824baf1ec7c2be78fd2>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2369f292b0f42fbecffef8359e85ebdc>@@HPEAUComputeContext@onnxruntime@@PEAPEAX@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_236f7c8f49088d4c504fd71f55f4280b>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_23a1bb0478ab7117c6dff4712434f02b>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_24865a883638478ee05cb85a5ed86e33>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_24b1e53580405f6c2ad3def2f5ac0f9b>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_25395042ec336ff6c2d2bf12e6e33481>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_25418f5506f1baf0333fc0e9c546e5f0>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2669dde96175d6d91119a033acb17aa8>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_27a631d2450c357a52927c1dfbd2efda>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_27eb941e257b57f9cf2ee9f3ef3087b3>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2875b3e2e66b274d36ac70e9f0680993>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_291539eac26e37a91034c8d9cfbf8005>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_29460ba72c9a9f61937f695b212c2385>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2997ea06f466f3cb7af7580139e0e9b1>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_29a27f66e5d65fb36ed8965e3fa92ded>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2a0432e2f861fdcc4b7f759b0200b6a4>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2a3f9f19c48a100998729328075237dc>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2bf41ec2c8021b2e4a040cfc2c7e5e74>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2c89f6b694672e0bdff0d3a443bad435>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2cbd0994bac82f132cec95ecbcb986ab>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2cc7c01007d8f546fc79115b36c9098b>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2cda3ec4180407fdb66cae08f370124f>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2d779cea19d25f2c662fbf9068287203>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2da95d9a6f78440b51d048ed6c8e3561>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2dc68f98e6942dc4cba921733a4cc521>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2de5c71bdb57850c67f17271c98887bb>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2df0990ebf51ceedd01650692ad351d9>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2ec601f90d3be6e3270c931423ef1071>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2f4103e1c4a1773101a6c1f055d56df4>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2f6217725d36a8fee3233a5a3a8acbc4>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_30e1101d7025e2d839d1daa9ac434cd1>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_30fd1fff2c31f5dad21f99700592798a>@@MMMMMMM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_31044cbb2065f4b8855bbdc646057e42>@@XPEAM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_310ceee0e4c58a7a83d7d00319ed9410>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_33215563a6aacb86deff746a6aa28a6c>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3354f944db7877754471d985be3fe29b>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_33598cc4eaf8359204b6791532beb5a1>@@X$$V@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_338e5fd506fd3a59b6f8e616d7366dcc>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_33ab9233a6e2a6fe6c24d1963cbc3111>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3418ac6d0a6ccb6c692baad9c09fd00f>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3435a69bc2a4a58cf25c9601fa916799>@@XPEA_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_34eb695d67d7709c51f33f1b8d0d7456>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_35df272685e40c7e2ea40583b6906dbe>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3674353b915e5a5f4e97c63731ea3b7b>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_36945aef9b69b8bc246158e60ded74ff>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_36dd25e706f1ff93211e6de0438e06c6>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_37fc46271b5a9d577e78557058b76819>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_37fc51724b8ab138f4abea1c86c474be>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3928e725003c7a62c76f41f34cee3234>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_399e4ac9a8be74e8d14d0d5e67b02a32>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_39a308a33350cfe399318acb8bdc0850>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3a8a940e89b14c9a8617dfa32f79a60a>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3aa61f6258ac2da5aa1399bf10de3644>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3b30dc70df2fd5467fa431e4f2f4c7c6>@@V?$shared_ptr@VIAllocator@onnxruntime@@@std@@HW4OrtMemType@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3b97251de202434e86d007c33f9345a1>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3bef22810faae0f9d9c56ac7d1b31770>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3bf9d7b5239a137326d2c5fa821fa737>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3c79a7d5f53525620be3b5426f6bc737>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3e55de506c3b90a3897c70c68b70e958>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3f0ef66d0b0de67b55e854d697da2fff>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3f2e81ef89df0c02055696f8c5f6254f>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3f66090bd830c9da19f940c92f6ab3b0>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3f6b039d1c2a9ddc0c4edd40a4875e7e>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3f992177007ef3a6a4a7f362ca0d72b8>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3feaee9fba56b8002e91e57f10529540>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4092000fa2c851eb2ddefc1b6efbf1fc>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_40d7d549b7296d37bd8a375a6a32735f>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_40f74427ec28f20a5d59b70a3f7ba162>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_416d8694f58a78bcb0cae00322a6dfbe>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_41ebeab5da8beef13ef5472118318ec8>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_42085d5d1ab621f3e103058b343368b0>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_423c7f4514cb0f580d17c1969d94ab4d>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_42580b4f3e49f7dcfbf37d76233bd856>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_42935452e405cc9507a07edc9f5911fc>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_429bffec90bbca49494510e27e518b79>@@XI@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_42fa4ee951b97f0e77b9de9f38db0750>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_430c7041764d504802816f1b3fb25d0a>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_43d76a454d7e446551a32b163679964a>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_44f740f4362270fce5c964ebb50cbf77>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_452b080256e46867afd3b39c75897486>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4623cd97b09f16cc323b1da4783c1a74>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_464b0c569b5e14ad1c13cf083ce55247>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_46a0ec34ba455071cbedc5983343a291>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_476f16e46856ad1afb9e2f3aa987cf47>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_47f8da96f41c768dc3c8df613df77fec>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4888e52cfaab8cdad25ecea017b54d57>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_493755c0718e549fc6eb6376aaf1c076>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4a3dbd1b89fc0bebb34de978cf1b5369>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4b9483331b5c6f0ec35078b873f8a5a1>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4baf23d0c206ed351c6ebf16f344f7e0>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4bd02c7b372889e26c922fb97a0c9ac3>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4c36c751efc3ade178fd6425d4750a0b>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4c8889970a5cc40b02f37f6aaa2ab4a4>@@VStatus@common@onnxruntime@@AEBVNodeArg@4@_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4cb6536179ac33ff59d8db68f89e6d48>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4d2bb8b4b91e7c65b08948d705bb6118>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4e978ba75a8a0f3f4d2d1e75d74ac4de>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4f07ad08d951f2412255fcd32991b778>@@V?$unique_ptr@VIAllocator@onnxruntime@@U?$default_delete@VIAllocator@onnxruntime@@@std@@@std@@F@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4f3d7353e76f345aa194bb4f18efcd6f>@@PEAVOpKernel@onnxruntime@@AEBVOpKernelInfo@3@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4f6fc7acd223589eaf3936f4e0c463ec>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5033badb7240167b30b90523bec68adf>@@X$$V@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_50815597be56ed0e327183a6f984971c>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_50ac1bbabd41636fda9e1bf36b1fc455>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_51049cb5fa4b41211d9b2f4f400d5578>@@V?$ComPtr@UIMLOperatorTensor@@@WRL@Microsoft@@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5151cdb5e2d0e967b0ff9b516f4dde5e>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_51e7ce4347ad68176c6a114dbe1a7fb9>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_51f7b34ad2d14ff9bfb03559ccadc81f>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5275443c3995dec7145dee969221190f>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_53958a524045125d538038a834c170f9>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_540effeb914d07d2f71f09cc0eadb0b4>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5455b7a3d85ed24852611958e08bf1e7>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_54c52b5102f405f26b86736abc33d9dc>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_55c987d2288bf4751b7cab2cd3c58f7e>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_55dd3d0ebe2e48cd699a7e50bd7c1613>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_55ea1c7ed7f7f7268714c360c9a1eb07>@@_JM_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_55f27842a844c2a0f8f9655ee109b4f8>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_563fdc041becc3012b0755fecfa41a3e>@@VStatus@common@onnxruntime@@AEBV?$vector@_KV?$allocator@_K@std@@@std@@AEBVTensor@4@AEAV74@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_56616cb04d479ecbbef60d16cb985ff3>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_576babf2632b8f119a1c6bd71dacdba2>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_577697c65f7f30ab5c890fb5e643c3c6>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_581e172f6e6bf6d53a792d4f2adbb4c0>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5820733bf712edf4856ad2454498e68c>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5868813267026bde29d771b4b1b80b4f>@@_JM_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5887a420a41b7e61b78788bc0c22095d>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_58e45bcf81f2eb4f6ae6b3f124defb13>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_58f61fab9db18b85afa307b409b47336>@@_NAEBVNode@onnxruntime@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_59a1cf9d667f240536256011adc0acbf>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_59d0c8449581e17e31aa5d36d4e1dfcc>@@_NH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5a03cad439a535c73c9479bab198ff8a>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5a274fa2bc43001df426f4be14bb9207>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5a4a1a47e435bf53505cb70a52e252af>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5b4207cefa66d538798d564c2c02cc47>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5cb0d9b3510f76020ef572153549b475>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5d23ede476722136670b473ee395a3f1>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5d65e3f0f83e3e12149b32c274720cbb>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5dc4cb1bad49b465af07c0f2ab16f9e0>@@V?$ComPtr@UIMLOperatorTensor@@@WRL@Microsoft@@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5dddc316c01bb02791c37b03fa523bb0>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5df54278b3724a9baf121a1f8852e38e>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5e0adb550519bb305a282a49bc052ff5>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5f12e6a0112b95dae1a14ebbefa23368>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5f8bb775dedd90ad37d19f6b92037c69>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6098da3ff9cb8b08151d3844bed31f70>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6137ab0b6712207e9054d1c872b492dd>@@V?$ComPtr@UIMLOperatorTensor@@@WRL@Microsoft@@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6140388ec8dda4d6c3a2334a907586b2>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_61f3572bba9686bdafa622dd4d9fc8d2>@@PEAVOpKernel@onnxruntime@@AEBVOpKernelInfo@3@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_626d338c2f6a43327586cf0d89586aa1>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6277f32b20d3d158c3ebcc1771d29c1a>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6299ae6da9b19142e396b5cd2562b3f4>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_62f1ce9bc208e37c7e5739c8f36388ed>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6367df6aa050372a33bd7465b9bffebf>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_63cff24d9185d14f18fbcbf2a1033b5f>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_63db87e2da8ecf77c1c67b6ddf9097c0>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_640443148c28920f15cf87dfb5e782ad>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_64826d400df5e2683a863a4dbb954602>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_64c62750e999b82780ab6a59a6ba6851>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_651d9e9e9fab134890e8df206635b6df>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_65379b8fe5c982ca5593c72a9026c7e8>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_670b3cf7fae0fc01758a26323e302b88>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_670b81b0a20483d6e6a649cadb77d57d>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6730021c9a4195800fc17a38baeda605>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6744a0008aca8ce8a0b29db8f1c01fa1>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_69c32f0b6e172d0bef2d15e0c00baa25>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6a160209166296c80c9867c0c24688ea>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6a32611970906250c6b47d8292ad00e7>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6ae1892ff4a4003e2e47baa65ff24d8d>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6b9436f4089e82f5864d5365b56a5e02>@@XPEAD@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6bf7af48e7a4158e20b4f833c15de130>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6c3806e2f5558f0233ccf8d05c3ab49e>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6c435e48e6003c8c444134537317003d>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6c91c6f34bd8327dc22f6b7f8735fd5b>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6cf09a4d87483efc80956d9a5ca8e5cd>@@VStatus@common@onnxruntime@@AEBVNodeArg@4@_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6d3877051847983c73e11af2112f64fa>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6da49eb0f8069eb4bbc177d850fb6dbc>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6db08e7b535fdfa2ab543d6397b422f2>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6dcb2a3f2f16914195595cee405bb7f9>@@X_K_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6e1ae7feac7554001d4e77c1f621284e>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6e3cb65d29853e95007cf81c805209f2>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6edb8e5a01fa4dd0c3e5fdadea9cdb78>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6f80d65fbc6388042df99a3b220305d1>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_70279f22222c392cb493fce9840987c7>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7042c4e252262e9c03c2cccafb92f0c5>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_70a5014ee65d8eb269c12c537da5a5f1>@@MMMMMMM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_71e695b5d6b9b0e0404ca139c0ef87a1>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_72e83488395612e0f6a2602b726a187c>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_72fa6f1dd132a0cb664556e9b32ab2a9>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7361ba811d5ff92bf50e102c73312480>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7365299d20f0c0cc81db24afaeca7624>@@MMMMMMM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_738d98ba114f0b992fde43f3c2b8082f>@@_NAEBUFunctionBodyBuildContext@onnx@@AEBVOpSchema@3@AEAVFunctionProto@3@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7414049e307c7d47f77a8e839817b7a9>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_745ebee95180d242f8918f6ee05b1702>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_747f5a5054cea5042105b3988bb8e54a>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_74a481eac2e8d64c2bfb5722b5b1cb89>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_76259788010337ce84c7523a6ac9ea2f>@@XAEBVNodeArg@onnxruntime@@_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_76f850960fe6ac327acd4613d41835f7>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_77304148f0ddead6f8725fd6b1fe3a6f>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_786b2ff03846fcfa21d00bf234deaff2>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_789f43d877e88a79c653d99212ff6d23>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_78d2ff8fcf22c6166aa97abbb5fd0d75>@@XPEAPEAUOrtValue@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_795d004014b095ac6c3348f10d228401>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_796aa6c99df67f823c02a52131c10530>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_79f39293405ab41c7ab7de2e65eb25d5>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7adac37e31393d4ac6b5bca719f468e8>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7adcd22a7346287699992d30bc8e4d12>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7afa01f5238052e0bc3d44d682bbe41c>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7bf0c45af15d48ff0bf3482d9ab1530d>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7c1e6047474e627b4a76b3aff4efd43b>@@MM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7c9d354511f23b5ab35f0d23acf59e69>@@_JM_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7ccee1bbd5e211d37343acf4a25d203e>@@XPEBMPEAM_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7d18ce867b586dae3ed4309094f85b3d>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7d43feae7ce5df77bbbe61f408666b24>@@V?$unique_ptr@VIAllocator@onnxruntime@@U?$default_delete@VIAllocator@onnxruntime@@@std@@@std@@F@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7d49a6264e4dd01effe57235ddc8a797>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7d4a5c6b07fd360e232dc4416f33eeb2>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7d5e6e2f53eb2e4c8474c9eb67ea5809>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7d68ba7556ef690147168e00d959e2c9>@@XPEBMPEAM_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7d7cf49bce0e19787ec70235e54b100e>@@PEAVOpKernel@onnxruntime@@AEBVOpKernelInfo@3@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7dcae2be4577512cc32ab12b37bbe9bb>@@VStatus@common@onnxruntime@@$$V@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7e6056f43d2e17d5b39d88df6efe7e61>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7e973787028859cebe35750559b8a424>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7ef4d7bf9a11acea8de40a758239a896>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7effc068e9678d1890888bb91fa9882e>@@MM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7fb11eaf1ed7921e4b12c72edf5431c8>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7fe593f35b7bf5ce6d3de3d99005ffa9>@@VStatus@common@onnxruntime@@AEBVNodeArg@4@_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7ff2aca09375d1b200a4b303c393fea9>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_80083253b4a44259ccb0f1ab2453639e>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8052aa8f31d91e8553d059cc8a579374>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_81b3e5fabf0f60201facddc5b3aeca88>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8205a59784c4f81abae5636754370a87>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_825b24178740a83899599d9a15262ad0>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_839e4f861e16af089c617eb4d1d9047f>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_83fc98e77223d87948a3045c6a6d1977>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_845fb0d365668e61a65e09bae6280c09>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8470282d05918db64db57459ab19a88b>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_84c0c09494d01cc30f16e3449edead54>@@XPEAH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_85098ac94eb0ea35e2ec901831d538f0>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_853403f8a1cb1bf480beff4c6dac6b21>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_854a3a76655e0ae1c9e2878289036568>@@VStatus@common@onnxruntime@@AEAVGraph@4@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_85740bb65c1a7256d972833e138b25dd>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_86b0d88e3b26101a7a3a4da61749b425>@@VStatus@common@onnxruntime@@AEBVNodeArg@4@_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8711802b90051c34567904599fdbb25a>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_872c985ed95e21f46464410321ab9d0c>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_87377fe9a7b55ca099852704c7649a37>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_87cb9d4d5e38fb120a31a53965033d4d>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_87d1adad0ec3c9a329fb0c8dabf3ed17>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_881846c2025fe9fdf21e10df8ef1aed7>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8825ba7f925d933a5700bea03837b34e>@@V?$unique_ptr@VIAllocator@onnxruntime@@U?$default_delete@VIAllocator@onnxruntime@@@std@@@std@@F@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8859cd3c08b6a8e9d40267fbf61322d1>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_88b1496ecc213c07fd24b30bb5c856e7>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_88cacdca8b660ada94d8f522e9f11bdd>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_88e1ec3a19eedcbddbcfd03d69c09c03>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_88e4cbdd9578a1aef93a2f7d3b7ececc>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_88f211d79a091f6a481688e49635f453>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8ac5cfcccd347e512ac9419fd63346cb>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8af3a30942023f4a3511597660fac0f0>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8b24a914bf112352c9369f2b44e2ccd1>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8ba5740a8c0829410cb3674fe8f010b1>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8baa7635c058d848d4bb5868b5a1e31c>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8ccc49f0cad898f6323ff4e82d1ce354>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8d89ab19f4dfa0ecffb29771863af791>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8ddc74f94abbe18c414cbbd5ea52c4fb>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8dfa231b74e328c8530f0e32ce2f974d>@@VStatus@common@onnxruntime@@$$V@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8e2d345faf08aedb7570dd988fdca755>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8ea5687524a42143533b684f5c9ce181>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8ee8d0015a85dfa1395e6275618758cf>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8f0d91faf2fd476a9a61179d9b72837d>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8f7a58d98eb24c89d9b0a7ccf81cddea>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8fd00d5632d83f4a297582c97b44acf3>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8ffce33af8696a7a42fa073f6875aacb>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9201c11196abf45c662a6757819da80c>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9222e83c56bbeb08f964662158da2721>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9289ae269610356b68d5c3f46e89bf66>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_929181c452e7cbade815a0403dd15e2f>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_92ce44a6835ab59773e2e1a9e5082039>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_92e7ca79b8942501ac9dcdff3174eb32>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_93063d24d3eaf65455400e96fd395987>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_930b54dfc8a64c42da270936f80e56b0>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_932e39bc42b54acf3286176bb275e4a7>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_93957b37c8fde7931003607e2ea412ad>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_93b65f05ddd7a75740e1646ed2ad5a8e>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_93dbd15e01be52d41a7a111df62d38a4>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_957302132dd7fea31b11af321304eee3>@@XAEBVNodeArg@onnxruntime@@_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_959b8361497e19898ca852a4058b29b4>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_95b9fc9fd7cabe9da00f754968571d1b>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_95c74ffd32a4d994a62a191daf0fff27>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_95dd41682914710e4e8d31623057e7d3>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_96fa572628c59e4d078744e93c0f7196>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_974421a2166ea3684014171d0d75f69e>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_977aa0989600c553e6ad21ef5f183702>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_97d1f16f53a6cde81773c5f27deb1fd2>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_97f1aa20e78dc8680c319e9629ef8e8e>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9800a6d45ddc69dc425addf83da8e9e3>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_981f87d47b0bee1dfc124a3791149631>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_98d34ef6b8d86da687aa9aa634b2afb2>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_991d63fb8a809dc84e4e523068ce486a>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9974bd71538aef5818f74e4a5e52a52e>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_99b6e836d69b69305b62d8ff2093a102>@@XMPEAM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9a0946b71204ca51d455c5e9aa49e823>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9a122db8c0c6f400c116ad5d2cdaf0bc>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9a2786289a4d18fa33a8facc5fc736fa>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9a56c482bf06c1a34241f10d7df60646>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9a74fa744077df9d937c4fe36054e9d4>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9a80c5c6458e824bfa0876a779605b0b>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9abb244ecf1fe7549bfa1b3b46427d69>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9ad4dd46fb7ebf522a3e85e715143745>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9ba9dde4e941db1311dbefb477d19165>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9baa1cdad376a56943ca503609534da1>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9c4f15881ef2441e36fb65a5f1a748dd>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9c55cd28535069d632248acbead74665>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9cff58ca0c6e88f847f22b27dae030be>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9d00353fd33377312606dc12e977172a>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9d98759a86a1a6c70a5f6713a8d099cc>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9dc7af5c7bcff785bd9190f2cb464b36>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9ddccf775856736f4137dfc22445dcab>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9e03109a12e41496ab9ad081a718600f>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9ea68d0a3164c0f7bc2786cdbb58d687>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9ef12b090c75e5f2f5ddaa1c0717f98a>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a047e4fa7a019da5da5dcb5093bd2da9>@@XPEBVNode@onnxruntime@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a0f179e1ced4abc8e1523256ebbadd18>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a103bb1e5b04405f4e2459b7d854fdbb>@@MM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a11d3e2153f19eecf5eacc6ad2a1a9c5>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a17d3ee64fefe07bb4c5c931150c8a1b>@@XPEBMPEAM_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a1b033c1238f74f04d136673bf3141a7>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a202458c5f736a51fd6e4cb6bbed2f45>@@VStatus@common@onnxruntime@@AEAV?$shared_ptr@VModel@onnxruntime@@@std@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a3360ab2153a0d5e361146591e0ed940>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a34df399fe5dbf1e2984cce096bb4cbc>@@VStatus@common@onnxruntime@@AEBVNodeArg@4@_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a38954a3fc0168033639a5d3db4af77f>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a47ee9eece330f845238b622e01ff20a>@@XMPEAM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a4df326263f2cb24ed288d1255c6569d>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a4fe3bd39dcd0107f90455cb45c21429>@@_NH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a5bc10369070c00aebb065dae57fa0bc>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a69e02b0210b70ea824684cc65a6da5c>@@VStatus@common@onnxruntime@@AEBV?$vector@_KV?$allocator@_K@std@@@std@@AEBVTensor@4@AEAV74@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a7a0686f23e7f9337eaa10f069ee321b>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a7f558f1dd0bc6dfbc34073d8010b2d2>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a856994ca323bdea896dcaf6811e93e3>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a8dc603fd8183714a0dc357fc9b787c1>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ab278d2c0c4ee1c6b80548eeeee0d417>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ab509a36b64c6185495bed43165d3ae5>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_abe09d436d912a4ff16d1ac6a5002952>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ac9c227ba6c5cb467d27ecbdbba9c871>@@X_K_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_acbad9bc57c95ab4ffc746caf04c813c>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_acccfeb1e0041e26bf01c6097464f8c5>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_adbb0ed82c3e353a2ef8b51f4773ed3d>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ae0d8b5a2da4af67276477f8d17abe42>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_af2c39467b4484bc2a2f611cf533daa7>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_af999ce2e03a2039dbf31c1ff13e0675>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_af9b4834cf87a70d331cb6fc7bbbeee5>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_afd3110d589aa504a36e81b6239b970d>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_aff25a6a369700d46cd3658ce112be53>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b02c4bc922420e968df79ed6aa499235>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b056c5876502bf8728ee42ccf0ca953b>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b079e40837048df263512dcc9441c93c>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b09bcb6c8a5faaa957decc75e6f3e6e4>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b0c700b83742c00837801ebad88797d0>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b123eebc6aab753cdc93a7cd4699b893>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b13b0c3bacb7bf27cf9bf4bac72849bb>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b18b645f3287a76fd9f5d36468b29bf7>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b2b5b00a4ad3647a9a40b4f0dbb0745d>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b31abea9b0adc93fa794390a998569d7>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b32e5f2bc2fafd58a4afcda5e8537c65>@@VStatus@common@onnxruntime@@AEAV?$shared_ptr@VModel@onnxruntime@@@std@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b346f4e5faa3c794cd68f36eadcb5c6c>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b3661b31d86a85c89e2f6a1a0742a349>@@XAEBVNodeArg@onnxruntime@@_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b379f94eb8587c8f8eb06ab04392317e>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b38c51459887501772a6d011e08a4556>@@MMMMMMM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b39fcf5d316099631cb4151a025dde22>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b3cbfd0276cf0ad6f3b58532d60f239f>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b4697c7f06fc1e7e59f4a5183c6a98c3>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b4b1fce215f20fdccd275ca27e2b3255>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b4e1f5c6fdfb97123733fced30ecae42>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b51428820b3477616d25ebf36e7dd645>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b592d9bc8cc53309aec51b3e43d6dad4>@@VStatus@common@onnxruntime@@HAEBUOrtValue@@AEBUOrtCallback@4@_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b63c2e5f4896a6001dfbdd641092de94>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b6aaeef95996305e5431d1fd80eaed87>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b6cf00bbc72a844e74b65b515f429504>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b720b525ca7596ec682823a87c40ac8d>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b77050229503eb6e58b80080f34d3ad8>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b86aa8ef1de0593a5547f08d792c1565>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b8846f4ca0753c507ed6516ddbf99aba>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b8bff811421756f68bf0701ba18268e9>@@XPEBMPEAM_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b937f7d4f097577d4fa0e0107ad25600>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b9ad0ccbf56c00a46342640cba7cdc2c>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ba7c69dcd3c7c32d7eba5d02fb06573e>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bb6f1fa388d8f370137cb6c6519f955b>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bb7b1b0ef71312cebe2ce7f754927441>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bb907a92df9fde70b67de149cb976bb1>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bbed65651851398ae9e91eb41072a5f9>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bbfb6c4f9961aa9de543511f0a00bf20>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bc47dcc73653c7f9cb7b31e06f49f967>@@XAEBVNodeArg@onnxruntime@@_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bc9a9f99cbeabaf2d0a936a0cad6b782>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bca65c99655015407a4683ba6812e592>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bca81c57471aa7b198b5b9c6df12b83d>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bce28680af6d6e63afbcec5203580c30>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bd3b717d06eb8cb8c6cdeb6f2f75dbb9>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_beb16572edee23057a5875bc228caa9d>@@VStatus@common@onnxruntime@@AEBVNodeArg@4@_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_beb5ca5c13f598ae93cc9dc5c5727597>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_beb991e0fd9fa3631b6b17fe3b5d669c>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bfe66c418aacc57ce1b58557a3a20356>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c007df17e8115f08c5c6e2ce7e2201c0>@@VStatus@common@onnxruntime@@AEAVGraph@4@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c1347ce9161a6b5cef4e0bceb24b27d3>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c162c83b8e727b7911e302875a688c52>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c20b2bc71558ce98a32f42bb85de6d28>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c24ed35691635f968e92d6373ba271aa>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c26c4492665ce46ed2b794163ccf01b8>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c273bcba413eb077d5fb961b15132175>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c2d5469345cd97550e4777826a9777d5>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c2fef1cfbd08e9e21bcb74b985df87e6>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c327e8b0febe30d927a0cd8dc39dba92>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c3857b8060a6c3155dc6bb6df09a4841>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c3d54ef5797172fcc9eaba420c8f75e8>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c4806bda3c509fc7c991e936211d831c>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c4d7f6cc60481397c12458592585c6f0>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c5677617c2b0074bdccd3f6596388b76>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c6108df36ce1e471ee073cdcb906363b>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c646dd14edaacae64487e0b96ad575f5>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c6728963f8bb91595df67d14c61fd8fa>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c6e8ce8febb47c28ae5d88eebb07bb81>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c720e61805dcd56f6632856caaf966d8>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c7bc13683ecb5a18d043a137e0218d42>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c81a90897540aaa4b34ce92ab90b53b8>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c89cf3298587f6daa5b2556ad200a769>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c89e6783ea7ba65dd2bbca371d42018d>@@VStatus@common@onnxruntime@@AEAV?$shared_ptr@VModel@onnxruntime@@@std@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c8a671616559eac778e178004f6bff38>@@VStatus@common@onnxruntime@@PEAXPEBUOrtApi@@PEAUOrtKernelContext@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c9d64b63a4c10e55647e3707de8acb40>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ca2e0a0561d13968d35862532b5f82aa>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ca605469c50319317c9a56255477531a>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ca87e505bf8a7e1f26579083c3683d94>@@_NH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ca99611dde14cbfee162988c3da30011>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ca9d22ac690a049ddaf8ded26ec4c6e4>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_cb274b2da89b230e840725fbe13c27e7>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_cc270699afce6708083292447c6ec5a7>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ccbc04edad8bea47ebc5361806e9195e>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_cd453f5abbb4020fb3775475830cd8d1>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ce1101c22724e1357d724096af023481>@@VStatus@common@onnxruntime@@AEBVTensorShape@4@AEBUOrtMemoryInfo@@AEAUOrtValue@@AEA_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ceef0f6dad2176ea59816e5c9d007c22>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_cf0d99f02d80c9fcb9d4fc5032f5b198>@@XMPEAM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_cfa93eededd189da5db0d1b9a54368f6>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_cfb3b6eaf74a6d2adee4cb59404bca02>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_cfdeab09b00be16ca9b756aad9e71d8c>@@XAEBVNode@onnxruntime@@AEAV?$function@$$A6A?AV?$ComPtr@UIMLOperatorTensor@@@WRL@Microsoft@@I@Z@std@@PEBXPEAUDmlGraphNodeCreateInfo@Adapter@MachineLearning@AI@Windows@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d01cb7d72935562762275ef2725e3ce2>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d084e33f090b9a2ef4ac7752a4304e52>@@XAEBVNodeArg@onnxruntime@@_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d0c5713ac46ee06d27e6324a463b7bee>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d12714ea155ac3d05d46fd33bcaf55d0>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d16d0ed0a77c40b384d41d6d45ea4e55>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d2d693a490e9887da5a024c703dc8e3e>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d2e668c941b29e0c58674cef7fa23f2c>@@XPEAX@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d44234032f9da3a8e659111375d4b0b2>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d47c5253ca7eceaa0e1c3b2cf5640fa1>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d48cb9baeb37d51a45fff2e006fd2ed3>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d5d62f4b858abcded92348c54a906570>@@XAEBVNodeArg@onnxruntime@@_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d5dc432e2d4add6c02a3fbdbdb6568b6>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d73cfc3b818335b575efed71024f8847>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d843aae243ab497a4cddae9c413aa41d>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d846c7c2d29b00ef66091f3bac323c48>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d8a9ac16dd775f4bb1f670ae469711dc>@@XPEBVDataTypeImpl@onnxruntime@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d8b06a4dd6f551b1fc6c7009ad0ed428>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d8b96520499b24df75ab37992af46b4d>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d8c2999bab01791ce81720c649701452>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d966a160cb34bd8cd4e5e8dbd91fdf85>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d9adada7822ba67bc618dd6220c7e11e>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_da71868c78371b6753545a4c8498c6e4>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_daa3e6298aefc79ad774de1fc533e850>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dcbbb46dcb2ccb38f1b7e3e89a0ca172>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dccf783802d22b3c48245e980c348c0b>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dd4a8a293eb1e8d9bc247b4bc07121c6>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_df18081ccc34b5e376ea2f099a6c86d1>@@VStatus@common@onnxruntime@@AEBVTensorShape@4@AEBUOrtMemoryInfo@@AEAUOrtValue@@AEA_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_df8f09957380fda37bd75205e74ab017>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dfdbf34c7b75816fc0d7ddf3d1f4736c>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dfdef6da643cd6a11fea2d67407fdc34>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dff178d3d11d9fe36bd8141e8e6f7e2f>@@PEAVOpKernel@onnxruntime@@AEBVOpKernelInfo@3@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e021ada2350e9ccd28f4a55f38144d36>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e0371779d439769322780aabf313f8d1>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e08a75ecff8a2a27b308be3f8dfddc66>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e1a7abf114b955966097bd83c14de705>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e1b9ebecb019a93c9b1d900fc5dac815>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e1f234778fb80305bf70a017789747a5>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e2aaeba0e7fe634207a31efbbb188fef>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e2bb8f55a256e670e2f45d1b39e2618b>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e318b9a1ac6011cec45976779008bc1d>@@VStatus@common@onnxruntime@@AEAVGraph@4@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e34957b95c2645f541b4bd46255fbbb0>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e3b884cd04a55a9aff856d7ab36b0422>@@MMMMMMM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e483d02e0524579d835af30c8a6874b1>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e4af5663d262f6d7cf485c6657cb9e95>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e5304a863af2c1f046e5da60dfc5f195>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e60c0af24df176341124ead910e74b4f>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e77a57374f1e055f8281ce5dd65e8ebd>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e79e85aa06c0020ce8ba4eced08c2c92>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e89cae078ae99afbefebcbd69b7a7f5c>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e8b80a28a395cc4498887c668735db69>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e8dabe6ee14f0907c4413646f291102d>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e90c0bd946d9218d9f45b675a7b78c24>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e971bf56206beddda75940cb23d81d7c>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e9e8bddd64a71567d96eadd0fe1d2a17>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ea73e250586924fb71aef6e9068c74b3>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_eb6625ff603571627d15a6f97beef5e6>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_eb887415aed41a3927a754246db16d35>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_eb9c4de1a9cbbc50c248fee15cd2820e>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ec38228ee8b8eb97ec41ac102234721a>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ecee2654318c52376f803bc94d6bc4ee>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ed05756275fe268c82e7c72f39e62f2a>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ed53b4c0957e241f93b3be9c259f7b0d>@@VStatus@common@onnxruntime@@PEAX_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ee50a4ed472e80d4977c99ba85a1d7df>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ee5e1bfaf2a0db686c570e5f95f6aa95>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_eedfabccf04dae8365cd4a4775f4c12a>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ef997e7e75a3ce38d15a0a98d436f03c>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ef9ce4d0dc32ab06ddd4426fedb787f1>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_efb13713d16ec9cd372290e4d816e310>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f0421a9025619b3b88dddf09f010b1ec>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f1fa811d25327e50fa43acb91e2eb6a4>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f273d4d4788dd6d522544c26d76a5ae4>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f29d64af0687bfdd6e3496c166444014>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f30a6117225d3cb8e4a11f826263726e>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f3baf3c6fac0ad86964ff832e3055849>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f4b2ad37b32ce448782bd96a2cfadb78>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f4c664674d3291639f6f1c524accc00e>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f51c3a2a2774821bc4773e2abbaa11b4>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f55fed088cc6887bf534564a0809769f>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f588fb4ed0c783dc6991c6110b1943e5>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f5db0169896386e7220581461257fa02>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f60a4fa32157d6fa8e2622fa2d06d23c>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f6391278f73df45e5c03a1b0ebcd91ef>@@XI@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f6ba89529e75bbc8ce63885b5ef6f940>@@XPEBMPEAM_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f6ee07f46f90bd0fd4cd2ececdbe52e0>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f76c1721a2f76590ee1ac32dd1a7059b>@@MM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f78d8769dd40946ef61ad5956cbf10fe>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f79206f00cb74e35e11935d1c0ff9449>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f8241a17ccaf97c109b664362fd083d5>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f87191d92810657cd25a2973b1734a59>@@VStatus@common@onnxruntime@@PEAX_K@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f8dbfaed9d55440bb15cbf009ac934a6>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f90b302c32ce6f81af3f56ee1f3a9b51>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f92ea10dca143475672352184887f31b>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fa9cea3a5917d32ed85305ced3e8f02e>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_faa349663837247e8666c82fc4f50c4f>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fb0df1f24c66125b0dcb6e0f4b2b8c09>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fb1d0169209b6e282abb9c9575470f62>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fbddeeb35b871ddb923a0706e4876ef6>@@XAEAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fc59cfd6180e7962f9dbfcf6d01970a3>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fc7d1f6c2adcff66a9b847972f40b566>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fcb655464ad0e43ce2671f06c665bb0e>@@VStatus@common@onnxruntime@@AEAV?$shared_ptr@VModel@onnxruntime@@@std@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fd07f717d8be7a0b4ac28d8863ca11e3>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fd266a9166d0b10d35a0d8f14994daa2>@@XAEAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fd2f32690f4c94955691e93dd0de8250>@@X_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fd4f2bbf00caf507fea1cc6053ab984e>@@X_J_J@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ffee7b280847cd780e3c0aef3f3c198c>@@_JM_N@std@@
.?AV?$_Iosb@H@std@@
.?AV?$_Mpunct@_W@std@@
.?AV?$_Mpunct@D@std@@
.?AV?$_Mpunct@G@std@@
.?AV?$_Ref_count@VModel@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@UCpuProviderFactory@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@UDMLProviderFactory@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@UInternalRegistrationInfo@Adapter@MachineLearning@AI@Windows@@@std@@
.?AV?$_Ref_count_obj2@V?$unordered_map@PEAVKernelDef@onnxruntime@@V?$shared_ptr@UInternalRegistrationInfo@Adapter@MachineLearning@AI@Windows@@@std@@U?$hash@PEAVKernelDef@onnxruntime@@@4@U?$equal_to@PEAVKernelDef@onnxruntime@@@4@V?$allocator@U?$pair@QEAVKernelDef@onnxruntime@@V?$shared_ptr@UInternalRegistrationInfo@Adapter@MachineLearning@AI@Windows@@@std@@@std@@@4@@std@@@std@@
.?AV?$_Ref_count_obj2@V?$unordered_map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@UFuncInfo@FuncManager@onnxruntime@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@UFuncInfo@FuncManager@onnxruntime@@@std@@@2@@std@@@std@@
.?AV?$_Ref_count_obj2@V?$unordered_map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VTensorProto@onnx@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VTensorProto@onnx@@@std@@@2@@std@@@std@@
.?AV?$_Ref_count_obj2@VAllocatorManager@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@VAllocatorWrapper@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@VBucketizedBufferAllocator@Dml@@@std@@
.?AV?$_Ref_count_obj2@VCommandQueue@Dml@@@std@@
.?AV?$_Ref_count_obj2@VCPUAllocator@Dml@@@std@@
.?AV?$_Ref_count_obj2@VCustomRegistry@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@VExecutionContext@Dml@@@std@@
.?AV?$_Ref_count_obj2@Vinput_buffer_adapter@detail@nlohmann@@@std@@
.?AV?$_Ref_count_obj2@VKernelRegistry@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@VModel@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@VOnnxRuntimeOpSchemaRegistry@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@VSchemaRegistryManager@onnxruntime@@@std@@
.?AV?$_Ref_count_resource@PEAVBFCArena@onnxruntime@@U?$default_delete@VBFCArena@onnxruntime@@@std@@@std@@
.?AV?$_Ref_count_resource@PEAVIAllocator@onnxruntime@@U?$default_delete@VIAllocator@onnxruntime@@@std@@@std@@
.?AV?$_Ref_count_resource@PEAVModel@onnxruntime@@U?$default_delete@VModel@onnxruntime@@@std@@@std@@
.?AV?$_Ref_count_resource@PEAXP6AXPEAX@Z@std@@
.?AV?$Acos@M@onnxruntime@@
.?AV?$Acosh@M@onnxruntime@@
.?AV?$Add@_J@onnxruntime@@
.?AV?$Add@H@onnxruntime@@
.?AV?$Add@M@onnxruntime@@
.?AV?$Add@N@onnxruntime@@
.?AV?$Affine@M@contrib@onnxruntime@@
.?AV?$ArgMax@H@onnxruntime@@
.?AV?$ArgMax@M@onnxruntime@@
.?AV?$ArgMax@N@onnxruntime@@
.?AV?$ArgMin@H@onnxruntime@@
.?AV?$ArgMin@M@onnxruntime@@
.?AV?$ArgMin@N@onnxruntime@@
.?AV?$ArrayFeatureExtractorOp@_J@ml@onnxruntime@@
.?AV?$ArrayFeatureExtractorOp@H@ml@onnxruntime@@
.?AV?$ArrayFeatureExtractorOp@M@ml@onnxruntime@@
.?AV?$ArrayFeatureExtractorOp@N@ml@onnxruntime@@
.?AV?$ArrayFeatureExtractorOp@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@ml@onnxruntime@@
.?AV?$Asin@M@onnxruntime@@
.?AV?$Asinh@M@onnxruntime@@
.?AV?$Atan@M@onnxruntime@@
.?AV?$Atanh@M@onnxruntime@@
.?AV?$Attention@M@contrib@onnxruntime@@
.?AV?$AttentionWrapper@M@contrib@onnxruntime@@
.?AV?$BahdanauAttention@M@contrib@onnxruntime@@
.?AV?$basic_filebuf@DU?$char_traits@D@std@@@std@@
.?AV?$basic_ifstream@DU?$char_traits@D@std@@@std@@
.?AV?$basic_ios@_WU?$char_traits@_W@std@@@std@@
.?AV?$basic_ios@DU?$char_traits@D@std@@@std@@
.?AV?$basic_iostream@DU?$char_traits@D@std@@@std@@
.?AV?$basic_istream@DU?$char_traits@D@std@@@std@@
.?AV?$basic_ofstream@DU?$char_traits@D@std@@@std@@
.?AV?$basic_ostream@_WU?$char_traits@_W@std@@@std@@
.?AV?$basic_ostream@DU?$char_traits@D@std@@@std@@
.?AV?$basic_ostringstream@_WU?$char_traits@_W@std@@V?$allocator@_W@2@@std@@
.?AV?$basic_ostringstream@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@
.?AV?$basic_streambuf@_WU?$char_traits@_W@std@@@std@@
.?AV?$basic_streambuf@DU?$char_traits@D@std@@@std@@
.?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@
.?AV?$basic_stringbuf@_WU?$char_traits@_W@std@@V?$allocator@_W@2@@std@@
.?AV?$basic_stringbuf@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@
.?AV?$basic_stringstream@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@
.?AV?$BatchNorm@M@onnxruntime@@
.?AV?$BatchNorm@N@onnxruntime@@
.?AV?$BiasGelu@M$00@contrib@onnxruntime@@
.?AV?$BiasGelu@M$0A@@contrib@onnxruntime@@
.?AV?$BinarizerOp@M@ml@onnxruntime@@
.?AV?$BitShift@_K@onnxruntime@@
.?AV?$BitShift@E@onnxruntime@@
.?AV?$BitShift@I@onnxruntime@@
.?AV?$CDist@M@contrib@onnxruntime@@
.?AV?$CDist@N@contrib@onnxruntime@@
.?AV?$Clip_6@M@onnxruntime@@
.?AV?$Clip_6Base@M@clip_internal@onnxruntime@@
.?AV?$codecvt@_WDU_Mbstatet@@@std@@
.?AV?$codecvt@DDU_Mbstatet@@@std@@
.?AV?$codecvt@GDU_Mbstatet@@@std@@
.?AV?$codecvt_utf8@_W$0BAPPPP@$0A@@std@@
.?AV?$collate@_W@std@@
.?AV?$collate@D@std@@
.?AV?$collate@G@std@@
.?AV?$ConstantOfShapeBase@U?$TypeList@UMLFloat16@onnxruntime@@MNCFH_JEGI_K_N@onnxruntime@@@onnxruntime@@
.?AV?$Conv@M@onnxruntime@@
.?AV?$ConvTranspose@M@onnxruntime@@
.?AV?$ConvTransposeWithDynamicPads@M@contrib@onnxruntime@@
.?AV?$Cos@M@onnxruntime@@
.?AV?$Cosh@M@onnxruntime@@
.?AV?$Crop@M@contrib@onnxruntime@@
.?AV?$CropAndResize@M@contrib@onnxruntime@@
.?AV?$ctype@_W@std@@
.?AV?$ctype@D@std@@
.?AV?$ctype@G@std@@
.?AV?$CumSum@_J@onnxruntime@@
.?AV?$CumSum@H@onnxruntime@@
.?AV?$CumSum@M@onnxruntime@@
.?AV?$CumSum@N@onnxruntime@@
.?AV?$DepthToSpace@M@onnxruntime@@
.?AV?$DequantizeLinear@C@onnxruntime@@
.?AV?$DequantizeLinear@E@onnxruntime@@
.?AV?$DequantizeLinear@H@onnxruntime@@
.?AV?$Det@M@onnxruntime@@
.?AV?$DictVectorizerOp@_JM@ml@onnxruntime@@
.?AV?$DictVectorizerOp@_JN@ml@onnxruntime@@
.?AV?$DictVectorizerOp@_JV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@ml@onnxruntime@@
.?AV?$DictVectorizerOp@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_J@ml@onnxruntime@@
.?AV?$DictVectorizerOp@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@ml@onnxruntime@@
.?AV?$DictVectorizerOp@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@N@ml@onnxruntime@@
.?AV?$Div@_J@onnxruntime@@
.?AV?$Div@H@onnxruntime@@
.?AV?$Div@M@onnxruntime@@
.?AV?$Div@N@onnxruntime@@
.?AV?$DmlOperatorActivationTemplate@$0CD@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CE@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CF@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CG@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CH@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CJ@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CK@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CL@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CM@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CN@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CO@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CP@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0DA@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0DB@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0DC@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0DD@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0DE@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0FK@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0IA@@Dml@@
.?AV?$DmlOperatorConvolutionTemplate@$00$00$00@Dml@@
.?AV?$DmlOperatorConvolutionTemplate@$00$00$0A@@Dml@@
.?AV?$DmlOperatorConvolutionTemplate@$00$0A@$0A@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_ADD_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_ADD1_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_DIVIDE_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_AND_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_EQUALS_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_GREATER_THAN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_GREATER_THAN_OR_EQUAL_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_LESS_THAN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_LESS_THAN_OR_EQUAL_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_OR_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_XOR_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_MULTIPLY_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_SUBTRACT_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_ADD_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_ADD1_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_MAX_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_MIN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseQLinear@UDML_ELEMENT_WISE_DEQUANTIZE_LINEAR_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseQLinear@UDML_ELEMENT_WISE_QUANTIZE_LINEAR_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ABS_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ACOS_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ACOSH_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ASIN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ASINH_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ATAN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ATANH_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_CEIL_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_COS_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_COSH_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ERF_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_EXP_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_FLOOR_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_IS_NAN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_LOG_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_LOGICAL_NOT_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_RECIP_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SIGN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SIN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SINH_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SQRT_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_TAN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorPoolingTemplate@$0DI@$00@Dml@@
.?AV?$DmlOperatorPoolingTemplate@$0DI@$0A@@Dml@@
.?AV?$DmlOperatorPoolingTemplate@$0DJ@$00@Dml@@
.?AV?$DmlOperatorPoolingTemplate@$0DJ@$0A@@Dml@@
.?AV?$DmlOperatorPoolingTemplate@$0DK@$00@Dml@@
.?AV?$DmlOperatorPoolingTemplate@$0GO@$0A@@Dml@@
.?AV?$DmlOperatorReduceTemplate@$00@Dml@@
.?AV?$DmlOperatorReduceTemplate@$01@Dml@@
.?AV?$DmlOperatorReduceTemplate@$02@Dml@@
.?AV?$DmlOperatorReduceTemplate@$03@Dml@@
.?AV?$DmlOperatorReduceTemplate@$04@Dml@@
.?AV?$DmlOperatorReduceTemplate@$05@Dml@@
.?AV?$DmlOperatorReduceTemplate@$06@Dml@@
.?AV?$DmlOperatorReduceTemplate@$07@Dml@@
.?AV?$DmlOperatorReduceTemplate@$08@Dml@@
.?AV?$DmlOperatorReduceTemplate@$09@Dml@@
.?AV?$DmlOperatorReduceTemplate@$0A@@Dml@@
.?AV?$DmlOperatorReduceTemplate@$0L@@Dml@@
.?AV?$Dropout@MM@onnxruntime@@
.?AV?$Dropout@MN@onnxruntime@@
.?AV?$Dropout@NM@onnxruntime@@
.?AV?$Dropout@NN@onnxruntime@@
.?AV?$DynamicQuantizeLinear@E@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@_J@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@_K@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@C@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@E@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@F@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@G@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@H@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@I@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Ceil@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Elu@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Exp@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Exp@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Floor@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$HardSigmoid@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$LeakyRelu@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Log@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Log@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Neg@_J@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Neg@C@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Neg@H@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Neg@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Neg@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$ParametricSoftplus@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Reciprocal@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Reciprocal@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Relu@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Relu@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$ScaledTanh@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Selu@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Sigmoid@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Sigmoid@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Softplus@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Softsign@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Sqrt@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Sqrt@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Tanh@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Tanh@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$ThresholdedRelu@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$EmbedLayerNorm@M@contrib@onnxruntime@@
.?AV?$Equal@_J@onnxruntime@@
.?AV?$Equal@_N@onnxruntime@@
.?AV?$Equal@H@onnxruntime@@
.?AV?$Equal@M@onnxruntime@@
.?AV?$Equal@N@onnxruntime@@
.?AV?$Erf@M@onnxruntime@@
.?AV?$Expand@_J@onnxruntime@@
.?AV?$Expand@_K@onnxruntime@@
.?AV?$Expand@_N@onnxruntime@@
.?AV?$Expand@C@onnxruntime@@
.?AV?$Expand@E@onnxruntime@@
.?AV?$Expand@F@onnxruntime@@
.?AV?$Expand@G@onnxruntime@@
.?AV?$Expand@H@onnxruntime@@
.?AV?$Expand@I@onnxruntime@@
.?AV?$Expand@M@onnxruntime@@
.?AV?$Expand@N@onnxruntime@@
.?AV?$Expand@UMLFloat16@onnxruntime@@@onnxruntime@@
.?AV?$Expand_8@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@onnxruntime@@
.?AV?$FusedGemm@M@contrib@onnxruntime@@
.?AV?$Gelu@M@contrib@onnxruntime@@
.?AV?$Gemm@M@onnxruntime@@
.?AV?$Gemm@N@onnxruntime@@
.?AV?$Greater@_J@onnxruntime@@
.?AV?$Greater@H@onnxruntime@@
.?AV?$Greater@M@onnxruntime@@
.?AV?$Greater@N@onnxruntime@@
.?AV?$Hardmax@M@onnxruntime@@
.?AV?$IAttentionMechanism@M@contrib@onnxruntime@@
.?AV?$IdentityOp@$00@onnxruntime@@
.?AV?$IdentityOp@$0A@@onnxruntime@@
.?AV?$ImageScaler@M@contrib@onnxruntime@@
.?AV?$InstanceNorm@M@onnxruntime@@
.?AV?$IsNaN@M@onnxruntime@@
.?AV?$IsNaN@UMLFloat16@onnxruntime@@@onnxruntime@@
.?AV?$LabelEncoder_2@_J_J@ml@onnxruntime@@
.?AV?$LabelEncoder_2@_JM@ml@onnxruntime@@
.?AV?$LabelEncoder_2@_JV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@ml@onnxruntime@@
.?AV?$LabelEncoder_2@M_J@ml@onnxruntime@@
.?AV?$LabelEncoder_2@MV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@ml@onnxruntime@@
.?AV?$LabelEncoder_2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_J@ml@onnxruntime@@
.?AV?$LabelEncoder_2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@ml@onnxruntime@@
.?AV?$LayerNorm@M$00@contrib@onnxruntime@@
.?AV?$LayerNorm@M$0A@@contrib@onnxruntime@@
.?AV?$LayerNorm@N$00@contrib@onnxruntime@@
.?AV?$LayerNorm@N$0A@@contrib@onnxruntime@@
.?AV?$Less@_J@onnxruntime@@
.?AV?$Less@H@onnxruntime@@
.?AV?$Less@M@onnxruntime@@
.?AV?$Less@N@onnxruntime@@
.?AV?$LpNorm@M@onnxruntime@@
.?AV?$LpNorm@N@onnxruntime@@
.?AV?$LRN@M@onnxruntime@@
.?AV?$map@_JMU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JM@std@@@2@@std@@
.?AV?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@MU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@std@@@2@@std@@
.?AV?$MapType@V?$map@_J_JU?$less@_J@std@@V?$allocator@U?$pair@$$CB_J_J@std@@@2@@std@@@onnxruntime@@
.?AV?$MapType@V?$map@_JMU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JM@std@@@2@@std@@@onnxruntime@@
.?AV?$MapType@V?$map@_JNU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JN@std@@@2@@std@@@onnxruntime@@
.?AV?$MapType@V?$map@_JV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$less@_J@2@V?$allocator@U?$pair@$$CB_JV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@std@@@2@@std@@@onnxruntime@@
.?AV?$MapType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_JU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_J@std@@@2@@std@@@onnxruntime@@
.?AV?$MapType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@MU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@std@@@2@@std@@@onnxruntime@@
.?AV?$MapType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@NU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@N@std@@@2@@std@@@onnxruntime@@
.?AV?$MapType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V12@U?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V12@@std@@@2@@std@@@onnxruntime@@
.?AV?$MatMul@_J@onnxruntime@@
.?AV?$MatMul@H@onnxruntime@@
.?AV?$MatMul@M@onnxruntime@@
.?AV?$MatMul@N@onnxruntime@@
.?AV?$MatMulInteger16@FFH@contrib@onnxruntime@@
.?AV?$Max_6@M@onnxruntime@@
.?AV?$Mean_6@M@onnxruntime@@
.?AV?$Mean_8@M@onnxruntime@@
.?AV?$MeanVarianceNormalization_0@M@onnxruntime@@
.?AV?$MeanVarianceNormalization_1@M@onnxruntime@@
.?AV?$messages@_W@std@@
.?AV?$messages@D@std@@
.?AV?$messages@G@std@@
.?AV?$Min_6@M@onnxruntime@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CD@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CE@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CF@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CG@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CH@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CJ@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CK@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CL@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CM@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CN@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CO@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CP@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0DA@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0DB@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0DC@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0DD@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0DE@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0FK@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0IA@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorConvolutionTemplate@$00$00$00@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorConvolutionTemplate@$00$00$0A@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorConvolutionTemplate@$00$0A@$0A@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_ADD_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_ADD1_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_DIVIDE_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_AND_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_EQUALS_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_GREATER_THAN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_GREATER_THAN_OR_EQUAL_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_LESS_THAN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_LESS_THAN_OR_EQUAL_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_OR_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_XOR_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_MULTIPLY_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_SUBTRACT_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_ADD_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_ADD1_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_MAX_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_MIN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseQLinear@UDML_ELEMENT_WISE_DEQUANTIZE_LINEAR_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseQLinear@UDML_ELEMENT_WISE_QUANTIZE_LINEAR_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ABS_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ACOS_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ACOSH_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ASIN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ASINH_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ATAN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ATANH_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_CEIL_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_COS_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_COSH_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ERF_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_EXP_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_FLOOR_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_IS_NAN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_LOG_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_LOGICAL_NOT_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_RECIP_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SIGN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SIN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SINH_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SQRT_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_TAN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorPoolingTemplate@$0DI@$00@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorPoolingTemplate@$0DI@$0A@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorPoolingTemplate@$0DJ@$00@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorPoolingTemplate@$0DJ@$0A@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorPoolingTemplate@$0DK@$00@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorPoolingTemplate@$0GO@$0A@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$00@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$01@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$02@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$03@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$04@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$05@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$06@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$07@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$08@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$09@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$0A@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$0L@@Dml@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorEinSum@Dml@@$0M@@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorPadding@Dml@@$06@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorPadding@Dml@@$0L@@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorRegionOfInterestAlign@Dml@@$09@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorResize@Dml@@$06@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorResize@Dml@@$08@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorResize@Dml@@$09@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorResize@Dml@@$0L@@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorSlice@Dml@@$06@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorSlice@Dml@@$09@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorSlice@Dml@@$0L@@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorTopK@Dml@@$06@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorTopK@Dml@@$09@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorTopK@Dml@@$0L@@@@@
.?AV?$MLOperatorKernel@VDmlOperatorAffine@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorBatchNormalization@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorCast@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorConcat@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorConstantOfShape@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorConvInteger@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorCopy@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorCrop@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorCumSum@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorDepthToSpace@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorDynamicQuantizeLinear@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseBitShift@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseClip11@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseClip7@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseIf@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseIsInf@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseMean@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseMod@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwisePow@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseRound@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorExpand@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorEyeLike@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorGatedRecurrentUnit@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorGather@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorGatherElements@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorGatherNd@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorGemm@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorInstanceNormalization@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorLocalResponseNormalization@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorLongShortTermUnit@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorLpNormalization@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorMatMul@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorMatMulInteger@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorMaxUnpool@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorMeanVarNormalization@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorMemcpy@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorNeg@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorOneHot@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorQLinearAdd@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorQLinearConv@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorQLinearMatMul@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorRange@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorRecurrentNeuralNetwork@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorRegionOfInterestPooling@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorReverseSequence@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorScatter@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorScatterNd@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorSpaceToDepth@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorSplit@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorTile@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorTranspose@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorValueScale2d@Dml@@@@
.?AV?$money_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$money_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$money_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$money_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$money_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$money_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$moneypunct@_W$00@std@@
.?AV?$moneypunct@_W$0A@@std@@
.?AV?$moneypunct@D$00@std@@
.?AV?$moneypunct@D$0A@@std@@
.?AV?$moneypunct@G$00@std@@
.?AV?$moneypunct@G$0A@@std@@
.?AV?$Mul@_J@onnxruntime@@
.?AV?$Mul@H@onnxruntime@@
.?AV?$Mul@M@onnxruntime@@
.?AV?$Mul@N@onnxruntime@@
.?AV?$NonTensorType@V?$map@_J_JU?$less@_J@std@@V?$allocator@U?$pair@$$CB_J_J@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$map@_JMU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JM@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$map@_JNU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JN@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$map@_JV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$less@_J@2@V?$allocator@U?$pair@$$CB_JV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_JU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_J@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@MU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@NU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@N@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V12@U?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V12@@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$vector@V?$map@_JMU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JM@std@@@2@@std@@V?$allocator@V?$map@_JMU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JM@std@@@2@@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$vector@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@MU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@std@@@2@@std@@V?$allocator@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@MU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@std@@@2@@std@@@2@@std@@@onnxruntime@@
.?AV?$NonZero@_J@onnxruntime@@
.?AV?$NonZero@_N@onnxruntime@@
.?AV?$NonZero@E@onnxruntime@@
.?AV?$NonZero@H@onnxruntime@@
.?AV?$NonZero@M@onnxruntime@@
.?AV?$num_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$num_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$num_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$num_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$num_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$num_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$numpunct@_W@std@@
.?AV?$numpunct@D@std@@
.?AV?$numpunct@G@std@@
.?AV?$OneHotEncoderOp@_J@ml@onnxruntime@@
.?AV?$OneHotEncoderOp@M@ml@onnxruntime@@
.?AV?$OneHotEncoderOp@N@ml@onnxruntime@@
.?AV?$OneHotEncoderOp@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@ml@onnxruntime@@
.?AV?$OneHotOp@_J_J_J@onnxruntime@@
.?AV?$OneHotOp@_JHM@onnxruntime@@
.?AV?$OneHotOp@_JM_J@onnxruntime@@
.?AV?$OneHotOp@_JMH@onnxruntime@@
.?AV?$OneHotOp@_JMM@onnxruntime@@
.?AV?$OneHotOp@_JV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_J@onnxruntime@@
.?AV?$OneHotOp@HMH@onnxruntime@@
.?AV?$OneHotOp@HMM@onnxruntime@@
.?AV?$OneHotOp@M_J_J@onnxruntime@@
.?AV?$OneHotOp@MMM@onnxruntime@@
.?AV?$OneHotOp@MV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_J@onnxruntime@@
.?AV?$OpNodeInfoWrapper@UInferenceContext@onnx@@V?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTypeInferenceContext@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@WRL@Microsoft@@Unull_type@onnxruntime@@@Adapter@MachineLearning@AI@Windows@@
.?AV?$OpNodeInfoWrapper@VProtoHelperNodeContext@onnxruntime@@V?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorKernelCreationContextPrivate@@UIMLOperatorKernelCreationContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTensorShapeDescription@@UIMLOperatorAttributes1@@@WRL@Microsoft@@Unull_type@2@@Adapter@MachineLearning@AI@Windows@@
.?AV?$OpNodeInfoWrapper@VProtoHelperNodeContext@onnxruntime@@V?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@WRL@Microsoft@@Unull_type@2@@Adapter@MachineLearning@AI@Windows@@
.?AV?$OpNodeInfoWrapper@VProtoHelperNodeContext@onnxruntime@@V?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorSupportQueryContextPrivate@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@VNil@Details@WRL@Microsoft@@V4567@V4567@V4567@V4567@V4567@V4567@@23@@WRL@Microsoft@@Unull_type@2@@Adapter@MachineLearning@AI@Windows@@
.?AV?$Pool@MV?$MaxPool@$00@onnxruntime@@@onnxruntime@@
.?AV?$Pool@MVAveragePool@onnxruntime@@@onnxruntime@@
.?AV?$Pool@MVLpPool@onnxruntime@@@onnxruntime@@
.?AV?$PRelu@M@onnxruntime@@
.?AV?$PrimitiveDataType@_J@onnxruntime@@
.?AV?$PrimitiveDataType@_K@onnxruntime@@
.?AV?$PrimitiveDataType@_N@onnxruntime@@
.?AV?$PrimitiveDataType@C@onnxruntime@@
.?AV?$PrimitiveDataType@E@onnxruntime@@
.?AV?$PrimitiveDataType@F@onnxruntime@@
.?AV?$PrimitiveDataType@G@onnxruntime@@
.?AV?$PrimitiveDataType@H@onnxruntime@@
.?AV?$PrimitiveDataType@I@onnxruntime@@
.?AV?$PrimitiveDataType@M@onnxruntime@@
.?AV?$PrimitiveDataType@N@onnxruntime@@
.?AV?$PrimitiveDataType@UBFloat16@onnxruntime@@@onnxruntime@@
.?AV?$PrimitiveDataType@UMLFloat16@onnxruntime@@@onnxruntime@@
.?AV?$PrimitiveDataType@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@onnxruntime@@
.?AV?$QAttention@M@contrib@onnxruntime@@
.?AV?$QLinearAdd@C@contrib@onnxruntime@@
.?AV?$QLinearAdd@E@contrib@onnxruntime@@
.?AV?$QLinearLeakyRelu@C@contrib@onnxruntime@@
.?AV?$QLinearLeakyRelu@E@contrib@onnxruntime@@
.?AV?$QLinearLookupBase@C@contrib@onnxruntime@@
.?AV?$QLinearLookupBase@E@contrib@onnxruntime@@
.?AV?$QLinearMul@C@contrib@onnxruntime@@
.?AV?$QLinearMul@E@contrib@onnxruntime@@
.?AV?$QLinearSigmoid@C@contrib@onnxruntime@@
.?AV?$QLinearSigmoid@E@contrib@onnxruntime@@
.?AV?$QuantizeLinear@C@onnxruntime@@
.?AV?$QuantizeLinear@E@onnxruntime@@
.?AV?$ReduceKernel@$00@onnxruntime@@
.?AV?$ReduceKernel@$0A@@onnxruntime@@
.?AV?$ReduceKernelBase@$00@onnxruntime@@
.?AV?$ReduceKernelBase@$0A@@onnxruntime@@
.?AV?$ReduceL1@H@onnxruntime@@
.?AV?$ReduceL1@M@onnxruntime@@
.?AV?$ReduceL2@H@onnxruntime@@
.?AV?$ReduceL2@M@onnxruntime@@
.?AV?$ReduceLogSum@H@onnxruntime@@
.?AV?$ReduceLogSum@M@onnxruntime@@
.?AV?$ReduceLogSumExp@H@onnxruntime@@
.?AV?$ReduceLogSumExp@M@onnxruntime@@
.?AV?$ReduceLogSumExp@N@onnxruntime@@
.?AV?$ReduceMax@_J@onnxruntime@@
.?AV?$ReduceMax@C@onnxruntime@@
.?AV?$ReduceMax@E@onnxruntime@@
.?AV?$ReduceMax@H@onnxruntime@@
.?AV?$ReduceMax@M@onnxruntime@@
.?AV?$ReduceMax@N@onnxruntime@@
.?AV?$ReduceMean@H@onnxruntime@@
.?AV?$ReduceMean@M@onnxruntime@@
.?AV?$ReduceMean@N@onnxruntime@@
.?AV?$ReduceMin@_J@onnxruntime@@
.?AV?$ReduceMin@C@onnxruntime@@
.?AV?$ReduceMin@E@onnxruntime@@
.?AV?$ReduceMin@H@onnxruntime@@
.?AV?$ReduceMin@M@onnxruntime@@
.?AV?$ReduceMin@N@onnxruntime@@
.?AV?$ReduceProd@_J@onnxruntime@@
.?AV?$ReduceProd@H@onnxruntime@@
.?AV?$ReduceProd@M@onnxruntime@@
.?AV?$ReduceSum@_J@onnxruntime@@
.?AV?$ReduceSum@H@onnxruntime@@
.?AV?$ReduceSum@M@onnxruntime@@
.?AV?$ReduceSum@N@onnxruntime@@
.?AV?$ReduceSumSquare@H@onnxruntime@@
.?AV?$ReduceSumSquare@M@onnxruntime@@
.?AV?$ReduceSumSquare@N@onnxruntime@@
.?AV?$Resize@E@onnxruntime@@
.?AV?$Resize@H@onnxruntime@@
.?AV?$Resize@M@onnxruntime@@
.?AV?$RNN@M@onnxruntime@@
.?AV?$RoiAlign@M@onnxruntime@@
.?AV?$RoiAlign@N@onnxruntime@@
.?AV?$RoiPool@M@onnxruntime@@
.?AV?$Round@M@onnxruntime@@
.?AV?$Round@N@onnxruntime@@
.?AV?$Round@UMLFloat16@onnxruntime@@@onnxruntime@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorKernelCreationContextPrivate@@UIMLOperatorKernelCreationContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTensorShapeDescription@@UIMLOperatorAttributes1@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTypeInferenceContext@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorSupportQueryContextPrivate@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@VNil@Details@WRL@Microsoft@@V4567@V4567@V4567@V4567@V4567@V4567@@23@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIExecutionProvider@Dml@@UIWinmlExecutionProvider@Adapter@MachineLearning@AI@Windows@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIMLOperatorKernel@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIMLOperatorKernelContext@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIMLOperatorKernelFactory@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIMLOperatorRegistry@@UIMLOperatorRegistryPrivate@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIMLOperatorShapeInferrer@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIMLOperatorSupportQueryPrivate@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIMLOperatorTensor@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIUnknown@@@WRL@Microsoft@@
.?AV?$RuntimeClassBaseT@$01@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00U?$ChainInterfaces@UIMLOperatorKernelCreationContextPrivate@@UIMLOperatorKernelCreationContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTensorShapeDescription@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTypeInferenceContext@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00U?$ChainInterfaces@UIMLOperatorSupportQueryContextPrivate@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@VNil@Details@WRL@Microsoft@@V4567@V4567@V4567@V4567@V4567@V4567@@23@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIExecutionProvider@Dml@@UIWinmlExecutionProvider@Adapter@MachineLearning@AI@Windows@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIMLOperatorKernel@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIMLOperatorKernelContext@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIMLOperatorKernelFactory@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIMLOperatorRegistry@@UIMLOperatorRegistryPrivate@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIMLOperatorShapeInferrer@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIMLOperatorSupportQueryPrivate@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIMLOperatorTensor@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIUnknown@@@Details@WRL@Microsoft@@
.?AV?$SampleOp@M@contrib@onnxruntime@@
.?AV?$Scale@M@contrib@onnxruntime@@
.?AV?$ScalerOp@_J@ml@onnxruntime@@
.?AV?$ScalerOp@H@ml@onnxruntime@@
.?AV?$ScalerOp@M@ml@onnxruntime@@
.?AV?$ScalerOp@N@ml@onnxruntime@@
.?AV?$Scan@$07@onnxruntime@@
.?AV?$Scan@$08@onnxruntime@@
.?AV?$SequenceTensorType@_J@onnxruntime@@
.?AV?$SequenceTensorType@_K@onnxruntime@@
.?AV?$SequenceTensorType@_N@onnxruntime@@
.?AV?$SequenceTensorType@C@onnxruntime@@
.?AV?$SequenceTensorType@E@onnxruntime@@
.?AV?$SequenceTensorType@F@onnxruntime@@
.?AV?$SequenceTensorType@G@onnxruntime@@
.?AV?$SequenceTensorType@H@onnxruntime@@
.?AV?$SequenceTensorType@I@onnxruntime@@
.?AV?$SequenceTensorType@M@onnxruntime@@
.?AV?$SequenceTensorType@N@onnxruntime@@
.?AV?$SequenceTensorType@UBFloat16@onnxruntime@@@onnxruntime@@
.?AV?$SequenceTensorType@UMLFloat16@onnxruntime@@@onnxruntime@@
.?AV?$SequenceTensorType@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@onnxruntime@@
.?AV?$SequenceType@V?$vector@V?$map@_JMU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JM@std@@@2@@std@@V?$allocator@V?$map@_JMU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JM@std@@@2@@std@@@2@@std@@@onnxruntime@@
.?AV?$SequenceType@V?$vector@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@MU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@std@@@2@@std@@V?$allocator@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@MU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@std@@@2@@std@@@2@@std@@@onnxruntime@@
.?AV?$Sin@M@onnxruntime@@
.?AV?$Sin@N@onnxruntime@@
.?AV?$Sinh@M@onnxruntime@@
.?AV?$SkipLayerNorm@M@contrib@onnxruntime@@
.?AV?$SkipLayerNorm@N@contrib@onnxruntime@@
.?AV?$Softmax@M@onnxruntime@@
.?AV?$Softmax@N@onnxruntime@@
.?AV?$SpaceToDepth@M@onnxruntime@@
.?AV?$SparseTensorType@_J@onnxruntime@@
.?AV?$SparseTensorType@_K@onnxruntime@@
.?AV?$SparseTensorType@_N@onnxruntime@@
.?AV?$SparseTensorType@C@onnxruntime@@
.?AV?$SparseTensorType@E@onnxruntime@@
.?AV?$SparseTensorType@F@onnxruntime@@
.?AV?$SparseTensorType@G@onnxruntime@@
.?AV?$SparseTensorType@H@onnxruntime@@
.?AV?$SparseTensorType@I@onnxruntime@@
.?AV?$SparseTensorType@M@onnxruntime@@
.?AV?$SparseTensorType@N@onnxruntime@@
.?AV?$SparseTensorType@UBFloat16@onnxruntime@@@onnxruntime@@
.?AV?$SparseTensorType@UMLFloat16@onnxruntime@@@onnxruntime@@
.?AV?$Sub@_J@onnxruntime@@
.?AV?$Sub@H@onnxruntime@@
.?AV?$Sub@M@onnxruntime@@
.?AV?$Sub@N@onnxruntime@@
.?AV?$Sum_6@M@onnxruntime@@
.?AV?$Sum_6@N@onnxruntime@@
.?AV?$Sum_8@M@onnxruntime@@
.?AV?$Sum_8@N@onnxruntime@@
.?AV?$SVMRegressor@M@ml@onnxruntime@@
.?AV?$Tan@M@onnxruntime@@
.?AV?$TensorType@_J@onnxruntime@@
.?AV?$TensorType@_K@onnxruntime@@
.?AV?$TensorType@_N@onnxruntime@@
.?AV?$TensorType@C@onnxruntime@@
.?AV?$TensorType@E@onnxruntime@@
.?AV?$TensorType@F@onnxruntime@@
.?AV?$TensorType@G@onnxruntime@@
.?AV?$TensorType@H@onnxruntime@@
.?AV?$TensorType@I@onnxruntime@@
.?AV?$TensorType@M@onnxruntime@@
.?AV?$TensorType@N@onnxruntime@@
.?AV?$TensorType@UBFloat16@onnxruntime@@@onnxruntime@@
.?AV?$TensorType@UMLFloat16@onnxruntime@@@onnxruntime@@
.?AV?$TensorType@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@onnxruntime@@
.?AV?$ThreadPoolTempl@VEnv@onnxruntime@@@concurrency@onnxruntime@@
.?AV?$time_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$time_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$time_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$time_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$time_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$time_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$TopK@$08M@onnxruntime@@
.?AV?$TopK@$08N@onnxruntime@@
.?AV?$TopK@$09M@onnxruntime@@
.?AV?$TopK@$09N@onnxruntime@@
.?AV?$TopK@$0L@_J@onnxruntime@@
.?AV?$TopK@$0L@M@onnxruntime@@
.?AV?$TopK@$0L@N@onnxruntime@@
.?AV?$TreeEnsembleClassifier@_J@ml@onnxruntime@@
.?AV?$TreeEnsembleClassifier@H@ml@onnxruntime@@
.?AV?$TreeEnsembleClassifier@M@ml@onnxruntime@@
.?AV?$TreeEnsembleClassifier@N@ml@onnxruntime@@
.?AV?$TreeEnsembleRegressor@M@ml@onnxruntime@@
.?AV?$TreeEnsembleRegressor@N@ml@onnxruntime@@
.?AV?$Unique@M@contrib@onnxruntime@@
.?AV?$Upsample@E@onnxruntime@@
.?AV?$Upsample@H@onnxruntime@@
.?AV?$Upsample@M@onnxruntime@@
.?AV?$VersionedKernel@VDmlOperatorEinSum@Dml@@$0M@@@
.?AV?$VersionedKernel@VDmlOperatorPadding@Dml@@$06@@
.?AV?$VersionedKernel@VDmlOperatorPadding@Dml@@$0L@@@
.?AV?$VersionedKernel@VDmlOperatorRegionOfInterestAlign@Dml@@$09@@
.?AV?$VersionedKernel@VDmlOperatorResize@Dml@@$06@@
.?AV?$VersionedKernel@VDmlOperatorResize@Dml@@$08@@
.?AV?$VersionedKernel@VDmlOperatorResize@Dml@@$09@@
.?AV?$VersionedKernel@VDmlOperatorResize@Dml@@$0L@@@
.?AV?$VersionedKernel@VDmlOperatorSlice@Dml@@$06@@
.?AV?$VersionedKernel@VDmlOperatorSlice@Dml@@$09@@
.?AV?$VersionedKernel@VDmlOperatorSlice@Dml@@$0L@@@
.?AV?$VersionedKernel@VDmlOperatorTopK@Dml@@$06@@
.?AV?$VersionedKernel@VDmlOperatorTopK@Dml@@$09@@
.?AV?$VersionedKernel@VDmlOperatorTopK@Dml@@$0L@@@
.?AV?$Walker@H@Regexp@re2@@
.?AV?$Walker@PEAVRegexp@re2@@@Regexp@re2@@
.?AV?$Walker@UFrag@re2@@@Regexp@re2@@
.?AV?$Where@_J@onnxruntime@@
.?AV?$Where@E@onnxruntime@@
.?AV?$Where@H@onnxruntime@@
.?AV?$Where@M@onnxruntime@@
.?AV?$Where@N@onnxruntime@@
.?AV?$Where@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@onnxruntime@@
.?AV?$wstring_convert@V?$codecvt_utf8@_W$0BAPPPP@$0A@@std@@_WV?$allocator@_W@2@V?$allocator@D@2@@std@@
.?AV__non_rtti_object@std@@
.?AV_ExceptionPtr_normal@?A0x327206fc@@
.?AV_Facet_base@std@@
.?AV_Generic_error_category@std@@
.?AV_Iostream_error_category@std@@
.?AV_Locimp@locale@std@@
.?AV_Ref_count_base@std@@
.?AV_System_error@std@@
.?AV_System_error_category@std@@
.?AV<lambda_006c5dfae8d5efe14aa8dd98266e3df1>@@
.?AV<lambda_006f042491572090b778a6887556c541>@@
.?AV<lambda_007d356207fb2206257759d3648591e7>@@
.?AV<lambda_00f913cc4b69ebb86f45ae3c7b5157d6>@@
.?AV<lambda_019248982c19876b36bd9d56cbf00fcf>@@
.?AV<lambda_0251dfdaad61cfa5b2a46c99b78d27e8>@@
.?AV<lambda_0275254af342f4b71991ea84a162d515>@@
.?AV<lambda_0311f58d27b0724e7aff1c8a092d73e4>@@
.?AV<lambda_031e8dfd8d6a123502d9ec2194443e60>@@
.?AV<lambda_038eb253b7f4e93118566e43dda81530>@@
.?AV<lambda_03e7406b29b220fa131a6585ce48dcd2>@@
.?AV<lambda_040c82e16435d8fa2645bdc3a6feb222>@@
.?AV<lambda_0443539308e5b1117b55e6de227ed40c>@@
.?AV<lambda_0457fc55cb15f22bdcba75f54e0aa1c1>@@
.?AV<lambda_04d93a296e8e245be45f1b71871c71c4>@@
.?AV<lambda_05416e20ce8aa4e6f975518b44ddcde1>@@
.?AV<lambda_05b00e57ae6a5f779503e7606fdbb129>@@
.?AV<lambda_05c8e068d08f49052da87d86b6e09131>@@
.?AV<lambda_05d4d8e8ce6819b16e6fba2d5b9fcbb4>@@
.?AV<lambda_06244d6f86a0e04dd264c197f436ab90>@@
.?AV<lambda_0634e9236de63d479eb0b5769cc5905c>@@
.?AV<lambda_06471b7954288b9dda9f9d99a72bb7c6>@@
.?AV<lambda_066c50938341a8bc4180c44723e9e561>@@
.?AV<lambda_06c5d09c41dfd075b909f65599f357bc>@@
.?AV<lambda_07b5ec0d67fe9f3a2b9eff732577e516>@@
.?AV<lambda_07c38659a483ec71b80a34d988a12e67>@@
.?AV<lambda_07cc7db52a628b03f17e7b1fd5adf33e>@@
.?AV<lambda_084d58d9727b25a791fe4a65cdec3cd9>@@
.?AV<lambda_087f7d76293e9eae88f0507abf2d5be6>@@
.?AV<lambda_09052ceb1bf3b577c728dec9fe3eab9b>@@
.?AV<lambda_0920e8c792b09e0b2fbd0035321589a5>@@
.?AV<lambda_09804657b1d4ece14dbea3419a326007>@@
.?AV<lambda_099bb0573d1376fbed683cceb180bb81>@@
.?AV<lambda_099f8dd4d33b6b91bc0ee51d86ed6c1a>@@
.?AV<lambda_0a3d98e003a8310444f50c395f4ee870>@@
.?AV<lambda_0a786afdc494302a03a8347211af4f5e>@@
.?AV<lambda_0aa140aa205a0a79b623f0b6cf867084>@@
.?AV<lambda_0b3e445014abdc00ff4fcef82b0ddb16>@@
.?AV<lambda_0b4501e0c9b2f4b8bd34960a35816f56>@@
.?AV<lambda_0b4cc153b058489c05500d0b394c47a0>@@
.?AV<lambda_0c69e806d9e2fad559bbd11cd911ec3e>@@
.?AV<lambda_0c6b04fd5867b05b63eee5705dade40a>@@
.?AV<lambda_0d27c011736e5be9787a1d0ac63fe07a>@@
.?AV<lambda_0d57079c5084d18bf7b57a312b9c6ff1>@@
.?AV<lambda_0e9a1a7b6965f6b0c97b479e18664de1>@@
.?AV<lambda_0ef38e5b4dd40a551bcbede7fadd1472>@@
.?AV<lambda_1009fe77501925cd48ec95dce1652db1>@@
.?AV<lambda_1038cf6b5b3a06f7836c03470cc73cfe>@@
.?AV<lambda_104e5d4f3c346a5807a7db11389af990>@@
.?AV<lambda_115059020db09aa13bab825ad518034e>@@
.?AV<lambda_120fe39f3f94f79844c85c347c36a663>@@
.?AV<lambda_1221b4097effa32a8dd388b4455dd2c1>@@
.?AV<lambda_1270656b5748c7df835592233d045234>@@
.?AV<lambda_12732c75ed9773d86371326e53f563fc>@@
.?AV<lambda_12dd4c4dcfd6c2d8effe6071c823bd43>@@
.?AV<lambda_13046178b359e9c81742cfc406e97a28>@@
.?AV<lambda_13c0d49a798e8cd7f3691f8cf3d13739>@@
.?AV<lambda_14407bcf8cfc66db9a85fd2745a31d94>@@
.?AV<lambda_1472d44ac2293296de1d1858a76d2083>@@
.?AV<lambda_1476157c6a284e4f379191854345eea5>@@
.?AV<lambda_166f6feb25832b61746c79ccdee9d41a>@@
.?AV<lambda_16be1790a9760642272e826be4697ed1>@@
.?AV<lambda_170eb7415bead306baf0c6b0a2038992>@@
.?AV<lambda_173e0a40a6995e7b7babfb2fbad06d73>@@
.?AV<lambda_17829500fdcde99eb4798ae93821b557>@@
.?AV<lambda_182f1a27d8844e569a31380c256cc9bb>@@
.?AV<lambda_18a3b6757625880e23566c26c3f789bc>@@
.?AV<lambda_18c8bc7d6e1564eeb9ad39be83abfb9f>@@
.?AV<lambda_1a27d0f30cf9efcc6916e583d192ed19>@@
.?AV<lambda_1a6960ebca2811a967a9cbfd5069c842>@@
.?AV<lambda_1a8b8132ed3c0ab63c63224ad4161527>@@
.?AV<lambda_1aa88cfbcf551c26b6ab6d047872e0e0>@@
.?AV<lambda_1b259c1c37867f7117af2630fb7a6d3a>@@
.?AV<lambda_1bd85f327df3bce7adb1be01960673ad>@@
.?AV<lambda_1be1cbcaffa122d29a4666a385d05e9a>@@
.?AV<lambda_1cd5cfe6b76bb3ba1c53203d58a4eedd>@@
.?AV<lambda_1cdda74d195f4dc7bbdbaed3ee174bf7>@@
.?AV<lambda_1cfb91e6d30b05211dfbd68ad9e022a5>@@
.?AV<lambda_1ea0a0832a4cf660ea6a57031389b3fe>@@
.?AV<lambda_1f048742a5a93eb61d2d12e7e445b475>@@
.?AV<lambda_1f1cad28f5d50542c82767124ccbbeb8>@@
.?AV<lambda_1f71f65e3a443fc2082168237f5b3822>@@
.?AV<lambda_1f8675d15ee5efaa8f5579945c6d118a>@@
.?AV<lambda_1fd04ff7ed408ca4ebed6e8107f19670>@@
.?AV<lambda_21332c666c4782640bb01cbce240ea30>@@
.?AV<lambda_21a762d3684c6831db7ddce449ffa123>@@
.?AV<lambda_221457049dd6e599b1e592be3898266a>@@
.?AV<lambda_22478b9deeaf5ef5a9976c43d80db60d>@@
.?AV<lambda_22485ec429c0920bef998442ed4b0141>@@
.?AV<lambda_226d842a4bcdc8605120f4a2efdc3985>@@
.?AV<lambda_2344ddb4306265b449b2e4f4d9e8b0c6>@@
.?AV<lambda_236692037ce4d824baf1ec7c2be78fd2>@@
.?AV<lambda_2369f292b0f42fbecffef8359e85ebdc>@@
.?AV<lambda_236f7c8f49088d4c504fd71f55f4280b>@@
.?AV<lambda_23a1bb0478ab7117c6dff4712434f02b>@@
.?AV<lambda_24865a883638478ee05cb85a5ed86e33>@@
.?AV<lambda_24b1e53580405f6c2ad3def2f5ac0f9b>@@
.?AV<lambda_25395042ec336ff6c2d2bf12e6e33481>@@
.?AV<lambda_25418f5506f1baf0333fc0e9c546e5f0>@@
.?AV<lambda_2669dde96175d6d91119a033acb17aa8>@@
.?AV<lambda_27a631d2450c357a52927c1dfbd2efda>@@
.?AV<lambda_27eb941e257b57f9cf2ee9f3ef3087b3>@@
.?AV<lambda_2875b3e2e66b274d36ac70e9f0680993>@@
.?AV<lambda_291539eac26e37a91034c8d9cfbf8005>@@
.?AV<lambda_29460ba72c9a9f61937f695b212c2385>@@
.?AV<lambda_2997ea06f466f3cb7af7580139e0e9b1>@@
.?AV<lambda_29a27f66e5d65fb36ed8965e3fa92ded>@@
.?AV<lambda_2a0432e2f861fdcc4b7f759b0200b6a4>@@
.?AV<lambda_2a3f9f19c48a100998729328075237dc>@@
.?AV<lambda_2bf41ec2c8021b2e4a040cfc2c7e5e74>@@
.?AV<lambda_2c89f6b694672e0bdff0d3a443bad435>@@
.?AV<lambda_2cbd0994bac82f132cec95ecbcb986ab>@@
.?AV<lambda_2cc7c01007d8f546fc79115b36c9098b>@@
.?AV<lambda_2cda3ec4180407fdb66cae08f370124f>@@
.?AV<lambda_2d779cea19d25f2c662fbf9068287203>@@
.?AV<lambda_2da95d9a6f78440b51d048ed6c8e3561>@@
.?AV<lambda_2dc68f98e6942dc4cba921733a4cc521>@@
.?AV<lambda_2de5c71bdb57850c67f17271c98887bb>@@
.?AV<lambda_2df0990ebf51ceedd01650692ad351d9>@@
.?AV<lambda_2ec601f90d3be6e3270c931423ef1071>@@
.?AV<lambda_2f4103e1c4a1773101a6c1f055d56df4>@@
.?AV<lambda_2f6217725d36a8fee3233a5a3a8acbc4>@@
.?AV<lambda_30e1101d7025e2d839d1daa9ac434cd1>@@
.?AV<lambda_30fd1fff2c31f5dad21f99700592798a>@@
.?AV<lambda_31044cbb2065f4b8855bbdc646057e42>@@
.?AV<lambda_310ceee0e4c58a7a83d7d00319ed9410>@@
.?AV<lambda_33215563a6aacb86deff746a6aa28a6c>@@
.?AV<lambda_3354f944db7877754471d985be3fe29b>@@
.?AV<lambda_33598cc4eaf8359204b6791532beb5a1>@@
.?AV<lambda_338e5fd506fd3a59b6f8e616d7366dcc>@@
.?AV<lambda_33ab9233a6e2a6fe6c24d1963cbc3111>@@
.?AV<lambda_3418ac6d0a6ccb6c692baad9c09fd00f>@@
.?AV<lambda_3435a69bc2a4a58cf25c9601fa916799>@@
.?AV<lambda_34eb695d67d7709c51f33f1b8d0d7456>@@
.?AV<lambda_35df272685e40c7e2ea40583b6906dbe>@@
.?AV<lambda_3674353b915e5a5f4e97c63731ea3b7b>@@
.?AV<lambda_36945aef9b69b8bc246158e60ded74ff>@@
.?AV<lambda_36dd25e706f1ff93211e6de0438e06c6>@@
.?AV<lambda_37fc46271b5a9d577e78557058b76819>@@
.?AV<lambda_37fc51724b8ab138f4abea1c86c474be>@@
.?AV<lambda_3928e725003c7a62c76f41f34cee3234>@@
.?AV<lambda_399e4ac9a8be74e8d14d0d5e67b02a32>@@
.?AV<lambda_39a308a33350cfe399318acb8bdc0850>@@
.?AV<lambda_3a8a940e89b14c9a8617dfa32f79a60a>@@
.?AV<lambda_3aa61f6258ac2da5aa1399bf10de3644>@@
.?AV<lambda_3b30dc70df2fd5467fa431e4f2f4c7c6>@@
.?AV<lambda_3b97251de202434e86d007c33f9345a1>@@
.?AV<lambda_3bef22810faae0f9d9c56ac7d1b31770>@@
.?AV<lambda_3bf9d7b5239a137326d2c5fa821fa737>@@
.?AV<lambda_3c79a7d5f53525620be3b5426f6bc737>@@
.?AV<lambda_3e55de506c3b90a3897c70c68b70e958>@@
.?AV<lambda_3f0ef66d0b0de67b55e854d697da2fff>@@
.?AV<lambda_3f2e81ef89df0c02055696f8c5f6254f>@@
.?AV<lambda_3f66090bd830c9da19f940c92f6ab3b0>@@
.?AV<lambda_3f6b039d1c2a9ddc0c4edd40a4875e7e>@@
.?AV<lambda_3f992177007ef3a6a4a7f362ca0d72b8>@@
.?AV<lambda_3feaee9fba56b8002e91e57f10529540>@@
.?AV<lambda_4092000fa2c851eb2ddefc1b6efbf1fc>@@
.?AV<lambda_40d7d549b7296d37bd8a375a6a32735f>@@
.?AV<lambda_40f74427ec28f20a5d59b70a3f7ba162>@@
.?AV<lambda_416d8694f58a78bcb0cae00322a6dfbe>@@
.?AV<lambda_41ebeab5da8beef13ef5472118318ec8>@@
.?AV<lambda_42085d5d1ab621f3e103058b343368b0>@@
.?AV<lambda_423c7f4514cb0f580d17c1969d94ab4d>@@
.?AV<lambda_42580b4f3e49f7dcfbf37d76233bd856>@@
.?AV<lambda_42935452e405cc9507a07edc9f5911fc>@@
.?AV<lambda_429bffec90bbca49494510e27e518b79>@@
.?AV<lambda_42fa4ee951b97f0e77b9de9f38db0750>@@
.?AV<lambda_430c7041764d504802816f1b3fb25d0a>@@
.?AV<lambda_43d76a454d7e446551a32b163679964a>@@
.?AV<lambda_44f740f4362270fce5c964ebb50cbf77>@@
.?AV<lambda_452b080256e46867afd3b39c75897486>@@
.?AV<lambda_4623cd97b09f16cc323b1da4783c1a74>@@
.?AV<lambda_464b0c569b5e14ad1c13cf083ce55247>@@
.?AV<lambda_46a0ec34ba455071cbedc5983343a291>@@
.?AV<lambda_476f16e46856ad1afb9e2f3aa987cf47>@@
.?AV<lambda_47f8da96f41c768dc3c8df613df77fec>@@
.?AV<lambda_4888e52cfaab8cdad25ecea017b54d57>@@
.?AV<lambda_493755c0718e549fc6eb6376aaf1c076>@@
.?AV<lambda_4a3dbd1b89fc0bebb34de978cf1b5369>@@
.?AV<lambda_4b9483331b5c6f0ec35078b873f8a5a1>@@
.?AV<lambda_4baf23d0c206ed351c6ebf16f344f7e0>@@
.?AV<lambda_4bd02c7b372889e26c922fb97a0c9ac3>@@
.?AV<lambda_4c36c751efc3ade178fd6425d4750a0b>@@
.?AV<lambda_4c8889970a5cc40b02f37f6aaa2ab4a4>@@
.?AV<lambda_4cb6536179ac33ff59d8db68f89e6d48>@@
.?AV<lambda_4d2bb8b4b91e7c65b08948d705bb6118>@@
.?AV<lambda_4e978ba75a8a0f3f4d2d1e75d74ac4de>@@
.?AV<lambda_4f07ad08d951f2412255fcd32991b778>@@
.?AV<lambda_4f3d7353e76f345aa194bb4f18efcd6f>@@
.?AV<lambda_4f6fc7acd223589eaf3936f4e0c463ec>@@
.?AV<lambda_5033badb7240167b30b90523bec68adf>@@
.?AV<lambda_50815597be56ed0e327183a6f984971c>@@
.?AV<lambda_50ac1bbabd41636fda9e1bf36b1fc455>@@
.?AV<lambda_51049cb5fa4b41211d9b2f4f400d5578>@@
.?AV<lambda_5151cdb5e2d0e967b0ff9b516f4dde5e>@@
.?AV<lambda_51e7ce4347ad68176c6a114dbe1a7fb9>@@
.?AV<lambda_51f7b34ad2d14ff9bfb03559ccadc81f>@@
.?AV<lambda_5275443c3995dec7145dee969221190f>@@
.?AV<lambda_53958a524045125d538038a834c170f9>@@
.?AV<lambda_540effeb914d07d2f71f09cc0eadb0b4>@@
.?AV<lambda_5455b7a3d85ed24852611958e08bf1e7>@@
.?AV<lambda_54c52b5102f405f26b86736abc33d9dc>@@
.?AV<lambda_55c987d2288bf4751b7cab2cd3c58f7e>@@
.?AV<lambda_55dd3d0ebe2e48cd699a7e50bd7c1613>@@
.?AV<lambda_55ea1c7ed7f7f7268714c360c9a1eb07>@@
.?AV<lambda_55f27842a844c2a0f8f9655ee109b4f8>@@
.?AV<lambda_563fdc041becc3012b0755fecfa41a3e>@@
.?AV<lambda_56616cb04d479ecbbef60d16cb985ff3>@@
.?AV<lambda_576babf2632b8f119a1c6bd71dacdba2>@@
.?AV<lambda_577697c65f7f30ab5c890fb5e643c3c6>@@
.?AV<lambda_581e172f6e6bf6d53a792d4f2adbb4c0>@@
.?AV<lambda_5820733bf712edf4856ad2454498e68c>@@
.?AV<lambda_5868813267026bde29d771b4b1b80b4f>@@
.?AV<lambda_5887a420a41b7e61b78788bc0c22095d>@@
.?AV<lambda_58e45bcf81f2eb4f6ae6b3f124defb13>@@
.?AV<lambda_58f61fab9db18b85afa307b409b47336>@@
.?AV<lambda_59a1cf9d667f240536256011adc0acbf>@@
.?AV<lambda_59d0c8449581e17e31aa5d36d4e1dfcc>@@
.?AV<lambda_5a03cad439a535c73c9479bab198ff8a>@@
.?AV<lambda_5a274fa2bc43001df426f4be14bb9207>@@
.?AV<lambda_5a4a1a47e435bf53505cb70a52e252af>@@
.?AV<lambda_5b4207cefa66d538798d564c2c02cc47>@@
.?AV<lambda_5cb0d9b3510f76020ef572153549b475>@@
.?AV<lambda_5d23ede476722136670b473ee395a3f1>@@
.?AV<lambda_5d65e3f0f83e3e12149b32c274720cbb>@@
.?AV<lambda_5dc4cb1bad49b465af07c0f2ab16f9e0>@@
.?AV<lambda_5dddc316c01bb02791c37b03fa523bb0>@@
.?AV<lambda_5df54278b3724a9baf121a1f8852e38e>@@
.?AV<lambda_5e0adb550519bb305a282a49bc052ff5>@@
.?AV<lambda_5f12e6a0112b95dae1a14ebbefa23368>@@
.?AV<lambda_5f8bb775dedd90ad37d19f6b92037c69>@@
.?AV<lambda_6098da3ff9cb8b08151d3844bed31f70>@@
.?AV<lambda_6137ab0b6712207e9054d1c872b492dd>@@
.?AV<lambda_6140388ec8dda4d6c3a2334a907586b2>@@
.?AV<lambda_61f3572bba9686bdafa622dd4d9fc8d2>@@
.?AV<lambda_626d338c2f6a43327586cf0d89586aa1>@@
.?AV<lambda_6277f32b20d3d158c3ebcc1771d29c1a>@@
.?AV<lambda_6299ae6da9b19142e396b5cd2562b3f4>@@
.?AV<lambda_62f1ce9bc208e37c7e5739c8f36388ed>@@
.?AV<lambda_6367df6aa050372a33bd7465b9bffebf>@@
.?AV<lambda_63cff24d9185d14f18fbcbf2a1033b5f>@@
.?AV<lambda_63db87e2da8ecf77c1c67b6ddf9097c0>@@
.?AV<lambda_640443148c28920f15cf87dfb5e782ad>@@
.?AV<lambda_64826d400df5e2683a863a4dbb954602>@@
.?AV<lambda_64c62750e999b82780ab6a59a6ba6851>@@
.?AV<lambda_651d9e9e9fab134890e8df206635b6df>@@
.?AV<lambda_65379b8fe5c982ca5593c72a9026c7e8>@@
.?AV<lambda_670b3cf7fae0fc01758a26323e302b88>@@
.?AV<lambda_670b81b0a20483d6e6a649cadb77d57d>@@
.?AV<lambda_6730021c9a4195800fc17a38baeda605>@@
.?AV<lambda_6744a0008aca8ce8a0b29db8f1c01fa1>@@
.?AV<lambda_69c32f0b6e172d0bef2d15e0c00baa25>@@
.?AV<lambda_6a160209166296c80c9867c0c24688ea>@@
.?AV<lambda_6a32611970906250c6b47d8292ad00e7>@@
.?AV<lambda_6ae1892ff4a4003e2e47baa65ff24d8d>@@
.?AV<lambda_6b9436f4089e82f5864d5365b56a5e02>@@
.?AV<lambda_6bf7af48e7a4158e20b4f833c15de130>@@
.?AV<lambda_6c3806e2f5558f0233ccf8d05c3ab49e>@@
.?AV<lambda_6c435e48e6003c8c444134537317003d>@@
.?AV<lambda_6c91c6f34bd8327dc22f6b7f8735fd5b>@@
.?AV<lambda_6cf09a4d87483efc80956d9a5ca8e5cd>@@
.?AV<lambda_6d3877051847983c73e11af2112f64fa>@@
.?AV<lambda_6da49eb0f8069eb4bbc177d850fb6dbc>@@
.?AV<lambda_6db08e7b535fdfa2ab543d6397b422f2>@@
.?AV<lambda_6dcb2a3f2f16914195595cee405bb7f9>@@
.?AV<lambda_6e1ae7feac7554001d4e77c1f621284e>@@
.?AV<lambda_6e3cb65d29853e95007cf81c805209f2>@@
.?AV<lambda_6edb8e5a01fa4dd0c3e5fdadea9cdb78>@@
.?AV<lambda_6f80d65fbc6388042df99a3b220305d1>@@
.?AV<lambda_70279f22222c392cb493fce9840987c7>@@
.?AV<lambda_7042c4e252262e9c03c2cccafb92f0c5>@@
.?AV<lambda_70a5014ee65d8eb269c12c537da5a5f1>@@
.?AV<lambda_71e695b5d6b9b0e0404ca139c0ef87a1>@@
.?AV<lambda_72e83488395612e0f6a2602b726a187c>@@
.?AV<lambda_72fa6f1dd132a0cb664556e9b32ab2a9>@@
.?AV<lambda_7361ba811d5ff92bf50e102c73312480>@@
.?AV<lambda_7365299d20f0c0cc81db24afaeca7624>@@
.?AV<lambda_738d98ba114f0b992fde43f3c2b8082f>@@
.?AV<lambda_7414049e307c7d47f77a8e839817b7a9>@@
.?AV<lambda_745ebee95180d242f8918f6ee05b1702>@@
.?AV<lambda_747f5a5054cea5042105b3988bb8e54a>@@
.?AV<lambda_74a481eac2e8d64c2bfb5722b5b1cb89>@@
.?AV<lambda_76259788010337ce84c7523a6ac9ea2f>@@
.?AV<lambda_76f850960fe6ac327acd4613d41835f7>@@
.?AV<lambda_77304148f0ddead6f8725fd6b1fe3a6f>@@
.?AV<lambda_786b2ff03846fcfa21d00bf234deaff2>@@
.?AV<lambda_789f43d877e88a79c653d99212ff6d23>@@
.?AV<lambda_78d2ff8fcf22c6166aa97abbb5fd0d75>@@
.?AV<lambda_795d004014b095ac6c3348f10d228401>@@
.?AV<lambda_796aa6c99df67f823c02a52131c10530>@@
.?AV<lambda_79f39293405ab41c7ab7de2e65eb25d5>@@
.?AV<lambda_7adac37e31393d4ac6b5bca719f468e8>@@
.?AV<lambda_7adcd22a7346287699992d30bc8e4d12>@@
.?AV<lambda_7afa01f5238052e0bc3d44d682bbe41c>@@
.?AV<lambda_7bf0c45af15d48ff0bf3482d9ab1530d>@@
.?AV<lambda_7c1e6047474e627b4a76b3aff4efd43b>@@
.?AV<lambda_7c9d354511f23b5ab35f0d23acf59e69>@@
.?AV<lambda_7ccee1bbd5e211d37343acf4a25d203e>@@
.?AV<lambda_7d18ce867b586dae3ed4309094f85b3d>@@
.?AV<lambda_7d43feae7ce5df77bbbe61f408666b24>@@
.?AV<lambda_7d49a6264e4dd01effe57235ddc8a797>@@
.?AV<lambda_7d4a5c6b07fd360e232dc4416f33eeb2>@@
.?AV<lambda_7d5e6e2f53eb2e4c8474c9eb67ea5809>@@
.?AV<lambda_7d68ba7556ef690147168e00d959e2c9>@@
.?AV<lambda_7d7cf49bce0e19787ec70235e54b100e>@@
.?AV<lambda_7dcae2be4577512cc32ab12b37bbe9bb>@@
.?AV<lambda_7e6056f43d2e17d5b39d88df6efe7e61>@@
.?AV<lambda_7e973787028859cebe35750559b8a424>@@
.?AV<lambda_7ef4d7bf9a11acea8de40a758239a896>@@
.?AV<lambda_7effc068e9678d1890888bb91fa9882e>@@
.?AV<lambda_7fb11eaf1ed7921e4b12c72edf5431c8>@@
.?AV<lambda_7fe593f35b7bf5ce6d3de3d99005ffa9>@@
.?AV<lambda_7ff2aca09375d1b200a4b303c393fea9>@@
.?AV<lambda_80083253b4a44259ccb0f1ab2453639e>@@
.?AV<lambda_8052aa8f31d91e8553d059cc8a579374>@@
.?AV<lambda_81b3e5fabf0f60201facddc5b3aeca88>@@
.?AV<lambda_8205a59784c4f81abae5636754370a87>@@
.?AV<lambda_825b24178740a83899599d9a15262ad0>@@
.?AV<lambda_839e4f861e16af089c617eb4d1d9047f>@@
.?AV<lambda_83fc98e77223d87948a3045c6a6d1977>@@
.?AV<lambda_845fb0d365668e61a65e09bae6280c09>@@
.?AV<lambda_8470282d05918db64db57459ab19a88b>@@
.?AV<lambda_84c0c09494d01cc30f16e3449edead54>@@
.?AV<lambda_85098ac94eb0ea35e2ec901831d538f0>@@
.?AV<lambda_853403f8a1cb1bf480beff4c6dac6b21>@@
.?AV<lambda_854a3a76655e0ae1c9e2878289036568>@@
.?AV<lambda_85740bb65c1a7256d972833e138b25dd>@@
.?AV<lambda_86b0d88e3b26101a7a3a4da61749b425>@@
.?AV<lambda_8711802b90051c34567904599fdbb25a>@@
.?AV<lambda_872c985ed95e21f46464410321ab9d0c>@@
.?AV<lambda_87377fe9a7b55ca099852704c7649a37>@@
.?AV<lambda_87cb9d4d5e38fb120a31a53965033d4d>@@
.?AV<lambda_87d1adad0ec3c9a329fb0c8dabf3ed17>@@
.?AV<lambda_881846c2025fe9fdf21e10df8ef1aed7>@@
.?AV<lambda_8825ba7f925d933a5700bea03837b34e>@@
.?AV<lambda_8859cd3c08b6a8e9d40267fbf61322d1>@@
.?AV<lambda_88b1496ecc213c07fd24b30bb5c856e7>@@
.?AV<lambda_88cacdca8b660ada94d8f522e9f11bdd>@@
.?AV<lambda_88e1ec3a19eedcbddbcfd03d69c09c03>@@
.?AV<lambda_88e4cbdd9578a1aef93a2f7d3b7ececc>@@
.?AV<lambda_88f211d79a091f6a481688e49635f453>@@
.?AV<lambda_8ac5cfcccd347e512ac9419fd63346cb>@@
.?AV<lambda_8af3a30942023f4a3511597660fac0f0>@@
.?AV<lambda_8b24a914bf112352c9369f2b44e2ccd1>@@
.?AV<lambda_8ba5740a8c0829410cb3674fe8f010b1>@@
.?AV<lambda_8baa7635c058d848d4bb5868b5a1e31c>@@
.?AV<lambda_8ccc49f0cad898f6323ff4e82d1ce354>@@
.?AV<lambda_8d89ab19f4dfa0ecffb29771863af791>@@
.?AV<lambda_8ddc74f94abbe18c414cbbd5ea52c4fb>@@
.?AV<lambda_8dfa231b74e328c8530f0e32ce2f974d>@@
.?AV<lambda_8e2d345faf08aedb7570dd988fdca755>@@
.?AV<lambda_8ea5687524a42143533b684f5c9ce181>@@
.?AV<lambda_8ee8d0015a85dfa1395e6275618758cf>@@
.?AV<lambda_8f0d91faf2fd476a9a61179d9b72837d>@@
.?AV<lambda_8f7a58d98eb24c89d9b0a7ccf81cddea>@@
.?AV<lambda_8fd00d5632d83f4a297582c97b44acf3>@@
.?AV<lambda_8ffce33af8696a7a42fa073f6875aacb>@@
.?AV<lambda_9201c11196abf45c662a6757819da80c>@@
.?AV<lambda_9222e83c56bbeb08f964662158da2721>@@
.?AV<lambda_9289ae269610356b68d5c3f46e89bf66>@@
.?AV<lambda_929181c452e7cbade815a0403dd15e2f>@@
.?AV<lambda_92ce44a6835ab59773e2e1a9e5082039>@@
.?AV<lambda_92e7ca79b8942501ac9dcdff3174eb32>@@
.?AV<lambda_93063d24d3eaf65455400e96fd395987>@@
.?AV<lambda_930b54dfc8a64c42da270936f80e56b0>@@
.?AV<lambda_932e39bc42b54acf3286176bb275e4a7>@@
.?AV<lambda_93957b37c8fde7931003607e2ea412ad>@@
.?AV<lambda_93b65f05ddd7a75740e1646ed2ad5a8e>@@
.?AV<lambda_93dbd15e01be52d41a7a111df62d38a4>@@
.?AV<lambda_957302132dd7fea31b11af321304eee3>@@
.?AV<lambda_959b8361497e19898ca852a4058b29b4>@@
.?AV<lambda_95b9fc9fd7cabe9da00f754968571d1b>@@
.?AV<lambda_95c74ffd32a4d994a62a191daf0fff27>@@
.?AV<lambda_95dd41682914710e4e8d31623057e7d3>@@
.?AV<lambda_96fa572628c59e4d078744e93c0f7196>@@
.?AV<lambda_974421a2166ea3684014171d0d75f69e>@@
.?AV<lambda_977aa0989600c553e6ad21ef5f183702>@@
.?AV<lambda_97d1f16f53a6cde81773c5f27deb1fd2>@@
.?AV<lambda_97f1aa20e78dc8680c319e9629ef8e8e>@@
.?AV<lambda_9800a6d45ddc69dc425addf83da8e9e3>@@
.?AV<lambda_981f87d47b0bee1dfc124a3791149631>@@
.?AV<lambda_98d34ef6b8d86da687aa9aa634b2afb2>@@
.?AV<lambda_991d63fb8a809dc84e4e523068ce486a>@@
.?AV<lambda_9974bd71538aef5818f74e4a5e52a52e>@@
.?AV<lambda_99b6e836d69b69305b62d8ff2093a102>@@
.?AV<lambda_9a0946b71204ca51d455c5e9aa49e823>@@
.?AV<lambda_9a122db8c0c6f400c116ad5d2cdaf0bc>@@
.?AV<lambda_9a2786289a4d18fa33a8facc5fc736fa>@@
.?AV<lambda_9a56c482bf06c1a34241f10d7df60646>@@
.?AV<lambda_9a74fa744077df9d937c4fe36054e9d4>@@
.?AV<lambda_9a80c5c6458e824bfa0876a779605b0b>@@
.?AV<lambda_9abb244ecf1fe7549bfa1b3b46427d69>@@
.?AV<lambda_9ad4dd46fb7ebf522a3e85e715143745>@@
.?AV<lambda_9ba9dde4e941db1311dbefb477d19165>@@
.?AV<lambda_9baa1cdad376a56943ca503609534da1>@@
.?AV<lambda_9c4f15881ef2441e36fb65a5f1a748dd>@@
.?AV<lambda_9c55cd28535069d632248acbead74665>@@
.?AV<lambda_9cff58ca0c6e88f847f22b27dae030be>@@
.?AV<lambda_9d00353fd33377312606dc12e977172a>@@
.?AV<lambda_9d98759a86a1a6c70a5f6713a8d099cc>@@
.?AV<lambda_9dc7af5c7bcff785bd9190f2cb464b36>@@
.?AV<lambda_9ddccf775856736f4137dfc22445dcab>@@
.?AV<lambda_9e03109a12e41496ab9ad081a718600f>@@
.?AV<lambda_9ea68d0a3164c0f7bc2786cdbb58d687>@@
.?AV<lambda_9ef12b090c75e5f2f5ddaa1c0717f98a>@@
.?AV<lambda_a047e4fa7a019da5da5dcb5093bd2da9>@@
.?AV<lambda_a0f179e1ced4abc8e1523256ebbadd18>@@
.?AV<lambda_a103bb1e5b04405f4e2459b7d854fdbb>@@
.?AV<lambda_a11d3e2153f19eecf5eacc6ad2a1a9c5>@@
.?AV<lambda_a17d3ee64fefe07bb4c5c931150c8a1b>@@
.?AV<lambda_a1b033c1238f74f04d136673bf3141a7>@@
.?AV<lambda_a202458c5f736a51fd6e4cb6bbed2f45>@@
.?AV<lambda_a3360ab2153a0d5e361146591e0ed940>@@
.?AV<lambda_a34df399fe5dbf1e2984cce096bb4cbc>@@
.?AV<lambda_a38954a3fc0168033639a5d3db4af77f>@@
.?AV<lambda_a47ee9eece330f845238b622e01ff20a>@@
.?AV<lambda_a4df326263f2cb24ed288d1255c6569d>@@
.?AV<lambda_a4fe3bd39dcd0107f90455cb45c21429>@@
.?AV<lambda_a5bc10369070c00aebb065dae57fa0bc>@@
.?AV<lambda_a69e02b0210b70ea824684cc65a6da5c>@@
.?AV<lambda_a7a0686f23e7f9337eaa10f069ee321b>@@
.?AV<lambda_a7f558f1dd0bc6dfbc34073d8010b2d2>@@
.?AV<lambda_a856994ca323bdea896dcaf6811e93e3>@@
.?AV<lambda_a8dc603fd8183714a0dc357fc9b787c1>@@
.?AV<lambda_ab278d2c0c4ee1c6b80548eeeee0d417>@@
.?AV<lambda_ab509a36b64c6185495bed43165d3ae5>@@
.?AV<lambda_abe09d436d912a4ff16d1ac6a5002952>@@
.?AV<lambda_ac9c227ba6c5cb467d27ecbdbba9c871>@@
.?AV<lambda_acbad9bc57c95ab4ffc746caf04c813c>@@
.?AV<lambda_acccfeb1e0041e26bf01c6097464f8c5>@@
.?AV<lambda_adbb0ed82c3e353a2ef8b51f4773ed3d>@@
.?AV<lambda_ae0d8b5a2da4af67276477f8d17abe42>@@
.?AV<lambda_af2c39467b4484bc2a2f611cf533daa7>@@
.?AV<lambda_af999ce2e03a2039dbf31c1ff13e0675>@@
.?AV<lambda_af9b4834cf87a70d331cb6fc7bbbeee5>@@
.?AV<lambda_afd3110d589aa504a36e81b6239b970d>@@
.?AV<lambda_aff25a6a369700d46cd3658ce112be53>@@
.?AV<lambda_b02c4bc922420e968df79ed6aa499235>@@
.?AV<lambda_b056c5876502bf8728ee42ccf0ca953b>@@
.?AV<lambda_b079e40837048df263512dcc9441c93c>@@
.?AV<lambda_b09bcb6c8a5faaa957decc75e6f3e6e4>@@
.?AV<lambda_b0c700b83742c00837801ebad88797d0>@@
.?AV<lambda_b123eebc6aab753cdc93a7cd4699b893>@@
.?AV<lambda_b13b0c3bacb7bf27cf9bf4bac72849bb>@@
.?AV<lambda_b18b645f3287a76fd9f5d36468b29bf7>@@
.?AV<lambda_b2b5b00a4ad3647a9a40b4f0dbb0745d>@@
.?AV<lambda_b31abea9b0adc93fa794390a998569d7>@@
.?AV<lambda_b32e5f2bc2fafd58a4afcda5e8537c65>@@
.?AV<lambda_b346f4e5faa3c794cd68f36eadcb5c6c>@@
.?AV<lambda_b3661b31d86a85c89e2f6a1a0742a349>@@
.?AV<lambda_b379f94eb8587c8f8eb06ab04392317e>@@
.?AV<lambda_b38c51459887501772a6d011e08a4556>@@
.?AV<lambda_b39fcf5d316099631cb4151a025dde22>@@
.?AV<lambda_b3cbfd0276cf0ad6f3b58532d60f239f>@@
.?AV<lambda_b4697c7f06fc1e7e59f4a5183c6a98c3>@@
.?AV<lambda_b4b1fce215f20fdccd275ca27e2b3255>@@
.?AV<lambda_b4e1f5c6fdfb97123733fced30ecae42>@@
.?AV<lambda_b51428820b3477616d25ebf36e7dd645>@@
.?AV<lambda_b592d9bc8cc53309aec51b3e43d6dad4>@@
.?AV<lambda_b63c2e5f4896a6001dfbdd641092de94>@@
.?AV<lambda_b6aaeef95996305e5431d1fd80eaed87>@@
.?AV<lambda_b6cf00bbc72a844e74b65b515f429504>@@
.?AV<lambda_b720b525ca7596ec682823a87c40ac8d>@@
.?AV<lambda_b77050229503eb6e58b80080f34d3ad8>@@
.?AV<lambda_b86aa8ef1de0593a5547f08d792c1565>@@
.?AV<lambda_b8846f4ca0753c507ed6516ddbf99aba>@@
.?AV<lambda_b8bff811421756f68bf0701ba18268e9>@@
.?AV<lambda_b937f7d4f097577d4fa0e0107ad25600>@@
.?AV<lambda_b9ad0ccbf56c00a46342640cba7cdc2c>@@
.?AV<lambda_ba7c69dcd3c7c32d7eba5d02fb06573e>@@
.?AV<lambda_bb6f1fa388d8f370137cb6c6519f955b>@@
.?AV<lambda_bb7b1b0ef71312cebe2ce7f754927441>@@
.?AV<lambda_bb907a92df9fde70b67de149cb976bb1>@@
.?AV<lambda_bbed65651851398ae9e91eb41072a5f9>@@
.?AV<lambda_bbfb6c4f9961aa9de543511f0a00bf20>@@
.?AV<lambda_bc47dcc73653c7f9cb7b31e06f49f967>@@
.?AV<lambda_bc9a9f99cbeabaf2d0a936a0cad6b782>@@
.?AV<lambda_bca65c99655015407a4683ba6812e592>@@
.?AV<lambda_bca81c57471aa7b198b5b9c6df12b83d>@@
.?AV<lambda_bce28680af6d6e63afbcec5203580c30>@@
.?AV<lambda_bd3b717d06eb8cb8c6cdeb6f2f75dbb9>@@
.?AV<lambda_beb16572edee23057a5875bc228caa9d>@@
.?AV<lambda_beb5ca5c13f598ae93cc9dc5c5727597>@@
.?AV<lambda_beb991e0fd9fa3631b6b17fe3b5d669c>@@
.?AV<lambda_bfe66c418aacc57ce1b58557a3a20356>@@
.?AV<lambda_c007df17e8115f08c5c6e2ce7e2201c0>@@
.?AV<lambda_c1347ce9161a6b5cef4e0bceb24b27d3>@@
.?AV<lambda_c162c83b8e727b7911e302875a688c52>@@
.?AV<lambda_c20b2bc71558ce98a32f42bb85de6d28>@@
.?AV<lambda_c24ed35691635f968e92d6373ba271aa>@@
.?AV<lambda_c26c4492665ce46ed2b794163ccf01b8>@@
.?AV<lambda_c273bcba413eb077d5fb961b15132175>@@
.?AV<lambda_c2d5469345cd97550e4777826a9777d5>@@
.?AV<lambda_c2fef1cfbd08e9e21bcb74b985df87e6>@@
.?AV<lambda_c327e8b0febe30d927a0cd8dc39dba92>@@
.?AV<lambda_c3857b8060a6c3155dc6bb6df09a4841>@@
.?AV<lambda_c3d54ef5797172fcc9eaba420c8f75e8>@@
.?AV<lambda_c4806bda3c509fc7c991e936211d831c>@@
.?AV<lambda_c4d7f6cc60481397c12458592585c6f0>@@
.?AV<lambda_c5677617c2b0074bdccd3f6596388b76>@@
.?AV<lambda_c6108df36ce1e471ee073cdcb906363b>@@
.?AV<lambda_c646dd14edaacae64487e0b96ad575f5>@@
.?AV<lambda_c6728963f8bb91595df67d14c61fd8fa>@@
.?AV<lambda_c6e8ce8febb47c28ae5d88eebb07bb81>@@
.?AV<lambda_c720e61805dcd56f6632856caaf966d8>@@
.?AV<lambda_c7bc13683ecb5a18d043a137e0218d42>@@
.?AV<lambda_c81a90897540aaa4b34ce92ab90b53b8>@@
.?AV<lambda_c89cf3298587f6daa5b2556ad200a769>@@
.?AV<lambda_c89e6783ea7ba65dd2bbca371d42018d>@@
.?AV<lambda_c8a671616559eac778e178004f6bff38>@@
.?AV<lambda_c9d64b63a4c10e55647e3707de8acb40>@@
.?AV<lambda_ca2e0a0561d13968d35862532b5f82aa>@@
.?AV<lambda_ca605469c50319317c9a56255477531a>@@
.?AV<lambda_ca87e505bf8a7e1f26579083c3683d94>@@
.?AV<lambda_ca99611dde14cbfee162988c3da30011>@@
.?AV<lambda_ca9d22ac690a049ddaf8ded26ec4c6e4>@@
.?AV<lambda_cb274b2da89b230e840725fbe13c27e7>@@
.?AV<lambda_cc270699afce6708083292447c6ec5a7>@@
.?AV<lambda_ccbc04edad8bea47ebc5361806e9195e>@@
.?AV<lambda_cd453f5abbb4020fb3775475830cd8d1>@@
.?AV<lambda_ce1101c22724e1357d724096af023481>@@
.?AV<lambda_ceef0f6dad2176ea59816e5c9d007c22>@@
.?AV<lambda_cf0d99f02d80c9fcb9d4fc5032f5b198>@@
.?AV<lambda_cfa93eededd189da5db0d1b9a54368f6>@@
.?AV<lambda_cfb3b6eaf74a6d2adee4cb59404bca02>@@
.?AV<lambda_cfdeab09b00be16ca9b756aad9e71d8c>@@
.?AV<lambda_d01cb7d72935562762275ef2725e3ce2>@@
.?AV<lambda_d084e33f090b9a2ef4ac7752a4304e52>@@
.?AV<lambda_d0c5713ac46ee06d27e6324a463b7bee>@@
.?AV<lambda_d12714ea155ac3d05d46fd33bcaf55d0>@@
.?AV<lambda_d16d0ed0a77c40b384d41d6d45ea4e55>@@
.?AV<lambda_d2d693a490e9887da5a024c703dc8e3e>@@
.?AV<lambda_d2e668c941b29e0c58674cef7fa23f2c>@@
.?AV<lambda_d44234032f9da3a8e659111375d4b0b2>@@
.?AV<lambda_d47c5253ca7eceaa0e1c3b2cf5640fa1>@@
.?AV<lambda_d48cb9baeb37d51a45fff2e006fd2ed3>@@
.?AV<lambda_d5d62f4b858abcded92348c54a906570>@@
.?AV<lambda_d5dc432e2d4add6c02a3fbdbdb6568b6>@@
.?AV<lambda_d73cfc3b818335b575efed71024f8847>@@
.?AV<lambda_d843aae243ab497a4cddae9c413aa41d>@@
.?AV<lambda_d846c7c2d29b00ef66091f3bac323c48>@@
.?AV<lambda_d8a9ac16dd775f4bb1f670ae469711dc>@@
.?AV<lambda_d8b06a4dd6f551b1fc6c7009ad0ed428>@@
.?AV<lambda_d8b96520499b24df75ab37992af46b4d>@@
.?AV<lambda_d8c2999bab01791ce81720c649701452>@@
.?AV<lambda_d966a160cb34bd8cd4e5e8dbd91fdf85>@@
.?AV<lambda_d9adada7822ba67bc618dd6220c7e11e>@@
.?AV<lambda_da71868c78371b6753545a4c8498c6e4>@@
.?AV<lambda_daa3e6298aefc79ad774de1fc533e850>@@
.?AV<lambda_dcbbb46dcb2ccb38f1b7e3e89a0ca172>@@
.?AV<lambda_dccf783802d22b3c48245e980c348c0b>@@
.?AV<lambda_dd4a8a293eb1e8d9bc247b4bc07121c6>@@
.?AV<lambda_df18081ccc34b5e376ea2f099a6c86d1>@@
.?AV<lambda_df8f09957380fda37bd75205e74ab017>@@
.?AV<lambda_dfdbf34c7b75816fc0d7ddf3d1f4736c>@@
.?AV<lambda_dfdef6da643cd6a11fea2d67407fdc34>@@
.?AV<lambda_dff178d3d11d9fe36bd8141e8e6f7e2f>@@
.?AV<lambda_e021ada2350e9ccd28f4a55f38144d36>@@
.?AV<lambda_e0371779d439769322780aabf313f8d1>@@
.?AV<lambda_e08a75ecff8a2a27b308be3f8dfddc66>@@
.?AV<lambda_e1a7abf114b955966097bd83c14de705>@@
.?AV<lambda_e1b9ebecb019a93c9b1d900fc5dac815>@@
.?AV<lambda_e1f234778fb80305bf70a017789747a5>@@
.?AV<lambda_e2aaeba0e7fe634207a31efbbb188fef>@@
.?AV<lambda_e2bb8f55a256e670e2f45d1b39e2618b>@@
.?AV<lambda_e318b9a1ac6011cec45976779008bc1d>@@
.?AV<lambda_e34957b95c2645f541b4bd46255fbbb0>@@
.?AV<lambda_e3b884cd04a55a9aff856d7ab36b0422>@@
.?AV<lambda_e483d02e0524579d835af30c8a6874b1>@@
.?AV<lambda_e4af5663d262f6d7cf485c6657cb9e95>@@
.?AV<lambda_e5304a863af2c1f046e5da60dfc5f195>@@
.?AV<lambda_e60c0af24df176341124ead910e74b4f>@@
.?AV<lambda_e77a57374f1e055f8281ce5dd65e8ebd>@@
.?AV<lambda_e79e85aa06c0020ce8ba4eced08c2c92>@@
.?AV<lambda_e89cae078ae99afbefebcbd69b7a7f5c>@@
.?AV<lambda_e8b80a28a395cc4498887c668735db69>@@
.?AV<lambda_e8dabe6ee14f0907c4413646f291102d>@@
.?AV<lambda_e90c0bd946d9218d9f45b675a7b78c24>@@
.?AV<lambda_e971bf56206beddda75940cb23d81d7c>@@
.?AV<lambda_e9e8bddd64a71567d96eadd0fe1d2a17>@@
.?AV<lambda_ea73e250586924fb71aef6e9068c74b3>@@
.?AV<lambda_eb6625ff603571627d15a6f97beef5e6>@@
.?AV<lambda_eb887415aed41a3927a754246db16d35>@@
.?AV<lambda_eb9c4de1a9cbbc50c248fee15cd2820e>@@
.?AV<lambda_ec38228ee8b8eb97ec41ac102234721a>@@
.?AV<lambda_ecee2654318c52376f803bc94d6bc4ee>@@
.?AV<lambda_ed05756275fe268c82e7c72f39e62f2a>@@
.?AV<lambda_ed53b4c0957e241f93b3be9c259f7b0d>@@
.?AV<lambda_ee50a4ed472e80d4977c99ba85a1d7df>@@
.?AV<lambda_ee5e1bfaf2a0db686c570e5f95f6aa95>@@
.?AV<lambda_eedfabccf04dae8365cd4a4775f4c12a>@@
.?AV<lambda_ef997e7e75a3ce38d15a0a98d436f03c>@@
.?AV<lambda_ef9ce4d0dc32ab06ddd4426fedb787f1>@@
.?AV<lambda_efb13713d16ec9cd372290e4d816e310>@@
.?AV<lambda_f0421a9025619b3b88dddf09f010b1ec>@@
.?AV<lambda_f1fa811d25327e50fa43acb91e2eb6a4>@@
.?AV<lambda_f273d4d4788dd6d522544c26d76a5ae4>@@
.?AV<lambda_f29d64af0687bfdd6e3496c166444014>@@
.?AV<lambda_f30a6117225d3cb8e4a11f826263726e>@@
.?AV<lambda_f3baf3c6fac0ad86964ff832e3055849>@@
.?AV<lambda_f4b2ad37b32ce448782bd96a2cfadb78>@@
.?AV<lambda_f4c664674d3291639f6f1c524accc00e>@@
.?AV<lambda_f51c3a2a2774821bc4773e2abbaa11b4>@@
.?AV<lambda_f55fed088cc6887bf534564a0809769f>@@
.?AV<lambda_f588fb4ed0c783dc6991c6110b1943e5>@@
.?AV<lambda_f5db0169896386e7220581461257fa02>@@
.?AV<lambda_f60a4fa32157d6fa8e2622fa2d06d23c>@@
.?AV<lambda_f6391278f73df45e5c03a1b0ebcd91ef>@@
.?AV<lambda_f6ba89529e75bbc8ce63885b5ef6f940>@@
.?AV<lambda_f6ee07f46f90bd0fd4cd2ececdbe52e0>@@
.?AV<lambda_f76c1721a2f76590ee1ac32dd1a7059b>@@
.?AV<lambda_f78d8769dd40946ef61ad5956cbf10fe>@@
.?AV<lambda_f79206f00cb74e35e11935d1c0ff9449>@@
.?AV<lambda_f8241a17ccaf97c109b664362fd083d5>@@
.?AV<lambda_f87191d92810657cd25a2973b1734a59>@@
.?AV<lambda_f8dbfaed9d55440bb15cbf009ac934a6>@@
.?AV<lambda_f90b302c32ce6f81af3f56ee1f3a9b51>@@
.?AV<lambda_f92ea10dca143475672352184887f31b>@@
.?AV<lambda_fa9cea3a5917d32ed85305ced3e8f02e>@@
.?AV<lambda_faa349663837247e8666c82fc4f50c4f>@@
.?AV<lambda_fb0df1f24c66125b0dcb6e0f4b2b8c09>@@
.?AV<lambda_fb1d0169209b6e282abb9c9575470f62>@@
.?AV<lambda_fbddeeb35b871ddb923a0706e4876ef6>@@
.?AV<lambda_fc59cfd6180e7962f9dbfcf6d01970a3>@@
.?AV<lambda_fc7d1f6c2adcff66a9b847972f40b566>@@
.?AV<lambda_fcb655464ad0e43ce2671f06c665bb0e>@@
.?AV<lambda_fd07f717d8be7a0b4ac28d8863ca11e3>@@
.?AV<lambda_fd266a9166d0b10d35a0d8f14994daa2>@@
.?AV<lambda_fd2f32690f4c94955691e93dd0de8250>@@
.?AV<lambda_fd4f2bbf00caf507fea1cc6053ab984e>@@
.?AV<lambda_ffee7b280847cd780e3c0aef3f3c198c>@@
.?AVAbiCustomRegistry@Adapter@MachineLearning@AI@Windows@@
.?AVAbiCustomRegistryImpl@Adapter@MachineLearning@AI@Windows@@
.?AVAbiOpKernel@Adapter@MachineLearning@AI@Windows@@
.?AVAllocationInfo@Dml@@
.?AVAllocator@flatbuffers@@
.?AVAllocatorWrapper@onnxruntime@@
.?AVAnd@onnxruntime@@
.?AVAttentionBase@contrib@onnxruntime@@
.?AVAttentionCPUBase@contrib@onnxruntime@@
.?AVAttentionFusion@onnxruntime@@
.?AVAttributeProto@onnx@@
.?AVbad_alloc@std@@
.?AVbad_array_new_length@std@@
.?AVbad_cast@std@@
.?AVbad_exception@std@@
.?AVbad_function_call@std@@
.?AVbad_optional_access@std@@
.?AVbad_typeid@std@@
.?AVbad_variant_access@std@@
.?AVBatchNormalizationAddFusion@onnxruntime@@
.?AVBatchNormalizationMulFusion@onnxruntime@@
.?AVBFCArena@onnxruntime@@
.?AVBiasGeluFusion@onnxruntime@@
.?AVBiasSoftmaxFusion@onnxruntime@@
.?AVBucketizedBufferAllocator@Dml@@
.?AVCast@?A0xf2399911@onnxruntime@@
.?AVCastElimination@onnxruntime@@
.?AVCastMap@ml@onnxruntime@@
.?AVCategoryMapper@ml@onnxruntime@@
.?AVcharNode@@
.?AVClip@onnxruntime@@
.?AVCLogSink@logging@onnxruntime@@
.?AVClosable@Adapter@MachineLearning@AI@Windows@@
.?AVCoalesceWalker@re2@@
.?AVcodecvt_base@std@@
.?AVCommonSubexpressionElimination@onnxruntime@@
.?AVCompiler@re2@@
.?AVCompress@onnxruntime@@
.?AVConcat@onnxruntime@@
.?AVConcatBase@onnxruntime@@
.?AVConcatFromSequence@onnxruntime@@
.?AVConcatHelper@OperatorHelper@@
.?AVConstantFolding@onnxruntime@@
.?AVConstantOfShape@?A0x9bbafd50@onnxruntime@@
.?AVConstantOfShapeHelper@OperatorHelper@@
.?AVConvActivationFusion@onnxruntime@@
.?AVConvAddFusion@onnxruntime@@
.?AVConvBNFusion@onnxruntime@@
.?AVConvInteger@onnxruntime@@
.?AVConvMulFusion@onnxruntime@@
.?AVConvolutionHelperBase@OperatorHelper@@
.?AVCopyingFileInputStream@FileInputStream@io@protobuf@google@@
.?AVCopyingFileOutputStream@FileOutputStream@io@protobuf@google@@
.?AVCopyingInputStream@io@protobuf@google@@
.?AVCopyingInputStreamAdaptor@io@protobuf@google@@
.?AVCopyingOstreamOutputStream@OstreamOutputStream@io@protobuf@google@@
.?AVCopyingOutputStream@io@protobuf@google@@
.?AVCopyingOutputStreamAdaptor@io@protobuf@google@@
.?AVCPUAllocator@Dml@@
.?AVCPUAllocator@onnxruntime@@
.?AVCPUDataTransfer@onnxruntime@@
.?AVCPUExecutionProvider@onnxruntime@@
.?AVCropBase@contrib@onnxruntime@@
.?AVCropHelper@OperatorHelper@@
.?AVDataTransfer@Dml@@
.?AVDataTypeImpl@onnxruntime@@
.?AVDeepCpuAttnLstmOp@contrib@onnxruntime@@
.?AVDeepCpuGruOp@onnxruntime@@
.?AVDeepCpuLstmOp@onnxruntime@@
.?AVDefaultAllocator@flatbuffers@@
.?AVDepthToSpaceHelper@OperatorHelper@@
.?AVDmlCommandRecorder@Dml@@
.?AVDmlGraphOpKernelInfoWrapper@Adapter@MachineLearning@AI@Windows@@
.?AVDmlOperator@Dml@@
.?AVDmlOperatorActivation@Dml@@
.?AVDmlOperatorAffine@Dml@@
.?AVDmlOperatorBatchNormalization@Dml@@
.?AVDmlOperatorCast@Dml@@
.?AVDmlOperatorConcat@Dml@@
.?AVDmlOperatorConstantOfShape@Dml@@
.?AVDmlOperatorConvInteger@Dml@@
.?AVDmlOperatorConvolution@Dml@@
.?AVDmlOperatorCopy@Dml@@
.?AVDmlOperatorCrop@Dml@@
.?AVDmlOperatorCumSum@Dml@@
.?AVDmlOperatorDepthToSpace@Dml@@
.?AVDmlOperatorDynamicQuantizeLinear@Dml@@
.?AVDmlOperatorEinSum@Dml@@
.?AVDmlOperatorElementwiseBitShift@Dml@@
.?AVDmlOperatorElementwiseClip11@Dml@@
.?AVDmlOperatorElementwiseClip7@Dml@@
.?AVDmlOperatorElementwiseIf@Dml@@
.?AVDmlOperatorElementwiseIsInf@Dml@@
.?AVDmlOperatorElementwiseMean@Dml@@
.?AVDmlOperatorElementwiseMod@Dml@@
.?AVDmlOperatorElementwisePow@Dml@@
.?AVDmlOperatorElementwiseRound@Dml@@
.?AVDmlOperatorExpand@Dml@@
.?AVDmlOperatorEyeLike@Dml@@
.?AVDmlOperatorGatedRecurrentUnit@Dml@@
.?AVDmlOperatorGather@Dml@@
.?AVDmlOperatorGatherElements@Dml@@
.?AVDmlOperatorGatherNd@Dml@@
.?AVDmlOperatorGemm@Dml@@
.?AVDmlOperatorInstanceNormalization@Dml@@
.?AVDmlOperatorLocalResponseNormalization@Dml@@
.?AVDmlOperatorLongShortTermUnit@Dml@@
.?AVDmlOperatorLpNormalization@Dml@@
.?AVDmlOperatorMatMul@Dml@@
.?AVDmlOperatorMatMulInteger@Dml@@
.?AVDmlOperatorMaxUnpool@Dml@@
.?AVDmlOperatorMeanVarNormalization@Dml@@
.?AVDmlOperatorMemcpy@Dml@@
.?AVDmlOperatorNeg@Dml@@
.?AVDmlOperatorOneHot@Dml@@
.?AVDmlOperatorPadding@Dml@@
.?AVDmlOperatorPooling@Dml@@
.?AVDmlOperatorQLinearAdd@Dml@@
.?AVDmlOperatorQLinearConv@Dml@@
.?AVDmlOperatorQLinearMatMul@Dml@@
.?AVDmlOperatorRange@Dml@@
.?AVDmlOperatorRecurrentBase@Dml@@
.?AVDmlOperatorRecurrentNeuralNetwork@Dml@@
.?AVDmlOperatorReduce@Dml@@
.?AVDmlOperatorRegionOfInterestAlign@Dml@@
.?AVDmlOperatorRegionOfInterestPooling@Dml@@
.?AVDmlOperatorResize@Dml@@
.?AVDmlOperatorReverseSequence@Dml@@
.?AVDmlOperatorScatter@Dml@@
.?AVDmlOperatorScatterNd@Dml@@
.?AVDmlOperatorSlice@Dml@@
.?AVDmlOperatorSpaceToDepth@Dml@@
.?AVDmlOperatorSplit@Dml@@
.?AVDmlOperatorTile@Dml@@
.?AVDmlOperatorTopK@Dml@@
.?AVDmlOperatorTranspose@Dml@@
.?AVDmlOperatorValueScale2d@Dml@@
.?AVDNameNode@@
.?AVDNameStatusNode@@
.?AVDontUseNewUseMake@Details@WRL@Microsoft@@
.?AVDynamicQuantizeLSTM@contrib@onnxruntime@@
.?AVDynamicQuantizeMatMul@contrib@onnxruntime@@
.?AVDynamicQuantizeMatMulFusion@onnxruntime@@
.?AVEinsum@onnxruntime@@
.?AVEinSumHelper@OperatorHelper@@
.?AVEliminateDropout@onnxruntime@@
.?AVEliminateIdentity@onnxruntime@@
.?AVEliminateSlice@onnxruntime@@
.?AVEmbedLayerNormFusion@onnxruntime@@
.?AVEnv@onnxruntime@@
.?AVEnvThread@onnxruntime@@
.?AVEnvTime@onnxruntime@@
.?AVerror_category@std@@
.?AVexception@detail@nlohmann@@
.?AVexception@std@@
.?AVExecutionFrame@onnxruntime@@
.?AVExecutionPlanBase@onnxruntime@@
.?AVExecutionProvider@Dml@@
.?AVExecutionProviderImpl@Dml@@
.?AVExLibLoader@onnxruntime@@
.?AVExpandDims@contrib@onnxruntime@@
.?AVExpandElimination@onnxruntime@@
.?AVExpandHelper@OperatorHelper@@
.?AVExtendedThreadPoolInterface@concurrency@onnxruntime@@
.?AVEyeLike@onnxruntime@@
.?AVfacet@locale@std@@
.?AVfailure@ios_base@std@@
.?AVFastGeluFusion@onnxruntime@@
.?AVFatalException@protobuf@google@@
.?AVFeatureVectorizer@ml@onnxruntime@@
.?AVFileInputStream@io@protobuf@google@@
.?AVFileOutputStream@io@protobuf@google@@
.?AVFlatten@onnxruntime@@
.?AVFreeDimensionOverrideTransformer@onnxruntime@@
.?AVFunction@onnxruntime@@
.?AVFunctionImpl@onnxruntime@@
.?AVFunctionKernel@onnxruntime@@
.?AVFunctionProto@onnx@@
.?AVFusedConvFloat@contrib@onnxruntime@@
.?AVFusedGraphKernel@Dml@@
.?AVFuseReluClip@onnxruntime@@
.?AVGather@onnxruntime@@
.?AVGatherBase@onnxruntime@@
.?AVGatherElements@onnxruntime@@
.?AVGatherHelper@OperatorHelper@@
.?AVGatherND@onnxruntime@@
.?AVGatherNDBase@onnxruntime@@
.?AVGatherNdHelper@OperatorHelper@@
.?AVGeluApproximation@onnxruntime@@
.?AVGeluFusion@onnxruntime@@
.?AVGemmActivationFusion@onnxruntime@@
.?AVGemmHelper@OperatorHelper@@
.?AVGraph@onnxruntime@@
.?AVGraphInferencer@onnx@@
.?AVGraphInferencerImpl@onnxruntime@@
.?AVGraphInferencerImpl@shape_inference@onnx@@
.?AVGraphProto@onnx@@
.?AVGraphTransformer@Dml@@
.?AVGraphTransformer@onnxruntime@@
.?AVIAllocator@onnxruntime@@
.?AVIArenaAllocator@onnxruntime@@
.?AVICommandRecorder@Dml@@
.?AVIControlFlowKernel@controlflow@onnxruntime@@
.?AVIDataTransfer@onnxruntime@@
.?AVIExecutionFrame@onnxruntime@@
.?AVIExecutionProvider@onnxruntime@@
.?AVIExecutor@onnxruntime@@
.?AVIf@onnxruntime@@
.?AVImputerOp@ml@onnxruntime@@
.?AVInferenceContextImpl@onnxruntime@@
.?AVInferenceError@onnx@@
.?AVInferenceSession@onnxruntime@@
.?AVinput_buffer_adapter@detail@nlohmann@@
.?AVInsertCastTransformer@onnxruntime@@
.?AVinvalid_argument@std@@
.?AVinvalid_iterator@detail@nlohmann@@
.?AVInverse@contrib@onnxruntime@@
.?AVIOnnxRuntimeOpSchemaCollection@onnxruntime@@
.?AVios_base@std@@
.?AVISchemaRegistry@onnx@@
.?AVISequentialPlannerContext@onnxruntime@@
.?AVIsInf@onnxruntime@@
.?AVISink@logging@onnxruntime@@
.?AVITensorAllocator@onnxruntime@@
.?AVIterator@?$OrtValueTensorSlicer@$$CBUOrtValue@@@onnxruntime@@
.?AVIterator@?$OrtValueTensorSlicer@UOrtValue@@@onnxruntime@@
.?AVLabelEncoder@ml@onnxruntime@@
.?AVLayerNormFusion@onnxruntime@@
.?AVlength_error@std@@
.?AVLinearClassifier@ml@onnxruntime@@
.?AVLinearRegressor@ml@onnxruntime@@
.?AVLoggingWrapper@@
.?AVlogic_error@std@@
.?AVLoop@onnxruntime@@
.?AVLSTMBase@onnxruntime@@
.?AVMatMulAddFusion@onnxruntime@@
.?AVMatMulInteger@onnxruntime@@
.?AVMatMulIntegerBase@onnxruntime@@
.?AVMatMulIntegerToFloat@contrib@onnxruntime@@
.?AVMatMulIntegerToFloatBase@contrib@onnxruntime@@
.?AVMatMulIntegerToFloatFusion@onnxruntime@@
.?AVMatMulScaleFusion@onnxruntime@@
.?AVMax_8@onnxruntime@@
.?AVMaxPoolV8@onnxruntime@@
.?AVMaxpoolWithMask@contrib@onnxruntime@@
.?AVMaxUnpool@onnxruntime@@
.?AVMemcpyTransformer@onnxruntime@@
.?AVMessageLite@protobuf@google@@
.?AVMin_8@onnxruntime@@
.?AVMLAS_QGEMM_OUTPUT_PROCESSOR@@
.?AVMLAS_QGEMM_SCALE_BIAS_OUTPUT_PROCESSOR@@
.?AVMLKernelInferenceContext@Adapter@MachineLearning@AI@Windows@@
.?AVMLOperatorKernelFactory@@
.?AVMLOperatorShapeInferrer@@
.?AVMLOperatorSupportQuery@@
.?AVMLSchemaInferenceContext@Adapter@MachineLearning@AI@Windows@@
.?AVMLSupportQueryContext@Adapter@MachineLearning@AI@Windows@@
.?AVMod@onnxruntime@@
.?AVModelProto@onnx@@
.?AVMultinomial@onnxruntime@@
.?AVMurmurHash3@contrib@onnxruntime@@
.?AVNchwcAveragePool@contrib@onnxruntime@@
.?AVNchwcConv@contrib@onnxruntime@@
.?AVNchwcMaxPool@contrib@onnxruntime@@
.?AVNchwcPoolBase@contrib@onnxruntime@@
.?AVNchwcTransformer@onnxruntime@@
.?AVNchwcUpsample@contrib@onnxruntime@@
.?AVNhwcInferenceContext@contrib@onnxruntime@@
.?AVNhwcMaxPool@contrib@onnxruntime@@
.?AVNhwcTransformer@onnxruntime@@
.?AVNodeProto@onnx@@
.?AVNonMaxSuppression@onnxruntime@@
.?AVNonMaxSuppressionBase@onnxruntime@@
.?AVNonTensorTypeBase@onnxruntime@@
.?AVNormalizer@ml@onnxruntime@@
.?AVNot@onnxruntime@@
.?AVNotImplementedException@onnxruntime@@
.?AVNumCapturesWalker@re2@@
.?AVOneHotHelper@OperatorHelper@@
.?AVOnnxRuntimeException@onnxruntime@@
.?AVOnnxRuntimeOpSchemaRegistry@onnxruntime@@
.?AVOnnxTensorWrapper@Adapter@MachineLearning@AI@Windows@@
.?AVOperatorSetIdProto@onnx@@
.?AVOpKernel@onnxruntime@@
.?AVOpKernelContext@onnxruntime@@
.?AVOpKernelContextInternal@onnxruntime@@
.?AVOpKernelContextWrapper@Adapter@MachineLearning@AI@Windows@@
.?AVOpKernelInfoWrapper@Adapter@MachineLearning@AI@Windows@@
.?AVOpSchemaRegistry@onnx@@
.?AVOptimizerExecutionFrame@onnxruntime@@
.?AVOr@onnxruntime@@
.?AVOstreamOutputStream@io@protobuf@google@@
.?AVOStreamSink@logging@onnxruntime@@
.?AVother_error@detail@nlohmann@@
.?AVout_of_range@detail@nlohmann@@
.?AVout_of_range@std@@
.?AVPadBase@onnxruntime@@
.?AVPaddingHelper@OperatorHelper@@
.?AVpairNode@@
.?AVParallelExecutor@onnxruntime@@
.?AVparse_error@detail@nlohmann@@
.?AVpcharNode@@
.?AVpDNameNode@@
.?AVPoolBase@onnxruntime@@
.?AVPoolingHelperBase@OperatorHelper@@
.?AVPow@onnxruntime@@
.?AVPrimitiveDataTypeBase@onnxruntime@@
.?AVQLinearConv@onnxruntime@@
.?AVQLinearGlobalAveragePool@contrib@onnxruntime@@
.?AVQLinearMatMul@onnxruntime@@
.?AVRandomNormal@onnxruntime@@
.?AVRandomNormalLike@onnxruntime@@
.?AVRandomUniform@onnxruntime@@
.?AVRandomUniformLike@onnxruntime@@
.?AVRange@onnxruntime@@
.?AVrange_error@std@@
.?AVRangeHelper@OperatorHelper@@
.?AVRecurrentHelper@OperatorHelper@@
.?AVReduceHelperBase@OperatorHelper@@
.?AVRemoveDuplicateCastTransformer@onnxruntime@@
.?AVReorderInput@contrib@onnxruntime@@
.?AVReorderOutput@contrib@onnxruntime@@
.?AVRepetitionWalker@re2@@
.?AVReshape@onnxruntime@@
.?AVReshape_1@onnxruntime@@
.?AVReshapeFusion@onnxruntime@@
.?AVResizeHelper@OperatorHelper@@
.?AVResultException@wil@@
.?AVReverseSequenceOp@onnxruntime@@
.?AVRewriteRule@onnxruntime@@
.?AVRoiAlignBase@onnxruntime@@
.?AVRoiAlignHelper@OperatorHelper@@
.?AVRoiPoolingHelper@OperatorHelper@@
.?AVRoiPoolingHelperBase@OperatorHelper@@
.?AVRuleBasedGraphTransformer@onnxruntime@@
.?AVruntime_error@std@@
.?AVRuntimeClassBase@Details@WRL@Microsoft@@
.?AVScatter@onnxruntime@@
.?AVScatterND@onnxruntime@@
.?AVScatterNDBase@onnxruntime@@
.?AVSchemaError@onnx@@
.?AVSchemaRegistryManager@onnxruntime@@
.?AVSequenceAt@onnxruntime@@
.?AVSequenceConstruct@onnxruntime@@
.?AVSequenceEmpty@onnxruntime@@
.?AVSequenceErase@onnxruntime@@
.?AVSequenceInsert@onnxruntime@@
.?AVSequenceLength@onnxruntime@@
.?AVSequenceTensorTypeBase@onnxruntime@@
.?AVSequentialExecutor@onnxruntime@@
.?AVSequentialPlannerContext@onnxruntime@@
.?AVShape@onnxruntime@@
.?AVShapeToInitializer@onnxruntime@@
.?AVShrink@onnxruntime@@
.?AVSign@onnxruntime@@
.?AVSimpleTensorAllocator@onnxruntime@@
.?AVSimplifiedLayerNormFusion@onnxruntime@@
.?AVSimplifyWalker@re2@@
.?AVSize@onnxruntime@@
.?AVSkipLayerNormFusion@onnxruntime@@
.?AVSliceBase@onnxruntime@@
.?AVSliceHelper@OperatorHelper@@
.?AVSpaceDepthBase@onnxruntime@@
.?AVSpaceToDepthHelper@OperatorHelper@@
.?AVSparseTensorProto@onnx@@
.?AVSparseTensorTypeBase@onnxruntime@@
.?AVSplit@onnxruntime@@
.?AVSplitBase@onnxruntime@@
.?AVSplitHelper@OperatorHelper@@
.?AVSplitToSequence@onnxruntime@@
.?AVSqueeze@onnxruntime@@
.?AVSqueezeBase@onnxruntime@@
.?AVstl_critical_section_interface@details@Concurrency@@
.?AVstl_critical_section_win7@details@Concurrency@@
.?AVStringNormalizer@onnxruntime@@
.?AVStringStringEntryProto@onnx@@
.?AVSVMClassifier@ml@onnxruntime@@
.?AVSVMCommon@ml@onnxruntime@@
.?AVsystem_error@std@@
.?AVTelemetry@onnxruntime@@
.?AVTensorAllocatorWithMemPattern@onnxruntime@@
.?AVTensorAnnotation@onnx@@
.?AVTensorProto@onnx@@
.?AVTensorProto_Segment@onnx@@
.?AVTensorShapeProto@onnx@@
.?AVTensorShapeProto_Dimension@onnx@@
.?AVTensorTypeBase@onnxruntime@@
.?AVTensorWrapper@Adapter@MachineLearning@AI@Windows@@
.?AVTfIdfVectorizer@onnxruntime@@
.?AVThreadPoolInterface@Eigen@@
.?AVTileHelper@OperatorHelper@@
.?AVTokenizer@contrib@onnxruntime@@
.?AVTopKHelper@OperatorHelper@@
.?AVToStringWalker@re2@@
.?AVTrainingInfoProto@onnx@@
.?AVTranspose@onnxruntime@@
.?AVTransposeBase@onnxruntime@@
.?AVTransposeHelper@OperatorHelper@@
.?AVTrilu@contrib@onnxruntime@@
.?AVtype_error@detail@nlohmann@@
.?AVtype_info@@
.?AVTypeProto@onnx@@
.?AVTypeProto_Map@onnx@@
.?AVTypeProto_Opaque@onnx@@
.?AVTypeProto_Sequence@onnx@@
.?AVTypeProto_SparseTensor@onnx@@
.?AVTypeProto_Tensor@onnx@@
.?AVUnique@onnxruntime@@
.?AVUnpoolingHelper@OperatorHelper@@
.?AVUnsqueeze@onnxruntime@@
.?AVUnsqueezeBase@onnxruntime@@
.?AVUnsqueezeElimination@onnxruntime@@
.?AVUpsampleBase@onnxruntime@@
.?AVValidationError@checker@onnx@@
.?AVValueInfoProto@onnx@@
.?AVViewerFunctionImpl@onnxruntime@@
.?AVWindowsEnv@?A0x25a6b04d@onnxruntime@@
.?AVWindowsEnvTime@?A0x1cd5e214@onnxruntime@@
.?AVWindowsTelemetry@onnxruntime@@
.?AVWindowsThread@?A0x25a6b04d@onnxruntime@@
.?AVWinmlAdapterLoggingWrapper@@
.?AVWordConvEmbedding@contrib@onnxruntime@@
.?AVXor@onnxruntime@@
.?AVZeroCopyInputStream@io@protobuf@google@@
.?AVZeroCopyOutputStream@io@protobuf@google@@
.?AVZipMapOp@ml@onnxruntime@@
.`>J@
.==O/
.=9M/
.>2t8y
.0/0#
.0/011
.00cfg
.5!`8
.5/U8
.5~m)
.5<F8
.53D8
.5CU8
.5lW8
.5qF8
.B0(.
.CRT$XCA
.CRT$XCC
.CRT$XCL
.CRT$XCU
.CRT$XCZ
.CRT$XIA
.CRT$XIC
.CRT$XIZ
.CRT$XLA
.CRT$XLZ
.CRT$XPA
.CRT$XPZ
.CRT$XTA
.CRT$XTZ
.D2|8y
.data
.data$r
.didat$2
.didat$3
.didat$4
.didat$5
.didat$6
.didat$7
.E9f ~ZA
.edata
.gehcont
.gfids
.giats
.H;s(t8I;n
.H+1L
.H+9H
.idata$2
.idata$3
.idata$4
.idata$5
.idata$6
.json
.l0V4
.N:8<
.N02.
.P0D2
.P6A?AV?$OrtValueTensorSlicer@$$CBUOrtValue@@@onnxruntime@@AEBUOrtValue@@_J1@Z
.P6A?AV?$OrtValueTensorSlicer@UOrtValue@@@onnxruntime@@AEAUOrtValue@@_J1@Z
.P6A?AV?$unique_ptr@VTensor@onnxruntime@@U?$default_delete@VTensor@onnxruntime@@@std@@@std@@AEBVTensor@onnxruntime@@_J1V?$shared_ptr@VIAllocator@onnxruntime@@@1@PEAX@Z
.P6A?AVStatus@common@onnxruntime@@AEBV?$vector@_KV?$allocator@_K@std@@@std@@AEBVTensor@2@AEAV52@PEBVTensorShape@2@PEAX@Z
.P6A?AVStatus@common@onnxruntime@@AEBVNode@2@AEAVGraph@2@AEBV?$vector@PEBVTypeProto@onnx@@V?$allocator@PEBVTypeProto@onnx@@@std@@@std@@AEAV56@AEBUResolveOptions@42@@Z
.P6A?AVStatus@common@onnxruntime@@AEBVTensor@2@AEAV32@PEAX@Z
.P6A?AVStatus@common@onnxruntime@@PEAXAEAV?$vector@UOrtValue@@V?$allocator@UOrtValue@@@std@@@std@@0_K@Z
.P6A?AVStatus@common@onnxruntime@@PEB_J0PEA_J_K222222PEAVThreadPool@concurrency@2@PEAX@Z
.P6A?AVStatus@common@onnxruntime@@PEBH0PEAH_K222222PEAVThreadPool@concurrency@2@PEAX@Z
.P6A?AVStatus@common@onnxruntime@@PEBM0PEAM_K222222PEAVThreadPool@concurrency@2@PEAX@Z
.P6A?AVStatus@common@onnxruntime@@PEBN0PEAN_K222222PEAVThreadPool@concurrency@2@PEAX@Z
.P6A?AVTensor@onnxruntime@@AEBV01@AEBV?$vector@_JV?$allocator@_J@std@@@std@@_NV?$shared_ptr@VIAllocator@onnxruntime@@@3@PEBVTensorShape@1@PEAVThreadPool@concurrency@1@PEAX@Z
.P6A_NAEBUFunctionBodyBuildContext@onnx@@AEBVOpSchema@1@AEAVFunctionProto@1@@Z
.P6AMMMM@Z
.P6APEAVOpKernel@onnxruntime@@AEBVOpKernelInfo@1@@Z
.P6AX$$QEAVOpSchema@onnx@@@Z
.P6AXAEAUInferenceContext@onnx@@@Z
.P6AXAEAVOpSchema@onnx@@@Z
.P6AXPEAX@Z
.pdata
.rdata
.rdata$r
.rdata$T
.rdata$zETW0
.rdata$zETW1
.rdata$zETW2
.rdata$zETW9
.rdata$zzzdbg
.rsrc$01
.rsrc$02
.rtc$IAA
.rtc$IZZ
.rtc$TAA
.rtc$TZZ
.stls
.text
.text$di
.text$mn
.text$mn$00
.text$x
.text$yd
.tls$
.tls$ZZZ
.v0N2F4F6P8\:Z0,.8
.xdata
.xdata$x
-/./.
/:0^E
/E;y,u
/GTrrH
/I+6L
/t$Pv
: Conflicting with a registered kernel with op versions.
: failed validating the check: 
:"<*>*<
::0^E
:AM:am:PM:pm
:B<(:
:B<(:685
:B<(:F8j:B<(:|8
:B<(:x8B:(<b8B:(<
:b<P>\<
:H<(@l:H<(@
:H<(>\:
:H<(>`:h<H>(<
:H9q8v4H
:J< @M
:Jan:January:Feb:February:Mar:March:Apr:April:May:May:Jun:June:Jul:July:Aug:August:Sep:September:Oct:October:Nov:November:Dec:December
:lJ@LHN0R
:Please, install necessary language-pack-XX and configure locales
:Sun:Sunday:Mon:Monday:Tue:Tuesday:Wed:Wednesday:Thu:Thursday:Fri:Friday:Sat:Saturday
:TB~D
; expected 
; last read: '
;ba~H
;ba=@
;C,uj
;D$(t<H
;D9`(t
;E(|WA
;H9>t6H
;I9}(tiH
;s@s$H
;s@s)H
;t$@H
;w,~_
;y u6
? @ T T 3
?\u8A
?333333
?H+1H
?H+9H
?H9G`
@ 9p 
@ E;C
@ H;}
@ H;B 
@ H+A
@ I;@ 
@ I;A u
@ I+@
@ L+@
@ L9 
@(9A(u
@(A9A
@(Hct$(H
@(Lc|$(N
@,A9A
@.`446
@.data
@.didat
@.reloc
@.rsrc
@?*BL?fff?
@@packing unavailable
@\B~>RF.H
@08@H
@08@HP
@08@HPX`hr
@08NT
@0H;B
@0HcP(M
@0I;@8t[H
@0I;@8tGH
@0I;@8tHH
@0I;@8tJH
@0I;@8tLH
@0I;@8tOH
@0I;@8tPH
@0I;@8tSH
@0I;@8tTH
@0I;@8tUH
@0I;W
@0I9@(
@0I9@(u
@2\RF
@6.3L
@6f}L
@8@HP
@8@HP^T
@8@HPX`
@8|$(u'H
@8|$4t'A
@8|$P
@8|$Q
@8}pt
@8=<S]
@8=nS]
@87tZH
@88uI@
@8D9o\ucH
@8D9o\ugH
@8H;A@u
@8k t*LcK
@8k,H
@8n8u
@8o tNH
@8o tvH
@8oXt)
@8p t3H
@8p8t
@8p8t$H
@8q t
@8s tXH
@8sDt
@8t$(u
@8t$@t)H
@8t$`t)H
@8t$0
@8t$D
@8t$ht)H
@8t$pt
@8t$Xt
@8t$xt
@8t$Xt
@8t$xt)H
@8t$Xt)H
@8t$xt)H
@8t$Xt)H
@8u t%H
@8uot
@8u't
@8uXt%H
@8w`ui@8o t*H
@8w8t
@8w8u
@8y u
@A^_]
@A^_^
@A^_^][
@A^A\_
@A_A^]
@A_A^_
@A_A^_^[
@A_A^_^]
@A_A^A\
@A_A^A\_]
@A_A^A\_^
@A_A^A\_^][
@A_A^A]
@A_A^A]_^
@A_A^A]A\_^[
@A_A^A]A\_^]
@BB~>LF,H
@bD.F,H,J<NDPE
@bq|H
@bq|I
@bQ~I
@H;T$P
@H+D$XL
@HB(@
@Hc@(H;
@Hc\$pH
@HPX`h 
@HPXb
@HRD~E
@nBPDjB
@Nf;E
@pBTDXFJHTJlN*P8R
@PH9h
@SUVATAUAVAWH
@SUVWATAUAVAWH
@SUVWATAUAVAWI
@SUVWATAVAWH
@SUVWAUAVAWH
@SUVWAVAWH
@SUVWAVH
@SUVWH
@SUWAVAWH
@SVWATAUAVAWH
@SVWAUAVAWH
@SVWAVAWH
@SVWAVH
@SVWAVI
@SVWH
@SVWI
@TFLH
@UAUAVH
@UAUH
@USVATAUAVAWH
@USVAVAWH
@USVWATAUAVAWH
@USVWATAUAVH
@USVWATAUAWH
@USVWATAVAWH
@USVWAUAVAWH
@USVWAVAWH
@USVWAVH
@USVWAWH
@USVWH
@USWATAUAVAWH
@USWAVAWH
@USWH
@UVWATAUAVAWH
@UVWAVAWH
@VWATAUAVAWH
@XB\DPFTHNJBLZNTD4@8
@xHc@pM
[ VWAVH
[%hs(%hs)]
[%hs]
'[', '{', or a literal
[:^alnum:]
[:^alpha:]
[:^ascii:]
[:^blank:]
[:^cntrl:]
[:^digit:]
[:^graph:]
[:^lower:]
[:^print:]
[:^punct:]
[:^space:]
[:^upper:]
[:^word:]
[:^xdigit:]
[:alnum:]
[:alpha:]
[:ascii:]
[:blank:]
[:cntrl:]
[:digit:]
[:graph:]
[:lower:]
[:print:]
[:punct:]
[:space:]
[:upper:]
[:word:]
[:xdigit:]
[@uAN
[@uBN
[]^-\
[^\x00-\x{10ffff}]
[hHcCtL
[hHcCtL;
[json.exception.
[libprotobuf %s %s:%d] %s
[Memory] ExecutionFrame dynamically allocates 
[Memory] ExecutionFrame statically allocates 
[Memory] SessionStateInitializer statically allocates 
[ONNXRuntimeError]
[ShapeInferenceError] 
[thunk]:
[TypeInferenceError] 
\ P"4
\">$$
\$ @2
\$ 8Y
\$ D3
\$ D8t$(uZH
\$ E3
\$ H;
\$ H;]gH
\$ H+y
\$ I;
\$ Ic
\$ L;
\$ Lc
\$ Lc@
\$ M+
\$ UH
\$ UVWATAUAVAWH
\$ UVWATAVH
\$ UVWAVAWH
\$ UVWAVH
\$ UVWH
\$ VWATAVAWH
\$ VWAVH
\$ VWAWH
\$ VWI
\$ VWL
\$ WH
\$ WM
\$$E3
\$(@8l$0u[H
\$(D!|$ L
\$(D8|$0
\$(D8|$0usH
\$(D8d$0
\$(D8t$0
\$(E3
\$(H;
\$(H+
\$(Hc
\$(I;
\$@fD
\$@H+
\$`E3
\$`H;u(t
\$`I;
\$`L;
\$`M;
\$0@8{
\$0@8l$8
\$0~>
\$09_(
\$09Y
\$0A_A]_^]
\$0D8|$8
\$0D8d$8
\$0D8d$QtB
\$0E3
\$0H;
\$0H;U`t
\$0H+\$(H
\$0H9
\$0HcH
\$0I;
\$0L;
\$0Lc
\$0t L
\$5L3
\$8D8l$@
\$8D9x 
\$8E3
\$8H;
\$8H+
\$8HcF
\$8I;
\$8I+\$0H
\$8Lc
\$HA_A^A]A\_^]
\$hD9k(
\$HE3
\$HH;
\$HH;\$Pr
\$hH;U
\$hH+\$`H
\$hI;
\$hL;
\$hu~H
\$pD;k0
\$PE3
\$PfD
\$PH;u t
\$pH+
\$pHc
\$PI+
\$PL;Gp
\$PL;u
\$PL+
\$PLc` H
\$xD;
\$XE3
\$xE3
\$XH;
\$xH;
\$XH;u
\$XL;
\$xL;
\$XL;u@t
\$XL+
\$XM;
\.BNXP\R^TZPLVVN.
\.F0X2F4Z6D2L8L0.
\|^T`XbJdTfzj*l8n
\0^ `&^
\3JCy7
\6LlL
\directml.dll
\x%02x
\x{%x}
] != number of classlabels[
] (usually, this means you 
] already exists with value [
] because it's the graph's output.
] for now
] H;](
] is not supported this build 
] is not supported!
] is not supportted!
] not in lexicographic sorted order.
] not in sorted order.
] op_type [
] out of range [0, 
] out of range.
] should not be greater than specified axis dim value [
], could not find NodeArg 
], Value=
], while 
]. Actual value is 
]. It will be overwritten
]. Its actual value is: 
]0uxH
]hH+]`H
]Hv%L
]PfA;
]pH+]hH
]XfE9e
]XH;]`uN
]XH+]PH
^ ;C 
^ H+^
^ HcF
^ I+^
^".$,
^(+^0
^(H9_@~D
^@H9_@
^`.l\
^|*W?
^0@8w
^dNh~j2l
^H`(^lZH^(`
^jdXfhh
^pH9_@
^PhRBTHVDRDXLP,
^V`Tb8f
^v`Xbnd
^xIcFpL
_ H+_
_ HcO
_ I+_
__based(
__cdecl
__clrcall
__eabi
__fastcall
__int128
__int16
__int32
__int64
__int8
__pascal
__ptr64
__restrict
__stdcall
__strncnt
__swift_1
__swift_2
__thiscall
__unaligned
__vectorcall
__w64 
_0HcG(L
_8I+_0H
_bn_nchwc
_Cast
_DmlExecutionProvider
_fence_after
_fence_before
_FusedMatMulAndScale
_initterm
_initterm_e
_Int32
_kernel_time
_lock_locales
_min_zero_constant
_nchwc
_nhwc
_o____lc_codepage_func
_o____lc_collate_cp_func
_o____lc_locale_name_func
_o____mb_cur_max_func
_o___acrt_iob_func
_o___pctype_func
_o___std_exception_copy
_o___std_exception_destroy
_o___stdio_common_vfprintf
_o___stdio_common_vsnprintf_s
_o___stdio_common_vsprintf
_o___stdio_common_vsprintf_s
_o___stdio_common_vswprintf
_o__aligned_free
_o__aligned_malloc
_o__beginthreadex
_o__callnewh
_o__calloc_base
_o__cexit
_o__close
_o__configure_narrow_argv
_o__create_locale
_o__crt_atexit
_o__dclass
_o__difftime64
_o__errno
_o__execute_onexit_table
_o__fdclass
_o__fdsign
_o__free_base
_o__free_locale
_o__fseeki64
_o__fstat64i32
_o__get_errno
_o__get_stream_buffer_pointers
_o__Getdays
_o__Getmonths
_o__Gettnames
_o__gmtime64_s
_o__initialize_narrow_environment
_o__initialize_onexit_table
_o__invalid_parameter_noinfo
_o__invalid_parameter_noinfo_noreturn
_o__localtime64_s
_o__lock_file
_o__malloc_base
_o__mktime64
_o__purecall
_o__read
_o__realloc_base
_o__register_onexit_function
_o__seh_filter_dll
_o__set_errno
_o__sopen_s
_o__stat64i32
_o__Strftime
_o__strnicmp
_o__strtoi64
_o__towlower_l
_o__towupper_l
_o__unlock_file
_o__W_Getdays
_o__W_Getmonths
_o__W_Gettnames
_o__wcsdup
_o__Wcsftime
_o__wfsopen
_o__write
_o__wsopen_s
_o_abort
_o_acosf
_o_acoshf
_o_asinf
_o_asinhf
_o_atanf
_o_atanhf
_o_atol
_o_bsearch
_o_calloc
_o_ceil
_o_ceilf
_o_cosf
_o_coshf
_o_exp
_o_expf
_o_fclose
_o_fflush
_o_fgetc
_o_fgetpos
_o_floor
_o_floorf
_o_fmod
_o_fmodf
_o_fputc
_o_fread
_o_free
_o_frexp
_o_fseek
_o_fsetpos
_o_fwrite
_o_isalpha
_o_isdigit
_o_islower
_o_isspace
_o_isupper
_o_ldexp
_o_localeconv
_o_log
_o_log2
_o_log2f
_o_logf
_o_malloc
_o_pow
_o_powf
_o_remainderf
_o_rint
_o_rintf
_o_roundf
_o_setlocale
_o_setvbuf
_o_sin
_o_sinf
_o_sinhf
_o_sqrt
_o_sqrtf
_o_strcpy_s
_o_strerror
_o_strncpy_s
_o_strtod
_o_strtof
_o_strtoll
_o_strtoull
_o_tanf
_o_tanh
_o_tanhf
_o_terminate
_o_tolower
_o_ungetc
_o_wcsftime
_RDATA
_RuleBasedTransformer
_token_
_unlock_locales
_Unused
` 0"X&
` UAUAVH
` UAVAWH
` x$*&8(
` z$*&8(
`".$,
`":$(
`.rdata
`:^<F>^@Z<LBR:.
`:0K(
`:P_7
`2\RF
`20^E
`4.6,
`4dz>
`6j*M
`6O8L
`8@HPX`hpx
`A^_^
`A^_^[]
`A^_^][
`A^A\_^]
`A_A^_^[
`A_A^_^]
`A_A^A\_^[]
`A_A^A\_^][
`A_A^A]_^[]
`A_A^A]_^][
`A_A^A]A\_^[
`A_A^A]A\_^]
`adjustor{
`anonymous namespace'
`anonymous-namespace'::GetExternalDataInfo
`anonymous-namespace'::ReadExternalDataForTensor
`copy constructor closure'
`default constructor closure'
`dynamic atexit destructor for '
`dynamic initializer for '
`eh vector constructor iterator'
`eh vector copy constructor iterator'
`eh vector destructor iterator'
`eh vector vbase constructor iterator'
`eh vector vbase copy constructor iterator'
`generic-class-parameter-
`generic-method-parameter-
`generic-type-
`H;t$@
`Hb(`
`ldp4r2
`local static destructor helper'
`local static guard'
`local static thread guard'
`local vftable'
`local vftable constructor closure'
`managed vector constructor iterator'
`managed vector copy constructor iterator'
`managed vector destructor iterator'
`non-type-template-parameter
`omni callsig'
`placement delete closure'
`placement delete[] closure'
`RTTI
`scalar deleting destructor'
`string'
`template static data member constructor helper'
`template static data member destructor helper'
`template-parameter
`template-parameter-
`template-type-parameter-
`tf_half_pixel_for_nn` is deprecated since opset 13, 
`typeof'
`udt returning'
`unknown ecsu'
`vbase destructor'
`vbtable'
`vcall'
`vector constructor iterator'
`vector copy constructor iterator'
`vector deleting destructor'
`vector destructor iterator'
`vector vbase constructor iterator'
`vector vbase copy constructor iterator'
`vftable'
`virtual displacement map'
`vtordisp{
`vtordispex{
`XXZ\\Z^ZZL`LX,
{ AUH
{ AVH
{ H+{
{ L+{
{ UATAUAVAWH
{"cat" : "
{%d,%d}
{%d,}
{(fE9f
{(H;w
{(Hcs,I
{(Lcc A
{@D8gDtYH
{0H;S(tnH
{8Hcw
{additionalDocumentation}
{flat}
{for 
{HcH(H
{hHcCtH;
{name}
{pLcchI
{XLccPI
|$ @8y
|$ 9{(t
|$ 9y0tX
|$ ATAVAWH
|$ AVH
|$ D!
|$ E3
|$ H;
|$ I;
|$ I+
|$ Lc
|$ M;
|$ M+
|$ M+|$
|$ UATAUAVAW
|$ UATAUAVAWH
|$ UATAVH
|$ UAVAWH
|$ UH
|$$H;
|$(A^
|$(E3
|$(H;
|$(H;S
|$(Hc
|$(I;
|$(M;
|$@@8x
|$@A_A^A]A\
|$@A9}
|$@E3
|$@fA
|$@fD
|$@H;
|$@H;|$H
|$@H;U
|$@H;W
|$@HcD$ H
|$@I;
|$@L;
|$@L9
|$@Lc
|$`H;
|$`L;
|$`L;|$x
|$<D;|$@
|$<D9|$@
|$0A_A^
|$0A9~(
|$0E3
|$0H!\$8
|$0H!t$0H
|$0H;
|$0H+
|$0I;
|$0I+
|$0I9Z0
|$0L9
|$49{ 
|$4D8d$1u
|$4D8o
|$4E3
|$8;3
|$8A_A^A\
|$8D!|$4A
|$8D9|$@
|$8D9|$<
|$8E3
|$8fA
|$8H;
|$8H;\$0t
|$8H9]
|$8I;
|$8L;
|$HD8l$@
|$hD8t$0
|$HD8wPt
|$hD9W(
|$hE3
|$hH;
|$HH+|$@H
|$hH97
|$hI;
|$HI;
|$HIc
|$hL;
|$HL;
|$p+}
|$pE3
|$PH;
|$pH;
|$PH+
|$PHc
|$PI;
|$pI;
|$PI+
|$PL;}
|$XA;_ 
|$XA9_8
|$XE3
|$xH;
|$XH;
|$xH;
|$xH;}
|$XH9
|$XH9k(
|$xI;
|$XI+
|$xI9
|$XL;
|$xLc
|$xM+
|,D;/|'I
|0^2^4N6F8D4D:H2,
|H(L"
|ZZ\X^X`db`dFfVbDhN`4
} for per-channel quantization. Actual:
} for per-tensor/layer quantization or shape {
}, actural: 
}, Got: 
}, input shape = {
}. Actual:
}. Got: 
}.I;Ihs
}/I;Ihs I
}=HcF
}0|$0J
}0t$ 
}8H;]
}EM;Q`s6I
}HX\$*b
}HX\I
}HX\K
}jI;Pxs[I
}nI;Qxs_I
}OHcC
}SHcN
}XHcuPI
~ ;G 
~ A8x t
~ HcF
~#LcO0H
~&M9z@
~(+~0
~(A+~0A
~(E+~0E
~(H;~0tGA
~(H;U
~(L9u
~)M9z@
~*L!D$ L!D$(
~*Lc@ 3
~;A8Y$t
~<I;]
~3a*~3a*~3a*~3a*
~CD9o(t
~GH99u
~HoL2
~HoT2
~HoTr
~kH;>r*D8~8u
~L$ L
~'M9z@
~PA8xPt\L
~PHc3D
~PM9z@
~T$ f
~vNcD
~VZXZZZ\X^``LbN^DdN\4
~YM9z@
+/+E+F+M+s+v+
+:0^E
++<>||~~
++index < c.size()
++Q5@.
++Q5@.Q5@.Q5@.Q5@.
+Attempting to get an input that does not exist.
+ba~H
+bA~H
+ba~H
+bA~H
+ba~H
+bA~H
+ba~H
+H D;
+v$x+v$xv$+xv+$xv$+x+$vx+$vx$v+x+$vx$+vx+v $+v $v $+v +$v $++$ v+$ v$ v++$ v$+ v+xv$+ v$v$ +v+ $v$ ++x$v+ $v$v ++ $v$ +v
< u\A
<%ucL
<%ufL
<&>B,
<(d* , (
<.>@@FB D&HFL
</assembly>
<:u,H
<:u0H
<:wBH
<:wsH
<:wuH
<?u@H
<?xml version='1.0' encoding='UTF-8' standalone='yes'?>
<_u{H
<`>t@tB`DTF:J
<`t)H
<}wRI
<}wTI
<0|LE
<0t0<2t
<1~?<3~+<4t_<5t
<2?~>
<assembly xmlns='urn:schemas-microsoft-com:asm.v1' manifestVersion='1.0'>
<CbA~H
<ellipsis>
<F>(<
<H>(<::f<H>(<::
<N>8< @P<
<Nba|H
<p_fd> is less than 0.
<p_fd> less than 0.
<parse error>
<QubH
<-u!E
<U+%.4X>
<uninitialized>
<unknown>
<vte<x
<X>`@PBRD`FBH`JR@:<8
<xuG@
=:0^E
=;FTu
=;GTu
==> Context: 
=4?~?
=H99t8L
=t4A+
>$@n>42M
>(p*P.
>8@0B*@
>bA|I
>H;>t
>N@2>
>Z@HB(@
>zDXFhHvJXLhN
0 == center_point_box_ || 1 == center_point_box_
0 == memory_size % kMinAllocationSize
0 0 06070>0?0
0 00070<0?0
0 2"y
0 2.426R8
0 R&T*J,2.40P2J
0 T".$,, 
0!0)080:0
0!0)080;0
0$*&0(@*0,,.*042=
0.0f <= ratio_value && ratio_value < 1.0f
0`446
0< t6<$t,<+t"<vt
0<0<0A0
0=0=0
00000
00000=0=0
01050;0;0
01050;0<0A0
0123456789-
0123456789-+Ee
0123456789ABCDEFabcdef-+Xx
0123456789ABCDEFabcdef-+XxPp
0123456789abcdefghijklmnopqrstuvwxyz
040904E4
08@HPX`hr
08@HPZ
08@HR
08@P^T
08Bdg
09AZ__az
0A^_]
0A^_^
0A^_^[]
0A^_^][
0A^A]_^]
0A^A]A\_^
0A_A\_
0A_A^_
0A_A^_^[
0A_A^_^]
0A_A^A\
0A_A^A\_^
0A_A^A\_^[]
0A_A^A]
0A_A^A]A\]
0A_A^A]A\_
0A_A^A]A\_^[
0A_A^A]A\_^]
0bj^lNnFpDlDrZj4
0bQ~I
0f;D$Vu
0-g-o-p-
0H;t$Pt
0HcL$ H
0Lc{(D
0P2&6
0P2,422V6&846X8"6z8::T<X6J>T@L6bBM
0twA+
0X2h4d6X8h:j<X>h@
1 == capability.nodes.size()
1 2)2H2O2Q2_2
1 2_2
1.4.0
1.7.1
1.7.210427-2300.1.dml-1.6.8814570
1/111
1D input tensor
1D output tensor
1-D tensor of 2 elements: [crop_height, crop_width]. All cropped image patches are resized to this size. Both crop_height and crop_width need to be positive.
1-D tensor of axes that `starts` and `ends` apply to.
1-D tensor of ending indices (exclusive) of corresponding axis in axes
1-D tensor of floats
1-D tensor of shape (num_rois,) with each element denoting the index of the corresponding image in the batch.
1-D tensor of starting indices of corresponding axis in `axes`
1D9x,u
2&2(D*
2*2G2P2P2`2
2:@.2
2:0^E
2^6L8
2`2~2`
240|2<0e
2bb}H
2D matrix with shape (K,N)
2D matrix with shape (M,N)
2D8&t-L
2D9&t-L
2F482 6]
2H>^@HB
2H4(2]
2h4j6
2I;@8t:M
2L9&t-L
2N406
2P4.6
2P4.6,.
2P4.6,.`4.6,.
2P4.6,.62P4.6,.
2P4\2
2r4&6<4
32-bit hash value.
3333333
39r@t
3bb}H
4$bQ}X
468xH
4b6r8|:b<d>2B
4d8.:
4t6`8d:J<`>zB*D:F
4v6`8d:N<`>vB2D:F
4X6p8d:F<`8d@0BzD.B
5!8!0-g-
5:0^E
5-0^E
6 8(4}
6.8^: <
6:0^E
6`8t:
6bA|H
6HcB@H;D$ht
6JCy7JCy7JCy7JCy7
6N826
6N826 :
6v8n:5
6X8`:6>
7fA;E
'7JCy7
8$u'E
8,u5H
8?u$H
8@B(D`8pB@D(Bd8@B(Dn8`B@D(Bn8@B(DB8
8@HP(
8@HP^T
8@HPX`
8@HPX`hp
8@HPX`hpx
8@HPX`hpz
8@HPX`hpz4
8@HPX`j
8@HPX`j4
8@HPXb4
8@HPZ
8@J0^E
8@t1H
8@ttD
8@tTL
8@u4H
8\$(t,I
8\$(u
8\$(u&H
8\$`t*H
8\$0t
8\$0u
8\$1t
8\$1u
8\$tt1
8]tmH
8^,t7HcF
8^:^<b>P@lB
8^u3H
8_^][
8_0t"
81ueH
89:u6
8A^_^[
8A_A^_^][
8A_A^A]A\_^[]
8A_A^A]A\_^][
8Anu$
8B0^E
8D$Dt
8d3d12.dll
8giP9giP9giP9giP9
8L!8L
8L$8D
8N:^>V@f>VBf>VDf>VFf>VHf>VJf>VLf>VNf>VPf>VRf>i
8N:88 <
8n:T<H>b@NB`D\:,88
8O~/A
8p H:
8P:.<
8P:.<,2
8QptP8Q
8-uCH
8-uGH
8-uuH
8W(t^3
8X:x<6@^BpDBF(D1
8XugH
9$t>H
9$uFH
9(uIH
9)~P3
9:0^E
9?u\3
9@u7H
9\$ u H
9\$Pu
9\$Tu
9\$xu
9]xtV3
9_ ~dL
9{(~4H
9|$Tu
9}8~ZI
9=6')
9=X))
961c151d2e87f2686a955a9be24d316f1362bf21 3.7.1
99~CE
9A(t(
9h ~]3
9k ~GD8{
9o ~/H
9p ~2
9p ~GD8x
9p t.
9p,tHH
9Q ~!LcA H
9r(u1H
9rHu\L
9rxukL
9S v2Hc
9u@~[H
9uLv\A+
9x ~tH
9X t"D9` 
9x t$9p 
9X t(
9X@u+
9Y ~rD
9y(ubH
9Z(uCH
9Z(uUH
9zv.H
A != nullptr && B != nullptr
A + (M * K) <= A_end
A + (M * lda - (lda - K)) <= A_end
A 0-D tensor containing a single value corresponding to the number diagonals above or the main diagonal to exclude or include.Default value is 0 if it's not specified.
A 1-D input tensor that is to be processed.
A 1-D INT64 tensor containing the the count of each element of 'uniques' in the input 'x'
A 1-D INT64 tensor of the same size as 'x' containing the indices for each value in 'x' in the output 'uniques'
A 1-D tensor of the same type as 'x' containing all the unique values in 'x' sorted in the same order that they occur in the input 'x'
A 1-D values of (height, width).
A 1-D values of (leftBorder, topBorder, rightBorder, bottomBorder).
A 2D Matrix that represents the distance between each pair of the two collections of inputs.
A collection of intercepts.
A collection of weights of the model(s).
A Conv/ConvTranspose node has both 'auto_pad' and 'pads' attributes
A D8B t:H
A dimension cannot be less than -1, got 
A dso with name 
A float.
A H;AXuBH
A H;B t
A H;B t'
A H+A
A H9A
A H9YPD
A H9YPH
A high-performing neural network activation function.The GELU nonlinearity is
A L;A(
A L;AhL
A list of 2 (or 4 if bidirectional) activation functions for update, reset, and hidden gates. The activation functions must be one of the activation functions specified above. Optional: See the equations for default if not specified.
A list of 3 (or 6 if bidirectional) activation functions for input, output, forget, cell, and hidden. The activation functions must be one of the activation functions specified above. Optional: See the equations for default if not specified.
A list of floats.
A list of integers, along which to reduce. The default is to caculate along axes [0,2,3] for calculating mean and variance along each channel. Two variables with the same C-coordinate are associated with the same mean and variance.
A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor.
A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data).
A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given.
A list of ints.
A list of labels.
A list of strings. One and only one of 'keys_*'s should be set.
A list of strings. One and only one of 'value_*'s should be set.
A M9A
A node with a function body within a subgraph within another function body is currently not supported in ORT
A shape tensor must be a vector tensor.
A string indicating the desired element type of the output tensor, one of 'TO_FLOAT', 'TO_STRING', 'TO_INT64'.
A string to use when an input integer value is not found in the map.<br>One and only one of the 'default_*' attributes must be defined.
A string vocabulary array.<br>One and only one of the vocabularies must be defined.
A string.
A value that needs replacing.
A$]Z`I
A(9w`
A(H+A H
A,]R@
A,A;@
A:8ucI
A;@(u
A;@,t
A;^ |
A;_ |
A;A(u
A;A,u
A;A0t
A;E$u
A;P |
A;Q |
A;u |
A;U ~
A;u8H
A@A!AHI
A@H90t
A\A]A^_^[]
A\A]A^A__^[]
A\A^_^[]
A]A\]
A]A\_^[]
A^_^[
A^_^[]
A^_^]
A^_^][
A^A\]
A^A\_^]
A^A]]
A^A]_^]
A^A]A\_]
A^A]A\_^[]
A__^[]
A_A\]
A_A]_^]
A_A]A\_]
A_A]A\_^[
A_A]A\_^[]
A_A^]
A_A^^
A_A^^[]
A_A^_
A_A^_[]
A_A^_^[
A_A^_^[]
A_A^_^]
A_A^_^][
A_A^A\
A_A^A\_]
A_A^A\_^
A_A^A\_^[
A_A^A\_^[]
A_A^A]_]
A_A^A]_^
A_A^A]_^[
A_A^A]_^[]
A_A^A]A\]
A_A^A]A\^[]
A_A^A]A\_
A_A^A]A\_[]
A_A^A]A\_^
A_A^A]A\_^[
A_A^A]A\_^[]
A_A^A]A\_^]
A_A^A]A\_^][
A_scale
a_scale
A_zero_point
a_zero_point
A`H+AXH
A`H9Ah
A|U@ 
A+>Hc
A+M0A
A+U I
A<_B`
A<X@@
A0:(I/
A0H+A(H
A0H9A(
A0H9A(t H
A0L#D$(L
A0LcA
A4]J 
A4XH 
A5XH 
A8@8u
A8\$Pt
A8]TM
A8]Xt
A8~`t>H
A8~|t
A84<u
A8A8u
A8C8u
A8D8B8t4L
A8H;A@
A8H;A@u
A8H+A0H
A8Opt
A8v0tZH
A8w)t
A9@(u
A9@,u
A9]T~MH
A9]T~OM
A9^ ~
A9{(u
A96tZH
A9A(u
A9t$(
A9t$Pu;H
A9UTu
A9WTu
A9x(tfH
A9y(ukH
Access violation - no RTTI data!
ACLExecutionProvider
Acosh
AcquireSRWLockExclusive
across_channels
activation
activation and leaky_relu_alpha.
activation.
activation_
activation_alpha
activation_beta
activation_func_names.size() == static_cast<size_t>(num_directions_) * 2
activation_func_names.size() == static_cast<size_t>(num_directions_) * 3
activation_gamma
activation_params
activation_params count mismatch
activation_size
ActivationDescCount
ActivationDescs
activations
activations, avoiding computation if the input is invalid (as in, the
activations_.size() == static_cast<size_t>(num_directions)
adapterLuidHighPart
adapterLuidLowPart
add_B_tensor_proto
AddFoldedRange recurses too much.
Adding default CPU execution provider.
AddInitializedTensor already has tensor with name 
addition
Additional elements added to the side with higher coordinate indices in the output. Each padding value in "output_padding" must be less than the corresponding stride/dilation dimension. By default, this attribute is a zero vector. Note that this attribute doesn't directly affect the computed output values. It only controls the selection of the computed values, so changing this attribute only adds or removes output elements. If "output_shape" is explicitly provided, "output_padding" does not contribute additional size to "output_shape" but participates in the computation of the needed padding amount. This is also called adjs or adjustment in some frameworks.
address family not supported
address in use
address not available
Adlam
AE9r 
Affine
affine
Affine takes one input data (Tensor<T>) and produces one output data
aggregate_function
AhH;A`}J
AHH+A@H
AhH+Q
AHM;APu
ai.onnx
ai.onnx.ml
ai.onnx.preview.training
ai.onnx.training
ALH9y v&
align_corners
All implicit inputs should have OrtValue instances by now. 
All inputs must have the same shape
All inputs to Concat must have same rank
All inputs to 'Range' op must be of the same type
All nodes have been placed on [
All scan outputs MUST be tensors
All Tensor and Sequence types
All Tensor types
all types
Allocated memory at 
Allocation of memory pattern buffer for 
Allocation of tensor types requires a shape.
allocator
allocator != nullptr
Allocator already registered for 
Allocator with this OrtMemoryInfo is already registered.
allocator_ptr_
AllocPlan(ml_value_idx).program_counter.Ends().back() == program_counter
ALLOW_RELEASED_ONNX_OPSET_ONLY
allowed_activations.find(activations_[direction]) != allowed_activations.end()
allowed_directions.find(direction_) != allowed_directions.end()
Alpha
alpha
alpha == 1.0f && (beta == 0.0f || beta == 1.0f)
alpha_ > 0.0f
already connected
AMDiuA
An attribute specifying the number of scan_inputs M. 
An axes tensor must be a scalar or a 1-D tensor.
An axes tensor must be a vector tensor.
An axis tensor must be a scalar tensor.
An input tensor to hash.
An integer to use when an input string value is not found in the map.<br>One and only one of the 'default_*' attributes must be defined.
An integer vocabulary array.<br>One and only one of the vocabularies must be defined.
An integer.
An optional list of K flags, one for each scan_output. The i-th element of the list specifies whether the i-th scan_output should be constructed by appending or prepending a new value in each iteration: 0 indicates appending and 1 indicates prepending. If omitted, all scan_output tensors will be produced by appending a value in each iteration.
An optional list of K flags. The i-th element of the list specifies the axis for the i-th scan_output. The scan outputs are accumulated along the specified axis. If omitted, 0 will be used as the scan axis for every scan_output.
An optional list of K flags. The i-th element of the list specifies the axis for the i-th scan_output. The scan outputs are accumulated along the specified axis. If omitted, 0 will be used as the scan axis for every scan_output. Negative value for an axis means counting dimensions from the back. Accepted range is [-r, r-1].
An optional list of M flags. The i-th element of the list specifies the axis to be scanned (the sequence axis) for the i-th scan_input. If omitted, 0 will be used as the scan axis for every scan_input.
An optional list of M flags. The i-th element of the list specifies the axis to be scanned (the sequence axis) for the i-th scan_input. If omitted, 0 will be used as the scan axis for every scan_input. Negative value for an axis means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
An optional list of M flags. The i-th element of the list specifies the direction to be scanned for the i-th scan_input tensor: 0 indicates forward direction and 1 indicates reverse direction. If omitted, all scan_input tensors will be scanned in the forward direction.
an optional list of strings attribute that contains a list of separators - regular expressions to match separators Two consecutive segments in X connected by a separator would be divided into two tokens. For example, if the input is "Hello World!" and this attribute contains only one space character, the corresponding output would be ["Hello", "World!"]. To achieve character-level tokenization, one should set the 'separators' to [""], which contains an empty string.
An optional string. Token's regular expression in basic POSIX format (pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_03). If set, tokenizer may produce tokens matching the specified pattern. Note that one and only of 'tokenexp' and 'separators' should be set.
An OrtValue for this name has already been added.
An split tensor must be a vector tensor.
Anatolian_Hieroglyphs
Another operand has a dim value of 
APD8BP
ApH+9H
APH+AHH
api-ms-win-core-com-l1-1-0.dll
api-ms-win-core-debug-l1-1-0.dll
api-ms-win-core-delayload-l1-1-0.dll
api-ms-win-core-delayload-l1-1-1.dll
api-ms-win-core-errorhandling-l1-1-0.dll
api-ms-win-core-fibers-l1-1-0.dll
api-ms-win-core-file-l1-1-0.dll
api-ms-win-core-file-l1-2-0.dll
api-ms-win-core-handle-l1-1-0.dll
api-ms-win-core-heap-l1-1-0.dll
api-ms-win-core-interlocked-l1-1-0.dll
api-ms-win-core-libraryloader-l1-2-0.dll
api-ms-win-core-localization-l1-2-0.dll
api-ms-win-core-memory-l1-1-0.dll
api-ms-win-core-path-l1-1-0.dll
api-ms-win-core-processenvironment-l1-1-0.dll
api-ms-win-core-processthreads-l1-1-0.dll
api-ms-win-core-processthreads-l1-1-1.dll
api-ms-win-core-processthreads-l1-1-3.dll
api-ms-win-core-processtopology-obsolete-l1-1-0.dll
api-ms-win-core-profile-l1-1-0.dll
api-ms-win-core-rtlsupport-l1-1-0.dll
api-ms-win-core-string-l1-1-0.dll
api-ms-win-core-synch-l1-1-0.dll
api-ms-win-core-synch-l1-2-0.dll
api-ms-win-core-sysinfo-l1-1-0.dll
api-ms-win-core-sysinfo-l1-2-0.dll
api-ms-win-core-util-l1-1-0.dll
api-ms-win-core-winrt-error-l1-1-0.dll
api-ms-win-core-winrt-error-l1-1-1.dll
api-ms-win-core-winrt-string-l1-1-0.dll
api-ms-win-crt-locale-l1-1-0.dll
api-ms-win-crt-math-l1-1-0.dll
api-ms-win-crt-private-l1-1-0.dll
api-ms-win-crt-runtime-l1-1-0.dll
api-ms-win-crt-string-l1-1-0.dll
api-ms-win-eventing-provider-l1-1-0.dll
ApL+a
apply softmax to elements for dimensions softmax_axis or higher
AQ5@.
Arabic
arg_num < arg_counts.size()
ArgMax
ArgMin
Argument is not a tensor
argument list too long
Argument mismatch when removing edge.
argument out of domain
Argument type mismatch when adding edge.
Armenian
ArmNNExecutionProvider
array
Array of sequence lengths.  len(seq_lengths) should equal batch size N.
ArrayFeatureExtractor
AScaleTensor
Asinh
asymmetric
At least one element in the sequence is of a type different from others.
At least one output should be requested.
At most one dimension can be -1.
Atanh
ATAVAWH
ATensor
Attempt to retrieve final output before it was set.
Attempt to use DefaultLogger but none has been registered.
Attempted a typeid of nullptr pointer!
Attempting to broadcast an axis by a dimension other than 1. 
Attempting to get an output that does not exist.
Attempting to get index for an input which does not exist.
Attention
Attention layer weight shape error! Expected: {
Attention mechanism memory sequence lengths must have shape {
Attention mechanism memory sequence lengths value must in (0, 
Attention mechanism memory shape error! Expected: {
Attention memory layer weight shape error! Expected:{
Attention query layer weight shape error! Expected:{
Attention v weight shape error! Expected:{
AttentionFusion
Attibute name and type don't match
AttnLSTM
Attribute 
Attribute '
Attribute (name: 
Attribute `broadcast=1` needs to be passed to enable broadcasting.
Attribute axes has incorrect length
Attribute blocksize is not set.
Attribute border needs to be specified with four border elements, got 
attribute case_change_action has invalid value
attribute case_change_action is not set
Attribute dilations has incorrect size
Attribute dtype should be of integer type and specify a type.
Attribute expected to have a one-dim tensor
Attribute expected to have tensor type
attribute is_case_sensitive is not set
Attribute kernel_shape has incorrect size
Attribute kernel_shape has incorrect size.
Attribute kernel_shape must be specified
Attribute kernel_shape must be specified.
attribute mark is not set
attribute mincharnum is not set
attribute mincharnum must have a positive value
Attribute name and type don't match for '
attribute pad_value is not set
Attribute pads has incorrect length
Attribute pads has incorrect size
Attribute pads has incorrect size.
Attribute perm of Transpose has an invalid value. Value 
Attribute pooled_shape has incorrect length
Attribute pooled_shape must be specified
Attribute 'scales' is required.
Attribute 'scales' must have floats type.
Attribute shape is not set.
Attribute specification type mismatch.
Attribute strides has incorrect size
Attribute strides has incorrect size.
Attribute to is not set.
Attribute value for pads is required
Attribute 'value' of Constant node must exist with 'Tensor' data.
Attribute 'value_float' expect a float.
Attribute 'value_floats' expect a list of floats.
Attribute 'value_int' expect an integer.
Attribute 'value_ints' expect a list of integers.
Attribute 'value_string' expect a string.
Attribute 'value_strings' expect a list of strings.
Attribute: 
AUAVAWH
Authu
auto_pad
auto_pad == AutoPadType::NOTSET
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = ceil(input_shape[i] / strides[i])` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER.
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = input_shape[i] * strides[i]` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER.
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that the output size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the beginning for SAME_LOWER. VALID mean no padding. DEPRECATION NOTE: auto_pad is only intended to support legacy uses, and for framework authors, one is explicitly encouraged to use explicit padding specified in the pads attribute.
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that the output spatial size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the beginning for SAME_LOWER. VALID mean no padding.
Available memory of 
AVERAGE
AveragePool
Avestan
axes as an input and attribute cannot be specified at the same time.
'axes' attribute must not contain any duplicates
'axes' has a duplicate axis
'axes' has an axis outside of the tensor dimension count
'axes' has an out of range axis
'axes' has duplicates
Axes input is null
Axes that `starts` and `ends` apply to. It's optional. If not present, will be treated as [0, 1, ..., len(`starts`) - 1].
axes_right_stride >= 0 && static_cast<uint64_t>(axes_right_stride) < std::numeric_limits<size_t>::max()
axes_tensor != nullptr
axes_tensor->Shape().NumDimensions() == 0 || axes_tensor->Shape().NumDimensions() == 1
axes_tensor->Shape().NumDimensions() == 1
AxHcApM
axis 
axis <= X_NumDims && axis >= -X_NumDims
axis == 1 || axis == largest
axis >= -tensor_rank && axis <= tensor_rank - 1
Axis has less than the requested k elements.
'axis' must be in [
axis must be in [-r, r-1]
'axis' must be in [-rank(indices), rank(indices)-1]
'axis' must be in [-rank(indices)-1, rank(indices)]
axis must be in [-rank, rank-1].
axis must be in [-rank, rank-1]. input rank was 
Axis must be within range [
axis tensor can be int32 or int64 only
Axis tensor must be provided to the CumSum op
Axis tensor should be 0D or 1D
Axis tensor should be of type `int32_t` or `int64_t`
axis_tensor->Shape().IsScalar()
AxisCount
AxisDirection
AXL9I`t
AZeroPointTensor
b  ".
b  ".$
B ^"D
B + (N * ldb - (ldb - K)) <= B_end
B d"L
b H"*$z&
B H;A t
B H+B
B H9A 
B H9s
B I;@ u
B r"h 
B R$J&
b"-@,
B"DzF$H>J
B(,.88T:.<,
B(<.*8T:.<,>(BTD.F,>
B(9C(u
B(A9@
B(A9@(u
B(A9F
B(I9A(u
B(VLX
B,9C,u
B,A9F
b,d0.2,
B;Glu
B@D(B
B@D(Bd8@B(Dd8@B(D
B@D(BJ8A
B@D(BR8dB@D(B$8.h
b_scale
B_scale
B_zero_point
b_zero_point
B`9A`
B`D80
b`fRhff
b},4C
b}.<C
b}/<C
b}-4C
B}X\$
B}X|$
B}Xl$
B}XL$
B}Xl$
B}XL$
B}Xl$
B}XL$
B}Xl$
B}XL$
B}Xl$
B}XL$
B}Xl$
B}XL$
B09C0t
B0A9F
B0HcJ(M
B0I;B0
B6`:.<
b8H;u(t
BA;E$u
ba|H(
bad address
bad allocation
Bad arg in kInstAltMatch: 
Bad arg in kInstCapture: 
Bad args: nsubmatch=
bad array new length
Bad call to ParseState::ParsePerlFlags
bad cast
bad conversion
bad dimensions
bad exception
bad file descriptor
Bad final char: 
bad function call
Bad hex digit 
bad locale name
bad message
Bad node spec: 
Bad optional access
Bad read pointer - no RTTI data!
Bad reference count 
bad repetition operator
BAhL+
Balinese
Bamum
Base values for classification, added to final class score; the size must be the same as the classes or can be left unassigned (assumed 0)
base_values
Bassa_Vah
Batak
Batch dimension should match for MatMul;
batch_axis
batch_axis != time_axis
batch_axis < 2
batch_dims
batch_indices
batch_indices shape input tensor has wrong dimension
batch_size is < 1
BatchDimensionCount
BatchIndicesTensor
BatchNormalization
BatchNormalizationAddFusion
BatchNormalizationMulFusion
bb}H{
bb}HX
bb}HX'H
bbeHQ
BBH(J
Begin execution
Bengali
beta is expected to have 1 dimension, got 
beta is expected to have 1 dimensions, got 
beta is expected to have size of 
Beta should be of shape (hidden_size). 
beta_ > 0.0f
Beta1
Beta2
BFD(BJ@dBFD(BP@dBFD(B
bfloat16
Bhaiksuki
BhH9Ah
BHHcJ@M
BHI;BPu
Bias applied to each channel, same size as C.
Bias Gelu.
bias is expected to have 1 dimension, got 
Bias size (
Bias tensor.
BiasGelu
BiasGeluFusion
BiasSoftmax
BiasSoftmaxFusion
BiasTensor
bidirectional
bilinear
BILINEAR
Binarizer
BinForSize(bin_size * 2 - 1) == BinFromIndex(b)
BinForSize(bin_size * 2) != BinFromIndex(b)
BinForSize(bin_size + 255) == BinFromIndex(b)
BinForSize(bin_size) == BinFromIndex(b)
BinFromIndex(c->bin_num)->free_chunks.erase(h) > 0
BitShift
BLDlF
Blocks of [blocksize, blocksize] are moved.
blocksize
BlockSize
Blocksize must be positive
BMA:@
BMA:A
bn_B_tensor_proto
bn_mean_tensor_proto
bn_scale
bn_scale_tensor_proto
bn_var_tensor_proto
BND8B FE
BNfA;@
Bool to determine if hidden state is zeroes or passed along for timesteps past the given sequence_length.
boolean
Boolean whether to mark the beginning/end character with start of text character (0x02)/end of text character (0x03).
Boolean. Indicates whether upper or lower part of matrix is retained. Default is true.
Boolean. Whether the identification of stop words in X is case-sensitive. Default is false
Bopomofo
border
'Border' attribute must be present and must contain exactly 4 values - (left_border, top_border, right_border, bottom_border)
Both `data` and `indices` input tensors in GatherND op need to have rank larger than 0.
both data and indices tensor need to have rank larger than zero.
boxes
boxes and scores should have same num_batches.
boxes and scores should have same spatial_dimension.
boxes must be a 3D tensor.
boxes_tensor
Bp9Ap
BPI+BHH
bq$HX\
bq,HXS
bQ<@_
bQ<HX@
bQ4HXL
Brahmi
Braille
BRANCH_EQ
BRANCH_GT
BRANCH_GTE
BRANCH_LEQ
BRANCH_LT
breHQ
broadcast
broadcast bias across input for dimensions broadcast_axis to softmax_axis-1
Broadcast Output range [
broadcast_axis
BroadcastLooper requires two tensors as input.
broken pipe
BScaleTensor
BTensor
Buffer containing the initializer must be owned by the user.
buffer size is too small for string
buffers_.find(location) == buffers_.end()
BUG! Report to onnxruntime team.
Buginese
Buhid
bumped the operator version but 
BXDhFrHXJjN
BxH+Q
BxHcJpM
by either re-generating the model with latest exporter/converter 
bytemap range 
BZD^FFH.J
bZdFfZhDdLjLb,
BZeroPointTensor
C + (M * ldc - (ldc - N)) <= C_end
C +C0
C = ((A - A_zero_point) * (B - B_zero_point)) * (A_scale * B_scale)/C_scale + C_zero_point
C = (A_scale * (A - A_zero_point) + B_scale * (B - B_zero_point))/C_scale + C_zero_point
C 9H 
C 9P 
C E9$
C H+C
C H98
C HcL
C L+C
C(;B(
C(+C,
C(+C0
C(D;0
C(D90
C(H;C(u
C(H9C@u
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\controlflow\defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\controlflow\old.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\generator\defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\generator\old.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\logical\defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\logical\old.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\math\defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\math\old.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\nn\defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\nn\old.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\object_detection\defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\object_detection\old.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\quantization\defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\quantization\old.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\reduction\defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\reduction\old.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\rnn\defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\rnn\old.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\sequence\defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\tensor\defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\tensor\old.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\traditionalml\defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\onnx\onnx\defs\traditionalml\old.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\protobuf\src\google/protobuf/parse_context.h
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\arena.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\generated_message_util.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream_impl.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream_impl_lite.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\message_lite.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\repeated_field.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\re2\re2/walker-inl.h
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\re2\re2\bitstate.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\re2\re2\compile.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\re2\re2\dfa.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\re2\re2\nfa.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\re2\re2\onepass.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\re2\re2\parse.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\re2\re2\prog.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\re2\re2\re2.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\re2\re2\regexp.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\re2\re2\simplify.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\re2\re2\tostring.cc
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\wil\include\wil/wrl.h
C:\apilot\agent\_work\6\s\engine\lotus\cmake\external\wil\include\wil\resource.h
C:\apilot\agent\_work\6\s\engine\lotus\include\onnxruntime\core/common/const_pointer_container.h
C:\apilot\agent\_work\6\s\engine\lotus\include\onnxruntime\core/common/logging/logging.h
C:\apilot\agent\_work\6\s\engine\lotus\include\onnxruntime\core/framework/data_types.h
C:\apilot\agent\_work\6\s\engine\lotus\include\onnxruntime\core/framework/data_types_internal.h
C:\apilot\agent\_work\6\s\engine\lotus\include\onnxruntime\core/framework/ml_value.h
C:\apilot\agent\_work\6\s\engine\lotus\include\onnxruntime\core/framework/op_kernel.h
C:\apilot\agent\_work\6\s\engine\lotus\include\onnxruntime\core/framework/tensor.h
C:\apilot\agent\_work\6\s\engine\lotus\include\onnxruntime\core/graph/graph.h
C:\apilot\agent\_work\6\s\engine\lotus\include\onnxruntime\core/optimizer/graph_transformer.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops/cpu/activations.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops/cpu/crop_and_resize.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\bahdanau_attention.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\deep_cpu_attn_lstm.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\deep_cpu_attn_lstm.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\attention.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\attention_cpu_base.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\bias_gelu.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\embed_layer_norm.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\cdist.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\cpu_contrib_kernels.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\crop.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\element_wise_ops.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\expand_dims.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\fused_conv.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\fused_gemm.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\image_scaler.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\layer_norm.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\matmul_integer16.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\maxpool_with_mask.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\murmur_hash3.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\nchwc_ops.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\nchwc_ops.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_binary_op.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_global_average_pool.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_lookup_table.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\attention_quant.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\dynamic_quantize_matmul.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\nhwc_max_pool.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\skip_layer_norm.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\tokenizer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\trilu.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\trilu.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\contrib_ops\cpu\word_conv_embedding.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/common/safeint.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/framework/bfc_arena.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/framework/execution_frame.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/framework/execution_providers.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/framework/feeds_fetches_manager.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/framework/func_kernel.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/framework/mem_pattern_planner.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/framework/node_index_info.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/framework/op_kernel_context_internal.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/framework/ort_value_tensor_slicer.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/framework/sequential_execution_plan.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/framework/TensorSeq.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/graph/function_impl.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/graph/model_load_utils.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/optimizer/attention_fusion_helper.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/optimizer/initializer.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/platform/path_lib.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/common.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/activation/activations.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/controlflow/scan_utils.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/element_wise_ranged_transform.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/generator/constant_of_shape_base.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/generator/random.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/math/clip.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/math/element_wise_ops.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/math/gemm.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/math/matmul_helper.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/ml/cast_map.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/ml/category_mapper.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/ml/dictvectorizer.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/ml/feature_vectorizer.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/ml/label_encoder.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/ml/ml_common.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/ml/normalizer.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/nn/batch_norm.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/nn/conv_attributes.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/nn/conv_transpose_attributes.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/nn/dropout_op.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/nn/flatten.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/nn/instance_norm.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/nn/lp_norm.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/nn/lrn.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/nn/pool_attributes.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/nn/pool_base.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/nn/roi_pool.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/nn/shrink.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/nn/unpool.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/reduction/reduction_ops.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/deep_cpu_gru.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/rnn.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/rnn_helpers.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/concat.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/gather.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/identity_op.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/mean_variance_normalization.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/pad.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/reshape.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/slice.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/space_depth_ops.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/split.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/squeeze.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/transpose.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/unsqueeze.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/upsample.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/utils.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/MLOperatorAuthorHelper.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/OperatorHelper.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/SchemaInferenceOverrider.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core/session/inference_session.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\common\logging\logging.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\common\path.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\common\profiler.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\common\status.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\common\str_helper.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\common\threadpool.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\flatbuffers\flatbuffers_utils.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\allocation_planner.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\allocator.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\allocatormgr.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\bfc_arena.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\data_transfer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\data_transfer_manager.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\data_types.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\endian_utils.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\ex_lib_loader.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\execution_frame.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\execution_provider.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\feeds_fetches_manager.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\fuse_nodes_funcs.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\graph_partitioner.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\kernel_registry.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\mldata_type_utils.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\node_index_info.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\op_kernel.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\op_kernel_info.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\op_node_proto_helper.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\ort_value_tensor_slicer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\parallel_executor.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\provider_bridge_ort.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\sequential_executor.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\session_options.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\session_state.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\session_state_utils.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\simple_tensor_allocator.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\tensor.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\tensor_allocator_with_mem_pattern.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\tensor_shape.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\tensor_type_and_shape.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\tensorprotoutils.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\framework\utils.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\graph\contrib_ops\contrib_defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\graph\contrib_ops\nchwc_schema_defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\graph\contrib_ops\nhwc_schema_defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\graph\contrib_ops\quantization_defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\graph\dml_ops\dml_defs.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\graph\function.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\graph\graph.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\graph\graph_flatbuffers_utils.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\graph\graph_utils.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\graph\graph_viewer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\graph\model.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\graph\schema_registry.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\attention_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\bias_gelu_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\bias_softmax_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\common_subexpression_elimination.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\constant_folding.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\conv_activation_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\conv_add_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\conv_bn_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\conv_mul_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\dynamic_quantize_matmul_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\embed_layer_norm_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\fast_gelu_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\free_dim_override_transformer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\gelu_approximation.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\gelu_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\gemm_activation_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer_mgr.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer_utils.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\initializer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\insert_cast_transformer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\layer_norm_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\matmul_add_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\matmul_integer_to_float.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\matmul_scale_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\nchwc_transformer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\nhwc_transformer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\optimizer_execution_frame.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\relu_clip_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\reshape_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\rule_based_graph_transformer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\skip_layer_norm_fusion.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\transformer_memcpy.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\optimizer\unsqueeze_elimination.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\platform\windows\env.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\if.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\loop.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_8.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_9.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_utils.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\cpu_execution_provider.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\generator\constant_of_shape.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\generator\random.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\clip.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\cumsum.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\det.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_auxiliary_ops.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_compute_preprocessor.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_typed_compute_processor.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\element_wise_ops.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\gemm.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\gemm_helper.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\hardmax.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\matmul.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\matmul_integer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\quantize_linear_matmul.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\softmax.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\math\top_k.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\ml\cast_map.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\ml\feature_vectorizer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\ml\imputer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\ml\linearclassifier.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\ml\linearregressor.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\ml\ml_common.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\ml\onehotencoder.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\ml\scaler.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmclassifier.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmclassifier.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmregressor.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\ml\tree_ensemble_aggregator.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\ml\tree_ensemble_common.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\ml\zipmap.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv_integer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv_transpose.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\nn\instance_norm.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\nn\lrn.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\nn\pool.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\nn\qlinearconv.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\nn\roi_pool.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\nn\string_normalizer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\nn\tfidfvectorizer.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\nn\Unpool.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\non_max_suppression.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\non_max_suppression.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\roialign.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\reduction\reduction_ops.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\deep_cpu_gru.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\deep_cpu_lstm.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\lstm_base.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\lstm_base.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\rnn.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\rnn_helpers.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\sequence\concat_from_sequence.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\sequence\sequence_ops.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\cast_op.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\compress.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\concat.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\dynamicquantizelinear.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\eye_like.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_elements.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_elements.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_nd.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\isinf.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\nonzero_op.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\onehot.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\pad.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\quantize_linear.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\reshape_helper.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\reverse_sequence.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\reverse_sequence.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\scatter.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\scatter_nd.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\slice.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\space_depth_ops.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\split.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\tile.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\transpose.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\unsqueeze.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\upsample.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\dml_provider_factory.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\AbiCustomRegistry.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\BucketizedBufferAllocator.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\CommandAllocatorRing.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\CommandQueue.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DescriptorPool.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DmlCommandRecorder.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DmlCommon.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ExecutionContext.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ExecutionProvider.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/ApiHelpers.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/ApiTraits.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/GeneratedSchemaHelpers.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/SchemaHelpers.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\FusedGraphKernel.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GpuEvent.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphDescBuilder.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphKernelHelper.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphPartitioner.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphTransformer.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\MLOperatorAuthorImpl.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\MLOperatorAuthorImpl.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperator.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorActivation.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorAffine.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorBatchNormalization.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCast.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConcat.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConstantOfShape.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConvInteger.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConvolution.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCopy.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCrop.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCumSum.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorDepthToSpace.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorDynamicQuantizeLinear.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorEinSum.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorElementWise.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorExpand.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorEyeLike.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorGather.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorGemm.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorInstanceNormalization.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorLocalResponseNormalization.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorLpNormalization.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMatMul.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMatMulInteger.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMeanVarianceNormalization.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMemcpy.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorNeg.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorOneHot.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorPadding.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorPooling.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearAdd.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearConv.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearMatMul.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRange.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRecurrentNeuralNetwork.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorReduce.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorResize.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorReverseSequence.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRoiAlign.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRoiPooling.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorScatter.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSlice.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSpaceToDepth.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSplit.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTile.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTopk.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTranspose.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorValueScale2D.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\OperatorRegistration.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\OperatorUtility.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\PooledUploadHeap.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ReadbackHeap.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\TensorDesc.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\OperatorAuthorHelper\MLOperatorAuthorHelper.h
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\OperatorAuthorHelper\OperatorHelper.cpp
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\session\custom_ops.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\session\environment.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\session\inference_session.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\session\inference_session_utils.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\session\IOBinding.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\session\onnxruntime_c_api.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\session\ort_env.cc
C:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\util\math_cpu.cc
C:\apilot\agent\_work\6\s\engine\lotus\winml\adapter\winml_adapter_dml.cpp
C:\apilot\agent\_work\6\s\engine\lotus\winml\adapter\winml_adapter_session.cpp
C@f9{Dr
C@H+C8H
C_scale
c_shape != nullptr
c_shape is required if c_data is provided
C_zero_point
C++/WinRT version:2.0.190620.2
c->in_use() && (c->bin_num == kInvalidBinNum)
C0H9F0
C0HcH
c2->prev == h1
C8D8m
C8H;C@u
c8H;u t
C8H+C0H
c8L;uHt
CallContext:[%hs] 
called_ == 1
cAMDt
Can broadcast 0 by 0 or 1. 
Can not digest separators: 
Can not digest tokenexp: 
Can not find the execution provider 
Can not find the node 
Can not get shape initializer data!
Can only add a new input at the end of the current ones.
Canadian_Aboriginal
candidate_output.Shape().Size() == output_shape.Size()
candidate_output_dims[iter] == 1
Cannot apply CumSum operator on a scalar
cannot compare iterators of different containers
Cannot concatenate scalars
cannot find allocator
cannot get value
Cannot parse the diagonal elements along dims 
Cannot replace concat node with initializer:
Cannot scale 0 by any factor to generate a non-zero value. 
Cannot slice scalars
Cannot split using values in 'split' attribute. Axis=
cannot use at() with 
cannot use erase() with 
cannot use key() for non-object iterators
Cannot use 'reflect' mode to pad dimension with a value of 0. Input shape:
Cannot use SearchOnePass for unanchored matches.
Cannot use user supplied initializer with name: (
Can't 
can't constant fold 
Can't find node with index 
Can't happen
Can't merge shape info. Both source and target dimension have values but they differ. Source=
Can't reduce on dim with value of 0 if 'keepdims' is false. Invalid output shape would be produced. input_shape:
Can't remove node 
Can't slice a non-tensor OrtValue. Type was 
Can't use func with null ptr
Carian
Carries out batch normalization as described in the paper
Case not handled in ComputeSimple: 
case_change_action
Cast Input from int64 to int32
Cast mask from int64 to int32
cast node to cast from float16 to float32 on cpu
cast_to
CastElimination
CastFloat16Transformer
CastMap
CategoryMapper
cats_int64s
cats_strings
Caucasian_Albanian
Caught exception while destructing CustomOpsLoader with message: 
Caught exception while loading custom ops with message: 
CbQ~I
CD$0H
CD$HH
CD$P3
CDist
ceil_mode
ceil_result
ceil_result_relu
ceil_result_relu_bool
ceil_result_relu_int
Cell clip threshold. Clipping bounds the elements of a tensor in the range of [-threshold, +threshold] and is applied to the input of activations. No clip if not specified.
CellMemInitTensor
center_point_box
center_point_box only support 0 or 1
Chakma
ChannelCount
channels
channels_ <= X_shape[1]
channels_ > 0
channels_last
char 
Char embedding size does not match char_embedding_size attribute.
Char embedding size does not match conv kernal size 2.
char_embedding_size
char16_t
char32_t
char8_t
CHECK failed: !is_closed_: 
CHECK failed: (backup_bytes_) == (0): 
CHECK failed: (buffer_used_) == (buffer_size_): 
CHECK failed: (count) <= (buffer_used_): 
CHECK failed: (count) >= (0): 
CHECK failed: (min_bytes) <= (std::numeric_limits<size_t>::max() - kBlockHeaderSize): 
CHECK failed: (new_size) <= ((std::numeric_limits<size_t>::max() - kRepHeaderSize) / sizeof(old_rep->elements[0])): 
CHECK failed: (scc->visit_status.load(std::memory_order_relaxed)) == (SCCInfoBase::kRunning): 
CHECK failed: backup_bytes_ == 0 && buffer_.get() != NULL: 
CheckNodesInPathK returns false
CheckNodesInPathQ returns false
CheckNodesInPathV return false
CheckSliceParameters return false
CheckSliceParameters return false for slice2
CheckSliceParameters returns false for last_slice
CheckSliceParameters returns false for mask_slice
CheckSliceParameters returns false for slice1
checksum
Cherokee
ChH+C`H
ChH+Q
ChHcCtL;
Child node if expression is false
Child node if expression is false.
Child node if expression is true
Child node if expression is true.
Chosen support vectors
CL$(3
CL$(H
CL$hE3
class 
Class labels if using integer labels.<br>One and only one of the 'classlabels_*' attributes must be defined.
Class labels if using string labels.<br>One and only one of the 'classlabels_*' attributes must be defined.
Class labels when using integer labels. One and only one 'classlabels' attribute must be defined.
Class labels when using string labels. One and only one 'classlabels' attribute must be defined.
class_ids
class_nodeids
class_treeids
class_weights
classes.size() == 2 || classes.size() == 1
classes_strings
classlabels_int64s
classlabels_ints
classlabels_strings
classlabels_strings_.empty() ^ classlabels_int64s_.empty()
classlabels_strings_.size() > 0 || classlabels_ints_.size() > 0
cli::array<
cli::pin_ptr<
clip_ > 0.f
Clipped_ZeroPoint_FP
ClipThreshold
close() failed: 
CloseHandle
CM/L+
CoalesceWalker::ShortVisit called
coclass 
CoCreateFreeThreadedMarshaler
code != static_cast<int>(common::OK)
Coefficient of ELU default to 1.0.
Coefficient of ELU.
Coefficient of leakage default to 0.01.
Coefficient of leakage.
Coefficient of SELU default to 1.0507.
Coefficient of SELU default to 1.05070102214813232421875 (i.e., float32 approximation of 1.0507009873554804934193349852946).
Coefficient of SELU default to 1.6732.
Coefficient of SELU default to 1.67326319217681884765625 (i.e., float32 approximation of 1.6732632423543772848170429916717).
coefficients
coefficients_.size() > 0
cointerface 
com.microsoft
com.microsoft.dml
com.microsoft.experimental
com.microsoft.mlfeaturizers
com.microsoft.nchwc
Common
CommonSubexpressionElimination
CompanyName
CompareStringEx
Compiled kernel hashes must be provided
compiled_kernel_hashes != nullptr
Compiler::Copy called!
complex128
complex64
ComplexMul
ComplexMulConj
Compress
Compute_
compute_info_->create_state_func(&context, &func_state_) == 0
Computed size: 
ComputePad: pad type not supported.
Computes the indices of the {name} elements of the input tensor's element along the
Concat
concat first input value is not -1
Concat of 
concat_after_gather does not have expected number of inputs or output edges
concat_after_gather input 2 does not have expected value
concat_result
ConcatFromSequence
Concretely, given the (fused) inputs X (TxNxD), the previous hidden
cond_out
condition
ConditionTensor
Config key is empty or longer than maximum length 128
Config value is longer than maximum length 1024
Conflicting free dimension overrides.
connection aborted
connection already in progress
connection refused
connection reset
const
const 
const_ignore_index
const_one
const_one_float
const_zero
const_zero_float
const_zero_target_typed
constant
Constant
Constant initializer NodeArg shape should not be null. NodeArg: 
constant_value
ConstantFill
ConstantFolding
ConstantOfShape
Constrain bias type to 32-bit integer tensor.
Constrain filter type to 8-bit integer tensor.
Constrain index tensor to int64
Constrain indice type to int32 or int64
Constrain indices to integer types
Constrain input a and its zero point data type to 8-bit integer tensor.
Constrain input A data type to 8-bit integer tensor.
Constrain input A data types as 16-bit integer tensor
Constrain input A, b_scale and output Y data type as float tensor.
Constrain input a_scale, b_scale and output Y data type as float tensor.
Constrain input and output  types to float tensors.
Constrain input and output float tensors types.
Constrain input and output integer tensors types
Constrain input and output to all tensor types.
Constrain input and output types (except mean and inv_std_var) to float tensors.
Constrain input and output types to 8 bit signed and unsigned tensors.
Constrain input and output types to 8 bit tensors.
Constrain input and output types to all numeric tensors and bool tensors.
Constrain input and output types to all numeric tensors.
Constrain input and output types to all numerical tensor types.
Constrain input and output types to all tensor types.
Constrain input and output types to all tensors.
Constrain input and output types to any tensor type.
Constrain input and output types to float and 8 bit tensors.
Constrain input and output types to float or half tensors.
Constrain input and output types to float tensors
Constrain input and output types to float tensors.
Constrain input and output types to float/int tensors.
Constrain input and output types to float32 tensors.
Constrain input and output types to floating-point tensors.
Constrain input and output types to high-precision and 8 bit numeric tensors.
Constrain input and output types to high-precision numeric tensors.
Constrain input and output types to int8 tensors.
Constrain input and output types to integer tensors.
Constrain input and output types to numeric tensors.
Constrain input and output types to signed numeric tensors.
Constrain input and output types to singed/unsigned int8 tensors.
Constrain input and output types.
Constrain input b and its zero point data type to 8-bit integer tensor.
Constrain input B data type to 8-bit integer tensor.
Constrain input B data types as 16-bit integer tensor
Constrain input 'ratio' types to float tensors.
Constrain input type to 8-bit integer tensor.
Constrain input type to unsigned or signed 32-bit integer tensor, or string tensor. It should be utf-8 encoded if using unicode.
Constrain input types to 8 bit signed and unsigned tensors.
Constrain input types to all tensor types.
Constrain input types to any tensor type.
Constrain input types to common numeric type tensors.
Constrain input types to float tensors.
Constrain input types.
Constrain input types. Casting from complex is not supported.
Constrain input types. Casting from strings and complex are not supported.
Constrain input types. Strings and complex are not supported.
Constrain input w and its zero point data type to 8-bit integer tensor.
Constrain input x and its zero point data type to 8-bit integer tensor.
Constrain input X and output types to float/int tensors.
Constrain input 'X' and output 'Y' to all tensor types.
Constrain input Y types to float/int tensors.
Constrain input, weight, and output types to floating-point tensors.
Constrain input0 and output types to float tensors
Constrain mask index to integer types
Constrain mean and inv_std_var to be float tensors.
Constrain mean and inv_std_var to float tensors.
Constrain output data type to 32-bit integer tensor.T2 must be tensor(uint32) when T1 is tensor(uint8),or must be tensor(int32) when T1 is tensor(int8).
Constrain output mask types to boolean tensors.
Constrain output 'mask' types to boolean tensors.
Constrain output to int64 tensor, which should be a scalar though.
Constrain output to int64 tensor.
Constrain output to integral tensor. It must be a scalar(tensor of empty shape).
Constrain output type to 8-bit integer tensor.
Constrain output type to unsigned and signed 32-bit integer tensor.
Constrain output types to 32 bit tensors.
Constrain output types to all tensor types.
Constrain output types to any tensor type.
Constrain output types to be numerics.
Constrain output types to bool, int32, int64, float16, float, double tensors.
Constrain output types to boolean tensors.
Constrain output types to float tensors.
Constrain output types to integral tensors.
Constrain output types. Casting to complex is not supported.
Constrain output types. Casting to strings and complex are not supported.
Constrain output types. Strings and complex are not supported.
Constrain output y and its zero point data type to 8-bit integer tensor.
Constrain output Y data type as 32-bit integer tensor.
Constrain output y data type to 32-bit integer tensor.
Constrain output Y data types as 32-bit integer tensor.T3 must be tensor(uint32) when both T1 and T2 are tensor(uint16),or must be tensor(int32) when either T1 or T2 is tensor(int16).
Constrain position to integral tensor. It must be a scalar(tensor of empty shape).
Constrain repeat's type to int64 tensors.
Constrain roi type to float or double.
Constrain seq_lens to integer tensor.
Constrain seq_lens to integral tensors.
Constrain split size to integral tensor.
Constrain target to integer types
Constrain tiles and axis's type to int64 tensors.
Constrain to all tensor types.
Constrain to any tensor type.
Constrain to any tensor type. If the dtype attribute is not provided this must be a valid output type.
Constrain to boolean tensors.
Constrain to integer types
Constrain to tensor(float).
Constrain to tensor(int32).
Constrain types to float tensors.
Constrain types to int tensors.
Constrain weights types to 8 bit tensors.
Constrain 'x' and 'x_zero_point' to 8-bit integer tensors.
Constrain 'x' to float or int32 tensor.
Constrain 'x' to float tensor.
Constrain 'x', 'y_scale' to float tensors.
Constrain 'x_zero_point' and 'x' to 8-bit/32-bit integer tensor.
Constrain 'y', 'x_scale' to float tensors.
Constrain 'y_zero_point' and 'y' to 8-bit integer tensor.
Constrain 'y_zero_point' and 'y' to 8-bit integer tensors.
Constrain 'y_zero_point' and 'y' to 8-bit unsigned integer tensor.
Constrains input and output to only numeric types.
Constrains input to boolean tensor.
Constrains input to float tensors.
Constrains input to integral tensors.
Constrains input to only numeric types.
Constrains input types to all numeric tensors.
Constrains input/output to boolean tensors.
Constrains output to boolean tensor.
Constrains to boolean tensors.
consumed_inputs
context does not contain text
Conv filter size does not match embedding_size attribute.
Conv kernal size 1 does not match conv_window_size attribute .
conv_B_tensor_proto
conv_W_tensor_proto
conv_window_size
ConvActivationFusion
ConvAddFusion
ConvAddFusion_Add_B_
ConvAddFusion_B_
ConvBNFusion
ConvBnFusion_BN_B_
ConvBnFusion_W_
Conversion Error
ConvInteger
ConvMulFusion
ConvMulFusion_Mul_B_
ConvMulFusion_W_
ConvTranspose
ConvTransposeWithDynamicPads
coordinate_transform_mode:[
coordinate_transformation_mode
Coptic
Copy from/to host memory
copy_info.size() == num_feeds
CoreMLExecutionProvider
corrupted protobuf data: tensor shape size(
CoTaskMemAlloc
Could not finalize session options while constructing the inference session. Error Message: 
Could not find a CPU kernel and hence 
Could not find an implementation for the node 
Could not find chunk in bin
Could not find OrtValue with idx '
Could not find OrtValue with name '
Could not find Region for 
Could not infer data type from input tensor with data type 
Could not parse model successfully while constructing the inference session
Could not write a profile because no model was loaded.
count == 1
count_include_pad
counts
Couple the input and forget gates if 1, default 0.
Couple the input and forget gates if 1.
CoupleInputForget
CPH+CHH
CPH+Q
CPI+CHL
CPUExecutionProvider
Create_State_
CreateDirectoryA
CreateDirectoryW
CreateDXGIFactory2
CreateEventW
CreateFeedsFetchesManager must be called prior to execution of graph.
CreateFile2
CreateMutexExW
CreateSemaphoreExW
Creating 
Creating and using per session threadpools since use_per_session_threads_ is true
Creating BFCArena for 
Crop and image to the specified spatial dimensions. If scale is given,
crop_size
crop_size shape input tensor has wrong dimension
CropAndResize
cross device link
CrossChannel
CT$ L
CT$@L
CT$0H
CT$0L
CT$H3
CT$HL
CT$PL
CT$pL
CTensor
cubic
'Cubic' mode only support 2-D inputs ('Bicubic') or 4-D inputs with the corresponding outermost 2 scale values being 1 in the 
cubic_coeff_a
CUDA execution provider is not enabled.
CUDAExecutionProvider
cudaMalloc
CudaPinned
CumSum
Cuneiform
cur + size <= end
cur_index == &*indices_data.cend()
cur_input == end_input || cur_input->first >= 0
cur_iteration_ < num_iterations_
cur_out == end_out
cur1 == end1
current
current <= buffer_size_
custom op registered at runtime
custom_logger != nullptr
CXD9kP
CXD9sPu
CXt8A
Cypriot
Cyrillic
D D R R z | 
D!t$8I
D!u`M
d".$,6
D$ A8{
D$ A8z
d$ D!
D$ E3
d$ E3
D$ E3
d$ E3
D$ E3
d$ E3
D$ E3
d$ E3
D$ E3
D$ E8}P
D$ fA
D$ fD
D$ H;
D$ H;|$8t
D$ H;C
D$ H;Q
D$ H;S
D$ H;U
D$ H;U't
D$ H+
D$ H9
D$ Hi
D$ HI;
D$ I;
D$ I;V
D$ I+
D$ I+D$
D$ I+GP3
D$ L;
D$ L+
D$ LcI
D$ tl
d$$H;
D$$H;
d$$I;^8tcL
D$$L;
D$(!|$ L
D$(9W ~*
D$(A;G(
D$(D9o(
D$(E3
D$(H!\$ L
D$(H;
D$(H;Q`t*H
D$(H9
D$(Hc
D$(HcH
D$(I;
D$(I+
D$(L#
D$(L;
D$(L+
D$(Lc
D$(M;
D$,H;
d$@D;d$x
d$@E3
D$@E3
d$@E3
D$@E3
d$@E3
D$@E3
d$@E3
D$@H;
D$@H;C
d$@H;T$h
D$@H;U
D$@H+D$8H
D$@H+GP3
D$@H9
D$@Hc
D$@I;
D$@I+
d$@I+4$H
d$@Ic
d$@L!e
D$@L;
D$@L;e
D$@L+
D$@L9d$8
D$@L9x s
D$@Lc
D$@M;
D$@M9Fp
d$@vbL
D$`+D$d
d$`A;_8
D$`D9h(t
D$`E3
d$`E3
D$`E3
d$`E3
D$`E3
d$`E3
D$`E3
D$`fD
D$`H;
D$`H;D$xt!H
D$`H+
d$`H+
D$`H+
D$`H+D$XH
D$`I;
D$`I+
d$`Ic
D$`L!t$hA
D$`L;
D$`L;D$X
D$`L;e
d$`L;m
D$`L9t$X
d$`Lc
D$`tI
D$<@B
d$<Ic
D$0 H
D$08P
D$09H }
D$09P
D$09P }
D$0D;
D$0D9H,t
D$0D9P0t
D$0E3
D$0fH
D$0H!|$8
D$0H;
D$0H;Cxt
D$0H;D$8H
D$0H;Q
d$0H;T$X
D$0H;U
D$0H;V
D$0H+
D$0H+D$(H
D$0H+GP3
D$0H9H s
D$0H9P }
D$0H9P s
D$0H9X }
D$0Hc
D$0HcH
D$0I;
D$0I;G
D$0Ic
d$0Ic
D$0Ic
D$0L;
d$0L;d$HE
D$0Lc
D$0M;
D$1A:
d$4@2
D$4@B
d$4D8l$<tDH
d$4D8l$2u
d$4E3
d$4fH
d$8E+
D$8E3
d$8E3
D$8E3
d$8H!|$@
D$8H;
d$8H;
D$8H;
d$8H;
D$8H;
d$8H;|$HuUL
D$8H+
D$8H9C0
D$8Hc
D$8HcR H
D$8I;
D$8I+
D$8I+D$0H
D$8IcJ
D$8L;
d$8L;
D$8L;|$`
d$8L+
D$8L+D$0I
d$8L9
D$8L9
D$8Lc
D$8M;
D$8M9J(
d$B&h
D$d+D$`
d$dD;d$lt^
D$DH;
D$hD8e
D$HD8e
D$HD8l$xt)H
D$HD8m
D$hD9c(
D$hE2
D$hE3
D$HE3
D$hE3
d$HE3
D$HE3
D$hE3
d$HE3
D$hfD
D$HfI
D$hH!D$xH
D$hH;
d$HH;
D$hH;
d$HH;
D$HH;
d$HH;
D$HH;
D$hH;
D$HH;
D$HH;]
D$hH;D$ptEH
D$hH;U
D$hH+
D$HH+D$@H
d$HH+u
D$HH9
D$hH9s 
D$HHc
D$hHc@XH9A
D$hHc@XI9F
D$hI;
d$HI;
D$HI;
d$hI+
d$HIc
D$hJ9
D$HL;
D$hL;
D$HL;
d$HL;
D$hL;
D$HL;
D$HL;xxr8L
D$HL;xxrGL
D$HL+
D$HL9gXt
D$HLc
D$HLi
D$hLi
D$hM;
D$HM;
D$HMi
D$L@B
D$LD;
D$P%H
D$P@2
D$p+E
D$PA;
D$pD8u
D$PE2
d$pE3
D$pE3
D$PE3
d$PE3
D$pE3
D$PE3
D$pE3
d$pE3
D$pE3
D$PE3
D$pE3
D$PE3
D$PE8p
d$PE9f8~ZA
D$peH
D$PfA
D$PH;
D$pH;
D$PH;
D$pH;
D$PH;
D$pH;
D$PH;
D$pH;
D$PH;
D$pH;
D$PH;
d$pH;
D$PH;\$`s
D$PH;D$`
D$pH;E
D$pH;l$`|
D$pH;U
D$PH;U
D$PH;U0t
D$PH;V(t
D$pH+
D$PH+
D$pH+
D$pH+D$hH
D$PH9
D$PH98uoA
D$pH9D$HwwD
D$pH9E
D$PHc
D$pHc
D$PHc
D$PHc_$L
D$pHcD$p
D$pHcH
D$PI;
D$PI;W
D$PI+
d$PIc
D$pIc
D$PL;
D$pL;
D$PL;Pxr/I
D$pL+
D$PL+
D$pL+
D$PL+
D$PL9P }
D$PLc
D$PLcG$H
D$PM;
D$pM;
D$pM;Q`s7I
d$pM+
D$PMc
D$RH9D$@u
D$TE3
D$tHc
D$tLc
d$V&f(d*V,f.T0V2T488
D$x;X
D$X@8u
D$xA;
D$XD8e
D$xD8m
d$XD8t$@
D$xD8u
D$XD8u
D$xD9
d$XE3
D$XE3
D$xE3
d$XE3
D$XE3
D$xE3
D$XfA
D$xfD
D$XH!D$hL
D$XH;
d$xH;
D$XH;
D$xH;
D$XH;
d$xH;
D$xH;
D$XH;
D$xH;
d$xH;
D$XH;
d$xH;
D$xH;]
D$xH;E
D$XH;P
D$xH+
D$XH+
D$xH+
D$xH+D$pH
D$XH+D$PH
D$XH9P }
D$XH9Q
D$xHcX
D$xI;
D$XI+
D$xI+
D$XI+
D$xI+
D$XI96
d$XIc
D$XL;
d$XL;
D$xL;
D$XL;
D$XL;u@t
D$xLc
D$xM;
D$XM;
D,$< 
D,0< 
D,F2DPH&J8HRJ"HlJ:LTNRHJPTRLH*D
d:@<2>0@pB
D;/uQMcD$$A
D;0|HI
D;A ulE
D;A$}
D;B(|
D;C8|
D;E(D
D;g |
D;I$}
D;l$0u^H
D;q |
D;t$@
D;t$H|
D;T$p}WIc
d@>B$
D+t$8A
D1X0I
D2\0$
D6HD8m
D8 t(H
D8(tLH
D8)u#L
D8,:u
D8@4u
D8`8t
D8`8u
D8{=H
D8{5H
D8{5t
D8{6tgLcC$M
D8|$ 
D8|$ t
D8|$`H
D8|$0
D8~8t
D8~8u
D8<*u
D8<8u
D88tS3
D8A t
D8A0t
D8A0tFD8A1t+
D8A1u!
D8ahtSL
D8APtRA
D8b8L
D8d$ t
D8d$@td
D8d$ht
D8d$ht)H
D8d$PtB
D8d$xt
D8d$xt)H
D8d$Xt)H
D8e8t%H
D8f8t"H
D8fDt
D8H }
D8ipu;L
D8KQtPL
D8l$ 
D8l$@
D8L$`t
D8L$0uP
D8l$X
D8l$Xt
D8l$Xt)H
D8m(t
D8nDt
D8oPt
D8oPu
D8p8t
D8P8u
D8p8u
D8Q(uRH
D8Q8u
D8Qqt;HcB
D8Qqt@
D8Qqt=
D8Qqt>
D8Qqt5HcB
D8Qqt7HcB
D8Qqt8HcB
D8R8H
D8R8L
D8r8t 
D8r8t$
D8RHu
D8RIt
D8S8H
D8sPt
D8t$@
D8t$@t$H
D8t$@t)H
D8t$@u,L
D8t$0
D8t$0t
D8t$Ht
D8t$ht)H
D8t$pt
D8X5u
D8yhu
D8Yqt(HcB
D8Yqt)HcB
D8Yqt*HcB
D8Yqt,HcB
D8ZHu
D9#u5H
D9#u6H
D9;v!
D9@$|
D9`(t
D9`(u
D9`(uSH
D9`,I
D9{(t
D9{0u<L
D9|$hv
D9}(u
D9}/t
D9}0u
D9}8t
D9}8u
D9}Xt(
D9}XuBH
D9~(u
D9~(uAH
D9>v'
D9>v%
D93u9H
D9a(u
D9a(uwH
D9B(u
D9c(t
D9C8~73
D9d$<
D9e v
D9e0t)
D9e0u@H
D9ehu
D9f(~
D9f(t
D9f@~
D9fp~
D9fX~
D9h u
D9H$|
D9h(t
D9h(u
D9hht
D9HHt
D9HHt1
D9I$|
D9i(u
D9j(u
D9j8t
D9k ~%L
D9k(t
D9kh~%L
D9kP~%L
D9l$H
D9mHt
D9mP@
D9mP~!H
D9n(t
D9n(u
D9o(t
D9o(u
D9p |
D9p ~_M
D9p u
D9p(t
D9p(u
D9P0t
D9Q |
D9q ~!H
D9Q$|
D9Q0t
D9R(u%I
D9s(t
D9S(t!H
D9s(u
D9sp~!H
D9sP~!H
D9t$p
D9u(t
D9v ~/H
D9VHu?L
D9vLv
D9vp~
D9w ~/H
D9x }
D9x(t
D9x(u
D9X0H
D9X0t
D9Y0t
Data of TensorProto ( tensor name: 
data overflow
data tensor must have rank >= 1
data type 
Data type for starts and ends inputs' is not supported in this build. Got 
data type is different from updates type
data type is not supported
Data type mismatch
Data type of the input tensor MUST be same as that of the input sequence. Sequence data type (
Data types of the inputs must match for MatMul
data_0
data_1.Shape() == shape
DATA_BATCH
data_n.Shape() == shape
data_rank * 2 == pads.size()
data_rank > 0
data_scale
data_transfer registered is nullptr.
data_type
data_zero_point
DBF(DDBM
DBXI;BX
DCPH;CP
DCR (default) for depth-column-row order re-arrangement. Use CRD for column-row-depth order.
DCxH;Cx
dD9c(
dD9o(
DeadState in RunStateOnByte
DebugBreak
DecodePointer
decomposed_QLinearSigmoid_DequantizeLinear_
decomposed_QLinearSigmoid_input_
decomposed_QLinearSigmoid_output_
decomposed_QLinearSigmoid_QuantizeLinear_
decomposed_QLinearSigmoid_Sigmoid_
default 1; Pooled output Y's height.
default 1; Pooled output Y's width.
Default logger already set. 
default_float
default_int64
default_logger_id must be provided if instance_type is InstanceType::Default
default_string
Defines behaviour if 'axes' is empty. Default behaviour with 'false' is to reduce all axes. When axes is empty and this attribute is set to true, input tensor will not be reduced,and the output tensor would be equivalent to input tensor.
Defines how to aggregate leaf values within a target. <br>One of 'AVERAGE,' 'SUM,' 'MIN,' 'MAX.'
DelayLoadFailureHook
DeleteCriticalSection
DeleteFile() failed - path: 
DeleteFileW
delta
delta in Range operator can not be zero!
delta in Range operator should be scalar like tensor, yet got shape:
delta_casted
Denote x_resized as the coordinate of axis x in the resized tensor, x_original as the coordinate of axis x in the original tensor, length_original as the length of the original tensor in axis x, length_resized as the length of the resized tensor in axis x, roi_x = (start_x, end_x) of the axis x in input "roi", scale = length_resized / length_original, <br/>
DENSE
depth
Depth is negative.
DepthToSpace
DepthToSpace op: only 'DCR' and 'CRD' modes are supported
DequantizeLinear
DequantizeLinear with type int32 should have no zero point or all zero points should be 0
deque<T> too long
Describes the axis of the inputs when coerced to 2D; defaults to one because the 0th axis most likely describes the batch_size
Describes the axis of the inputs when coerced to 2D; defaults to one because the 0th axis most likely describes the batch_size. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
Deseret
Deserialize tensor 
destination address required
detect_negative
detect_positive
Devanagari
device or resource busy
Device:[
DeviceType:
DEXI;EXt
DF H;F t
DFA out of memory: size 
DFXI;FXt
DFXI;FXu
DGXH;GXt
DGXH;GXt[M
DGXI;GXu
DictVectorizer
Did not find session options in the model file to be used while running the model
Dilation not supported for AutoPadType::SAME_UPPER or AutoPadType::SAME_LOWER.
Dilation value along each spatial axis of filter.
Dilation value along each spatial axis of filter. If not present, the dilation defaults to 1 along each spatial axis.
dilation value along each spatial axis of the filter.
dilation value along each spatial axis of the filter. If not present, the dilation defaults is 1 along each spatial axis.
dilation value along each spatial axis of the filter. If not present, the dilation defaults to 1 along each axis.
dilation value along each spatial axis of the filter. If not present, the dilation defaults to 1 along each spatial axis.
dilations
Dilations
Dilations dimensions should match kernel shape
dilations.size() == kernel_shape.size()
dim_iter == rank
dim_param value with no name. Invalid ORT format model.
dim_size > 0
dim0_offset < dim0_size
dimension <= num_dims
Dimension could not be inferred: incompatible shapes
dimension for each axis in the list of axes, it uses this information to
Dimension mismatch in unification between 
Dimension of input 
Dimension on which to do the sort.
Dimension on which to do the sort. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
Dimension value inferred (
dimension. If the value passed to start or end is larger than the `n` (the
Dimension: 
DimensionCount
dims.size() == extents.size() && dims.size() >= steps.size()
dims.size() == extents_.size()
dims.size() == starts.size()
dims.size() == starts.size() && dims.size() == extents_.size() && dims.size() >= steps.size()
dims.size() == steps.size()
dims.size()=
dims[d_i] < d_max
dimstart <= dimend && dimend <= size()
Direction
direction
Direction of moving bits. It can be either "RIGHT" (for right shift) or "LEFT" (for left shift).
directions
directions.size() == num_entries
DirectML.dll
directory not empty
discarded
Div and Shape does not have edge
Div and Shape1 does not have edge
div_inputs.size() == 2
div_result
Divide by zero
DML allocator
DML CPU
Dml::ExecutionProviderImpl::CopyTensors
Dml::GraphTransformer::ApplyImpl
DML_OPERATOR_ACTIVATION_CELU
DML_OPERATOR_ACTIVATION_ELU
DML_OPERATOR_ACTIVATION_HARD_SIGMOID
DML_OPERATOR_ACTIVATION_HARDMAX
DML_OPERATOR_ACTIVATION_IDENTITY
DML_OPERATOR_ACTIVATION_LEAKY_RELU
DML_OPERATOR_ACTIVATION_LINEAR
DML_OPERATOR_ACTIVATION_LOG_SOFTMAX
DML_OPERATOR_ACTIVATION_PARAMETERIZED_RELU
DML_OPERATOR_ACTIVATION_PARAMETRIC_SOFTPLUS
DML_OPERATOR_ACTIVATION_RELU
DML_OPERATOR_ACTIVATION_RELU_GRAD
DML_OPERATOR_ACTIVATION_SCALED_ELU
DML_OPERATOR_ACTIVATION_SCALED_TANH
DML_OPERATOR_ACTIVATION_SHRINK
DML_OPERATOR_ACTIVATION_SIGMOID
DML_OPERATOR_ACTIVATION_SOFTMAX
DML_OPERATOR_ACTIVATION_SOFTPLUS
DML_OPERATOR_ACTIVATION_SOFTSIGN
DML_OPERATOR_ACTIVATION_TANH
DML_OPERATOR_ACTIVATION_THRESHOLDED_RELU
DML_OPERATOR_ADAM_OPTIMIZER
DML_OPERATOR_ARGMAX
DML_OPERATOR_ARGMIN
DML_OPERATOR_AVERAGE_POOLING
DML_OPERATOR_AVERAGE_POOLING_GRAD
DML_OPERATOR_BATCH_NORMALIZATION
DML_OPERATOR_CAST
DML_OPERATOR_CONVOLUTION
DML_OPERATOR_CONVOLUTION_INTEGER
DML_OPERATOR_CUMULATIVE_SUMMATION
DML_OPERATOR_DEPTH_TO_SPACE
DML_OPERATOR_DEPTH_TO_SPACE1
DML_OPERATOR_DIAGONAL_MATRIX
DML_OPERATOR_DYNAMIC_QUANTIZE_LINEAR
DML_OPERATOR_ELEMENT_WISE_ABS
DML_OPERATOR_ELEMENT_WISE_ACOS
DML_OPERATOR_ELEMENT_WISE_ACOSH
DML_OPERATOR_ELEMENT_WISE_ADD
DML_OPERATOR_ELEMENT_WISE_ADD1
DML_OPERATOR_ELEMENT_WISE_ASIN
DML_OPERATOR_ELEMENT_WISE_ASINH
DML_OPERATOR_ELEMENT_WISE_ATAN
DML_OPERATOR_ELEMENT_WISE_ATANH
DML_OPERATOR_ELEMENT_WISE_BIT_AND
DML_OPERATOR_ELEMENT_WISE_BIT_COUNT
DML_OPERATOR_ELEMENT_WISE_BIT_NOT
DML_OPERATOR_ELEMENT_WISE_BIT_OR
DML_OPERATOR_ELEMENT_WISE_BIT_SHIFT_LEFT
DML_OPERATOR_ELEMENT_WISE_BIT_SHIFT_RIGHT
DML_OPERATOR_ELEMENT_WISE_BIT_XOR
DML_OPERATOR_ELEMENT_WISE_CEIL
DML_OPERATOR_ELEMENT_WISE_CLIP
DML_OPERATOR_ELEMENT_WISE_CONSTANT_POW
DML_OPERATOR_ELEMENT_WISE_COS
DML_OPERATOR_ELEMENT_WISE_COSH
DML_OPERATOR_ELEMENT_WISE_DEQUANTIZE_LINEAR
DML_OPERATOR_ELEMENT_WISE_DIVIDE
DML_OPERATOR_ELEMENT_WISE_ERF
DML_OPERATOR_ELEMENT_WISE_EXP
DML_OPERATOR_ELEMENT_WISE_FLOOR
DML_OPERATOR_ELEMENT_WISE_IDENTITY
DML_OPERATOR_ELEMENT_WISE_IF
DML_OPERATOR_ELEMENT_WISE_IS_INFINITY
DML_OPERATOR_ELEMENT_WISE_IS_NAN
DML_OPERATOR_ELEMENT_WISE_LOG
DML_OPERATOR_ELEMENT_WISE_LOGICAL_AND
DML_OPERATOR_ELEMENT_WISE_LOGICAL_EQUALS
DML_OPERATOR_ELEMENT_WISE_LOGICAL_GREATER_THAN
DML_OPERATOR_ELEMENT_WISE_LOGICAL_GREATER_THAN_OR_EQUAL
DML_OPERATOR_ELEMENT_WISE_LOGICAL_LESS_THAN
DML_OPERATOR_ELEMENT_WISE_LOGICAL_LESS_THAN_OR_EQUAL
DML_OPERATOR_ELEMENT_WISE_LOGICAL_NOT
DML_OPERATOR_ELEMENT_WISE_LOGICAL_OR
DML_OPERATOR_ELEMENT_WISE_LOGICAL_XOR
DML_OPERATOR_ELEMENT_WISE_MAX
DML_OPERATOR_ELEMENT_WISE_MEAN
DML_OPERATOR_ELEMENT_WISE_MIN
DML_OPERATOR_ELEMENT_WISE_MODULUS_FLOOR
DML_OPERATOR_ELEMENT_WISE_MODULUS_TRUNCATE
DML_OPERATOR_ELEMENT_WISE_MULTIPLY
DML_OPERATOR_ELEMENT_WISE_POW
DML_OPERATOR_ELEMENT_WISE_QUANTIZE_LINEAR
DML_OPERATOR_ELEMENT_WISE_QUANTIZED_LINEAR_ADD
DML_OPERATOR_ELEMENT_WISE_RECIP
DML_OPERATOR_ELEMENT_WISE_ROUND
DML_OPERATOR_ELEMENT_WISE_SIGN
DML_OPERATOR_ELEMENT_WISE_SIN
DML_OPERATOR_ELEMENT_WISE_SINH
DML_OPERATOR_ELEMENT_WISE_SQRT
DML_OPERATOR_ELEMENT_WISE_SUBTRACT
DML_OPERATOR_ELEMENT_WISE_TAN
DML_OPERATOR_ELEMENT_WISE_TANH
DML_OPERATOR_ELEMENT_WISE_THRESHOLD
DML_OPERATOR_FILL_VALUE_CONSTANT
DML_OPERATOR_FILL_VALUE_SEQUENCE
DML_OPERATOR_GATHER
DML_OPERATOR_GATHER_ELEMENTS
DML_OPERATOR_GATHER_ND
DML_OPERATOR_GATHER_ND1
DML_OPERATOR_GEMM
DML_OPERATOR_GRU
DML_OPERATOR_JOIN
DML_OPERATOR_LOCAL_RESPONSE_NORMALIZATION
DML_OPERATOR_LP_NORMALIZATION
DML_OPERATOR_LP_POOLING
DML_OPERATOR_LSTM
DML_OPERATOR_MATRIX_MULTIPLY_INTEGER
DML_OPERATOR_MAX_POOLING
DML_OPERATOR_MAX_POOLING_GRAD
DML_OPERATOR_MAX_POOLING1
DML_OPERATOR_MAX_POOLING2
DML_OPERATOR_MAX_UNPOOLING
DML_OPERATOR_MEAN_VARIANCE_NORMALIZATION
DML_OPERATOR_MEAN_VARIANCE_NORMALIZATION1
DML_OPERATOR_NONZERO_COORDINATES
DML_OPERATOR_ONE_HOT
DML_OPERATOR_PADDING
DML_OPERATOR_QUANTIZED_LINEAR_CONVOLUTION
DML_OPERATOR_QUANTIZED_LINEAR_MATRIX_MULTIPLY
DML_OPERATOR_RANDOM_GENERATOR
DML_OPERATOR_REDUCE
DML_OPERATOR_RESAMPLE
DML_OPERATOR_RESAMPLE_GRAD
DML_OPERATOR_RESAMPLE1
DML_OPERATOR_REVERSE_SUBSEQUENCES
DML_OPERATOR_RNN
DML_OPERATOR_ROI_ALIGN
DML_OPERATOR_ROI_POOLING
DML_OPERATOR_SCATTER
DML_OPERATOR_SCATTER_ND
DML_OPERATOR_SLICE
DML_OPERATOR_SLICE_GRAD
DML_OPERATOR_SLICE1
DML_OPERATOR_SPACE_TO_DEPTH
DML_OPERATOR_SPACE_TO_DEPTH1
DML_OPERATOR_SPLIT
DML_OPERATOR_TILE
DML_OPERATOR_TOP_K
DML_OPERATOR_TOP_K1
DML_OPERATOR_UPSAMPLE_2D
DML_OPERATOR_VALUE_SCALE_2D
DMLCreateDevice1
DmlExecutionProvider
DmlFusedNode_
DmlFusedNodeDomain
dNf8d he
DNF8D HPD
DnnlExecutionProvider
DoCoalesce failed: r1->op() is 
DoCoalesce failed: r2->op() is 
does not have the graph for key 
Dogra
Domain already set in registry
domainToVersionMap
Done saving initialized tensors
Done saving OrtValue mappings.
double
double_data
DRHI9RHu!H
drop_states
Dropout
dst_implicit_input_idx < (int)node->ImplicitInputDefs().size()
Dt$HD;
dtype
dtype_ != nullptr
dtype_attribute->second.has_i()
dup_replacements.find(&arg) == dup_replacements.end()
Duplicate constant node sparse initializer name: '
Duplicate initializer (dense or ConstantNode): '
Duplicate initializer (dense, sparse or ConstantNode): '
Duplicate ngram detected, size: 
Duplicate of FusedMatMul. Going forward FusedMatMul should be used. This OP will be supported for backward compatibility.
Duplicate sparse_tensor_initializer: '
Duplicate stopwords not allowed
Duplicate type constraint name
duplicated allocator
duplicated allocator: 
duplicated location
duplicated ort_value index:
Duployan
dXflhdjBl`hLp,r
dXfnhdjFl`hNp,r
dXfphnjBljhLp,r
dxgi.dll
DynamicQuantizeLinear
DynamicQuantizeLSTM
DynamicQuantizeLSTM : 
DynamicQuantizeMatMul
DynamicQuantizeMatMul : input B scale must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
DynamicQuantizeMatMul : input B zero point must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
DynamicQuantizeMatMulFusion
DynamicSlice
DZ(\(^(
DzFbHjJ
DZFVDi
DZFxD
E D+E
E I+E
E(D8eXt
E(E+E0E
E(H+E H
E/H9E
E/L+E'I
E/uTH
E;}TH
E;~ |
E;A(|
E;a(u
E;g\|
E;H$}
E;J$|
E;P |
E;Q$}
E;u A
E;V(|+H
E;Y0t
E@H+E8H
E@L+E8I
E_Xsquared
E`!D$P
E`I9EXtgI+EXH
E`I9V
E`L+EXI
E|$`H
e0A^A]A\_^[]
e0fD;
E0HcH
E0Lc`
E8` u
E8~%A
E8~it
E8~it5I
E8<0u
E8eXt
E8fHu
E8J(u-M
E8L+E0I
E8L9)~XH
E8L9)~ZH
E8M9)~ZH
E8t$Pt
E8w8I
E8w8L
E8x u
E9~(t
E9<$t
E9<$u
E9A(~
E9A(~"3
E9e(t
E9fTtlH
E9fTtoH
E9fTu
E9g(tDH
E9g\~;H
E9l$(
E9n(t
E9n(u
E9P(u
E9s u6L
Each element of the sequence should be either tensor or map.
Ef(E9l$(t9I
EF0E3
Egyptian_Hieroglyphs
Eh!D$@
Eh!D$`
EH@8t$Xt
E'H9E
EhH;Ept
EhH;M
EHH+E@H
EhH9E
EHIcT
Einsum
Einsum expression string.
Einsum op: An implementation for the input type 
Einsum op: Could not copy the intermediate output's buffer into the op's output buffer. Error: 
Einsum op: Exception during MatMul operation: 
Einsum op: Input dimensions must be equal along an axis to be reduced across all inputs
Einsum op: Input shapes do not align
Einsum op: The candidate output cannot be reshaped into the op's output
Einsum op: The candidate output does not match the actual output's shape
Einsum op: There must be atleast one input
Einsum op: Transpose failed: 
Einsum op: Unsupported data type for Diagonal 
Einsum operands could not be broadcast together. Please check input shapes/equation provided.Input shape of operand 
Einsum subscripts does not contain enough subscript labels and there is no ellipsis for input 
Einsum subscripts string contains too many subscript labels when compared to the rank of the input
Einsum subscripts string contains too many subscript labels when compared to the rank of the input 
Either both scale and offset can be of feature size (
Either one of the separators OR tokenexp attributes required but none is set
Either scales or sizes MUST be provided as input.
Either the key tensor or the value tensor has NumDimensions > 1
Elbasan
elem_proto != nullptr
elem_type
elem_type_ != nullptr
element index is out of bounds
Element type of input 
Element type of input was unknown
Element type of inputs are expected to be the same.
elements, but feeds has 
EliminateDropout
EliminateIdentity
EliminateSlice
Ellipsis must indicate a fixed number of dimensions across all inputs
Ellipsis represents incompatible dimensions.
else_branch
Elu_Result
embedding_size
EmbedLayerNormalization
EmbedLayerNormFusion
Empty dimensions for input tensor
Empty graph proto from deserialization of ORT format model
Empty input dimensions.
Empty scale in attributes
Empty stopwords not allowed
Empty value of imputed values.
Enable broadcasting
enable_profiling
enable_profiling option in the model file must be an integer
enabled_
EncodePointer
Encountered nullptr.
Encountered unknown exception in Initialize()
Encountered unknown exception in Load()
Encountered unknown exception in Run()
end >= starts_.back()
end of a dimension with unknown size, it is recommended to pass in `INT_MAX`.
end of input
end_idx >= start_idx && end_idx <= total_items
Ending indices (exclusive) of corresponding axis in axes`
EndPadding
endpos: 
Ends must be a 1-D array
Engine failed to create a model!
ENGINE_ERROR
EnterCriticalSection
entiu
entry != initialized_tensors_to_allocate.end()
entry != kernel_create_info_map.cend()
entry != kernel_create_info_map_.cend()
entry != node_to_subgraph_ss.second.cend()
entry != nullptr
Entry exists in node 
entry.program_counter.HasValidEntries()
enum 
en-US
Env is null
env_ptr == p_instance_
Environment dependent string that denotes the locale according to which output strings needs to be upper/lowercased.Default en_US or platform specific equivalent as decided by the implementation.
Eo9F@
EP!D$0
EP_FAIL
EPH;}
EPH;E`
EpH;Ext
EPH9D$pt!3
EpI9V
Epsilon
epsilon
epsilon_ >= 0
Equal
equal const not matched.
equation
Er(D9n(t
ERROR
Error compiling '
Error during EndProfiling(): 
Error mapping feeds: 
Error mapping output names: 
Error merging shape info for output. '
Error parsing '
Error reverse compiling '
Error: Duplicate definition-site for (
errorCategory
errorCode
errorMessage
Ethiopic
euclidean
Ev(E9n(t-H
EvaluationStart
EvaluationStop
EventRegister
EventSetInformation
EventUnregister
EventWriteTransfer
EX_squared
exA_A^_^[]
Example 1:
Example 2:
Example 3:
Example 4:
Exceeded max transformer level. Current level is set to 
Exception
Exception caught: 
Exception during initialization: 
Exception during loading: 
Exception running nodes starting at 
exclude_outside
exclude_outside can be set to 1 only when mode is CUBIC. Current mode is set to 
exclusive
exec_plan_index
exec_plan_ptr
executable format error
Execution frame was null
Execution providers must be registered before the session is initialized.
Execution providers must be registered before the session is initialized. 
Execution type '
execution_mode
execution_mode is not valid
execution_mode option in the model file must be an integer
ExecutionProviderEvent
executionProviderIds
ExH;V
EXH+EPH
ExHc@
EXHcUPI
Existing entry in compiled kernel hashes for 
existing_entries.find(attribute_name) == existing_entries.cend()
existing->second == &tensor
Exiting due to terminate flag being set to true.
Expand
ExpandBroadcastLooper should only have a shape for the second input.
ExpandDims
ExpandDims echo operator.
expanded
expanded_target
expanded_target_int64
ExpandElimination
Expect mask data type is uint8 or float
Expect to have present state output when past state input is given
Expected a single float value!
Expected a single int64 value!
Expected AllocateFinalOutput to have been called to before we increment the iterator
Expected AllocateFinalOutput to have been called to before we read the OrtValue from the iterator.
Expected 'replace_value_int64' attribute since 'imputed_values_int64' is specified
Expected 'replaced_value_float' attribute since 'imputed_value_floats' is specified
Expected value:
Expecting a non-empty tokenexp
Expecting activation to be one of Affine, Relu, LeakyRelu, ThresholdedRelu, Tanh, ScaledTanh, Sigmoid, HardSigmoid, Elu, Softsign, Softplus. Got 
Expecting all elements to be tensors. Got: 
Expecting indices to be either int32_t or int64_t
Exponent
ExponentTensor
Extended allocation by 
Extending BFCArena for 
extents.size()=
extern "C" 
External data type cannot be UNDEFINED or STRING.
External data type must not be UNDEFINED or STRING.
extra_shape
extrapolation_value
EyeLike
EyeLike : Input tensor dimension is not 2
F +F0A+
F D+E
f D+f0D
F F ~ ~ 
F H;C 
F H;F(t
F H+F
F HcF
F HcL
F I+F
F L+F
f#L$@f
F(+F0
F(A9F
F(D+F0D
F(E+F0E
f(E+f0E
F(E+F0E
f(fE9o
F(H;F0t<9
F(HcF A
F(LcN(A
F,A9F
f;)sB
f;D$8u\H
F@f9^Dr
F@fD9nDr
F@H;FHt
F@H+F8H
F@I+F8H
F@I9F8u
F\HFJ(H
F`H9FX
F`L9~p~
F<H"JXFDL
f0h$j
F0H9F(t5H
f0jtJvJxDzP|`t,
f0M+f(I;
F8LcF H
f9,Yu
f9<Au
f9<su
f9KNu'
fA9,@u
fA9|U
fA99}
Factor used in computing the running mean and variance.e.g., running_mean = running_mean * momentum + mean * (1 - momentum), default is 0.9f.
Factor used in computing the running mean and variance.e.g., running_mean = running_mean * momentum + mean * (1 - momentum).
Faild to find path to qkv_matmul
Faild to find path v
Faild to find path v to Split
Faild to match concat node for Gather paths
Faild to match gemm gather path
Faild to match gemm path
Faild to match path 1 for unidirectional mask
Faild to match path 2 for unidirectional mask
Faild to match path 3 for unidirectional mask
Faild to match path 4 for unidirectional mask
Faild to match the path (Div-->Where-->Add) for unidirectional mask
Failed in match input mask subgraph
Failed in match Transpose attribute perm. Expected: 0, 2, 1, 3
Failed in match v_matmul and v_add input shape
Failed in match v_transpose attribute perm. Expected: 0, 2, 1, 3
Failed since multiple edges matched:
Failed to add kernel for 
Failed to allocate memory for requested buffer of size 
Failed to analyze start state.
Failed to construct locale with name:
Failed to convert dense initializer to sparse
Failed to convert mask to int32
Failed to copy tensor to 
Failed to create output tensor for 
Failed to create output tensor for If output 
Failed to create output tensor for output #
Failed to create the inter-op thread pool for the parallel executor, setting ExecutionMode to SEQUENTIAL
Failed to find a free memory block despite calling Extend. rounded_bytes=
Failed to find allocator for device 
Failed to find input name in the mapping: 
Failed to find kernel for 
Failed to find mask path
Failed to find path 1 of position shape.
Failed to find path 2 of position shape.
Failed to find path for k
Failed to find path for mask
Failed to find path for past_k
Failed to find path for present_k
Failed to find path for present_v and past_v
Failed to find path for q
Failed to find reshape shape path 1
Failed to find reshape shape path 2
Failed to find shape path
Failed to find Softmax node
Failed to find symbol in library, error code: 
Failed to get allocator for initializer '
Failed to get allocator for location: 
Failed to get allocator for optimizer
failed to get first output!
Failed to get initializer tensor.
Failed to get position embedding weights.
Failed to load library, error code: 
Failed to load model because protobuf parsing failed.
Failed to load model with error: 
Failed to load Q, K and V bias tensors, or data type is not float or float16.
Failed to load Q, K and V weights, or data type is not float or float16.
Failed to match position embedding subgraph.
Failed to match position subgraph.
Failed to match Shape node. 
Failed to match v_concat
Failed to obtain detect_negative
Failed to obtain detect_positive
Failed to parse model file!
Failed to parse model stream!
Failed to parse path root: 
Failed to serialize model!
Failed to unload DSO: 
Failed to unload library, error code: 
Failed to write value with snprintf().
FailFast
false
false literal
fast_gelu_output
FastGelu
FastGeluFusion
FATAL
Fatal error: 
Fatal error: 0 count processors from GetLogicalProcessorInformation
Fatal error: 0 count processors from GetSystemInfo
fB9,Ju
fB94Bu
fB94zu
fbs_attr cannot be null
fbs_node_arg_names cannot be null
fD;!s
fD;"s
fD;)s
fD;1s
fD;9s
fD9$Ou
fD9$Pu
fD9&u
fD9(t
fD94Ou
fD9P6u
fD9ZNuG
fE;8s
Feature id for each node.
FeatureVectorizer
feed_locations.size() == copy_info.size()
feeds.size() == feed_mlvalue_idxs.size()
feeds_fetches_manager_
feeds_fetches_manager_ && info_
fetch_alloc_info.size() == copy_info.size()
Fetches vector passed to GetOutputs contains 
fetches.empty() || fetches.size() == fetch_mlvalue_idxs_.size()
fetches.size() == node->OutputDefs().size()
fF9$Bu
fF94@u
fF94Cu
fffff
ffffff
fffffff
fFh(j|fdhFj(h
fG9,Du
FHH\J~H)
FHH+F@H
fHIcF@M
Field '
file exists
File not found!
file too large
file_path == nullptr
FileDescription
filename too long
FileVersion
filter number not equal to input channel number.
filter_info_ == nullptr
FilterScaleTensor
FilterTensor
FilterZeroPointTensor
final_output_mlvalue_
final_state_and_scan_outputs
FindClose
FindFirstFileW
FindNextFileW
First dimension (num_rois) of batch_indices and rois don't match
First input does not have rank 2
first input tensor has wrong dimension
First input tensor must have rank 3
First set of probability coefficients.
First, offset by this.<br>Can be length of features in an [N,F] tensor or length 1, in which case it applies to all features, regardless of dimension count.
Flag indicating whether the regression is a one-class SVM or not.
Flatten
float
FLOAT
float_data
float16
FLOAT16 is not supported
FLOATFLOATSGRAPHGRAPHSINTINTSSPARSE_TENSORSPARSE_TENSORSSTRINGSTRINGSTENSORTENSORSUNDEFINED
FLOATS
floor
Floor
FlsAlloc
FlsFree
FlsGetValue
FlsSetValue
Flush-to-zero and denormal-as-zero are 
fmod attribute must be true for float, float16 and double types
fmod must have value either 0 or 1
fmod_
FNH2F JQ
FnHBF
For each node, define what to do in the presence of a missing value: if a value is missing (NaN), use the 'true' or 'false' branch based on the value in this array.<br>This attribute may be left undefined, and the defalt value is false (0) for all nodes.
For each node, define what to do in the presence of a NaN: use the 'true' (if the attribute value is 1) or 'false' (if the attribute value is 0) branch based on the value in this array.<br>This attribute may be left undefined and the defalt value is false (0) for all nodes.
For example, the following tensor shapes are supported (with broadcast=1):
For internal use.
For map type num_values MUST be 2
For ort_value with index: 
For previous (depreciated) non-spatial cases, implementors are suggested
forgot to update the version range in DomainToVersionRange 
FormatMessageA
FormatMessageW
forward
Found '.' not part of an ellipsis in input: 
Found '.' not part of an ellipsis in the output subscript provided
Found a '.' not part of an ellipsis in input: 
Found a '.' not part of an ellipsis in the output subscript provided
found duplicated provider 
Found kernel for Op with name (
Found session/run/environment configuration in the model file to be used while running the model
Four modes: round_prefer_floor (default, as known as round half down), round_prefer_ceil (as known as round half up), floor, ceil. Only used by nearest interpolation. It indicates how to get "nearest" pixel in input tensor from x_original, so this attribute is valid only if "mode" is "nearest".
foward
FPE8fxI
FpH`JdLVN`PlT*V8X
FPH+FHH
FPL9@`
fPM;fXt5
frame != nullptr
FreeDimensionOverrideTransformer
FreeLibrary
func info for node: 
function
Function
function not supported
Fused
fused 
fused Add and Gelu
Fused an attention node for GPT.
Fused an attention node.
Fused Attention subgraphs 
fused Conv 
fused EmbedLayerNorm subgraphs 
fused Gelu subgraphs 
fused Gemm 
fused GPT2Gelu subgraphs 
fused LayerNorm subgraphs 
fused Matmul and Add 
Fused MatMul and Scale
fused op (
Fused reshape node: 
fused SkipLayerNorm subgraphs 
fused_
fused_activation
fused_activation_domain
fused_activation_since_version
fused_alpha
fused_beta
fused_function_subgraph
fused_gamma
fused_ratio
FusedActivation
FusedAdd
FusedBatchNormalization
FusedConv
FusedConvTranspose
FusedGemm
FusedInstanceNormalization
FusedMatMul
FusedMeanVarianceNormalization
FusedSum
FuseReluClip
FuseReluClip_
fusion_style == IExecutionProvider::FusionStyle::Function
fVhNjRlbnBp`rVh2f8
FXH+FPH
FxH+FpH
FXH+FPH
fXhnjnlXnNr
FXI;FXu
FXI9FPu
G D9x u
G H+G
G L+G
G L9 
G M+G
G(9G,t_
G(D+G0D
G(D9c(t
G(E+G0E
G(H;H0
G(H+G H
g(L9}
G@H;u
G0L9 
G0LcG
G8H+G0H
G8LcG H
gamma
Gamma
gamma is expected to have 1 dimension, got 
gamma is expected to have 1 dimensions, got 
gamma is expected to have size of 
Gamma should be of shape (hidden_size). 
gates
Gather
gather axis value not expected
gather indices not matched.
gather input 1 value is not expected
Gather node in path 2 is not linked to another subgraph.
Gather Tind type not supported in this build.
GatherElements
GatherElements op: Cannot operate on scalar input
GatherElements op: Data type of input 'data' should match the data type of the output
GatherElements op: 'indices' shape should have values within bounds of 'data' shape. Invalid value in indices shape is: 
GatherElements op: Rank of input 'data' needs to be equal to rank of input 'indices'
GatherElements op: Value in indices must be within bounds [
GatherND
GatherNDBase PrepareForCompute: Input count mismatch
Gaussian Error Linear Unit.
Gelu approximation
GeluApproximation
GeluFusion
Gemm bias is not constant
Gemm bias is not constant initializer
Gemm bias shape is not expected
Gemm bias shape not expected
Gemm does not have 3 inputs
Gemm weight is not constant initializer
Gemm weight shape is not expected
GEMM: Dimension mismatch, W: 
Gemm: Invalid bias shape for broadcast
GemmActivationFusion
GENERAL ERROR
generic
generic-type-
GenuD
GenuH
Georgian
Get preallocated buffer for initializer '
GetCPInfo
GetCurrentProcess
GetCurrentProcessId
GetCurrentThread
GetCurrentThreadId
GetEnvironmentVariableA
GetFileAttributesA
GetFileAttributesW
GetFileSizeEx
GetFileSizeEx 
GetFinalPathNameByHandle() failed: 
GetFinalPathNameByHandleW
GetFullPathNameA
GetFusedActivationAttr(info, activation_).IsOK()
GetLastError
GetLocaleInfoEx
GetLogicalProcessorInformation
GetModuleFileNameA
GetModuleHandleExW
GetModuleHandleW
GetNativeSystemInfo
GetProcAddress
GetProcessHeap
GetProvider
GetRestrictedErrorInfo
GetStringTypeW
GetSystemInfo
GetSystemTimeAsFileTime
GetSystemTimePreciseAsFileTime
gfffffffH
gfffffffH+
gfffffffH+9H
gfffffffL+
GHH+G@H
Given `data` tensor of rank r >= 1, and `indices` tensor of rank q >= 1, gather
Given model could not be parsed while creating inference session. Error message: 
GivenTensorFill
Glagolitic
gLcr@I
global
global_bias
global_weight
GlobalAveragePool
GlobalLpPool
GlobalMaxPool
Got invalid dimensions for input: 
Got nullptr for input tensor.
Got nullptr for sequence input.
Got nullptr from GetKernel for node: 
Got nullptr input for index tensor
Got weights of size: 
Gothic
Gp9Fpt
GPH+GHH
GPH9GHt@H
GpL;E
GPT2Gelu
GradientTensor
-grams
Grantha
GRAPH
Graph
graph
Graph attribute inferencing failed: 
Graph attribute inferencing returned type information for 
Graph attribute value was null. Invalid ORT format model.
Graph ctor should have created NodeArg for initializer. Missing:
Graph has 
Graph in 'body' attribute of Loop should have 
Graph is null. Invalid ORT format model.
Graph must be in single static assignment (SSA) form, however '
Graph state to be loaded into must be empty.
Graph to run if condition is false. Has N outputs: values you wish to be live-out to the enclosing scope. The number of outputs must match the number of outputs in the then_branch.
Graph to run if condition is true. Has N outputs: values you wish to be live-out to the enclosing scope. The number of outputs must match the number of outputs in the else_branch.
Graph transformers must be registered before the session is initialized.
graph_->GetNode(idx) != nullptr
graph_index
graph_inputs_excluding_initializers_.empty() && graph_inputs_including_initializers_.empty() && value_info_.empty() && graph_outputs_.empty()
graph_optimization_level
graph_optimization_level <= TransformerLevel::MaxLevel
graph_optimization_level is not valid
graph_optimization_level option in the model file must be an integer
graph_proto != nullptr
graph_proto cannot be null
graph_proto_ is not in sync with name_to_initial_tensor_.
GraphProto attribute inferencing is not enabled in this InferenceContextImpl instance.
GRAPHS
Greater
GreaterOrEqual
Greek
group
group count is <= 0
GroupCount
GrowStack() failed: 
GRU operator does not support double yet
GRUUnit
GRUUnit computes the activations of a standard GRU,
gsl::narrow_cast<int64_t>(input_axes_.size()) == num_scan_inputs_
gsl::narrow_cast<int64_t>(input_shape.Size()) == size
gsl::narrow_cast<int64_t>(output_axes_.size()) == num_scan_outputs
gsl::narrow_cast<int64_t>(tensor_shape.NumDimensions()) >= slice_dimension
gsl::narrow_cast<int64_t>(X_shape.NumDimensions()) >= axis
Gujarati
Gunjala_Gondi
Gurmukhi
GXD8o
GXD9gPuVE+
GXD9oP
GXH+GPH
GXI;GP
h != kInvalidChunkHandle
h |"X$N&0*
h < chunks_.size()
H 9J 
H 9J A
H A9J uZI
H D9a(
H H+H
H H99
H Hc@
H I;A
H L9!
H L9)
H L91t
h M9e
H SUVWAVAWH
H SVWH
H SWH
H UATAUAVAWH
h UAVAWH
h VAVAWH
h VWATAVAWH
h VWAUAVAWH
h VWAVH
H WATAUAVAWH
H!\$0H!]
H!]@H
H!]8H!]@
H!|$(L
H!|$@H
H!|$@L
H!|$`H
H!|$pH
H!|$PH
H!C H
H!D$ L
H!D$@D
H!D$03
H!D$0I
H!D$83
H!D$HL
H!D$PL
H!D$XH
H!E I
H!E/E3
H!E0H
H!E7H
H!E'9
H!E'M
H!L$@E3
H!L$0M
H!L$8H
H!L$H3
H!L$HD
H!t$@H
H!t$@L
H!T$0D
H!t$0L
H!t$pH
H!T$PH
h"jzlPj
H#D$HH
H#MPH
H#N0H
H#NHH
H(A;H
H(A;I
H)_`H
H)L$hL
H,A;I
H;\$ t*
H;\$ t7
H;\$(u
H;\$@
H;\$@t`M
H;\$`H
H;\$0rPH;]
H;\$H
H;\$P
H;\$Xw
H;]'|
H;]8u
H;]h|
H;^8u
H;_8u
H;{ r
H;{ u
H;{(|
H;{(}UI
H;{h|
H;{Hv
H;|$(t3H
H;|$`
H;|$0A
H;|$PL
H;} |
H;~0u
H;>r&D8~8u
H;3r5H
H;7r/H
H;A(s
H;A`t
H;A8}
H;A8u
H;AHu
H;APu
H;APuD
H;BPuSD
H;C r
H;C rIH
H;C`t
H;C8}
H;CPt
H;Cpu
H;Cxt
H;Cxu
H;D$@
H;D$`
H;D$0u
H;D$Hu
H;E |
H;E`uFH
H;E`uGL
H;E'|
H;Eh|
H;EPuCL
H;EPuGL
H;F t
H;F0t(H
H;F0tAL
H;F0u
H;Fxu
H;G@|
H;G0r<
H;G0v
H;G8u
H;GXt
H;GXu
H;H s
H;H(t$I;Q
H;H(u
H;H@s
H;HHsYH
H;K r
H;L$(H
H;L$@|
H;L$h
H;L$p
H;L$pt\H
H;NHr
H;P(t!L;
H;P(t#L;
H;P(t#M;
H;P(u
H;P0u
H;P8t~H
H;q s
H;Q tfL9
H;q(sfA
H;s r
H;s s
H;s(u
H;S8u
H;s8u
H;S8v
H;t$@
H;T$`
H;t$`u
H;t$8
H;T$8|
H;T$8r
H;t$h
H;T$H}
H;T$h}rE3
H;T$p
H;t$P
H;T$P|
H;T$x}uE3
H;T$Xr
H;t$xr
H;U t
H;U@t
H;uxA
H;V8v1H
H;W(t
H;w8r
H;WXt&H
H;XXs
H;xXu5
H;Y s
H;Y0tLH
H;ZPuOD
h_^[]
H_^][
H~B(LvN
H+,$M
H+\$XE3
H+|$pH
H+A8H;
H+C0H
H+C8H
H+C8H;
H+C8I;
H+D$ H
H+D$PA
H+D$PL;
H+F8H
H+F8H;
H+F8I;
H+F8L;
H+FxH
H+G0H
H+L$(H
H+L$(x?H
H+L$(xAH
H+NHH
H+Q H
H+q0L
H+t$(I
H+t$`O
H+T$8H
H+V0H
h< t <$
h<`@.B,
H0H!H(H!H0H
H0H#M0H
H0H;H8t@H
H0H;H8t<H
H0J2L0N4P
h0NFXHLJZL>HJNLF.
H0T,V0X,Z0\(^0`*b)
h8`<.>
H9[8t
H9\$ u
H9\$0w
H9\$hu
H9^0H
H9{h~(H
H9|$0
H9|$8v H
H9|$Xv H
H9}@t.
H9}@t:
H9}@tH
H9>t#H
H92t.L
H92t'L
H92wCH
H9CHt
H9D$`
H9D$8t
H9D$8t%
H9D$8t0
H9D$h
H9D$HtqI
H9G`v!H
H9Gxt+H
H9H s
H9iHs
H9L$@uVE
H9n(H
H9n8H
H9NHv
H9oHsI
H9P }
H9p s
H9P@r
H9p0H
H9q8v H
H9R8t
H9t$@tRH
H9t$htRH
H9t$hu=H
H9t$pu_H
H9T$PwfH
H9t$xvgL
H9Upt3
H9UX@
H9Uxt3
H9X s
H9x s
H9X0H
H9Y u\
H9Y0H
H9Y0t
H9Z(L
H9z0H
hA^_^[
hA_A^_^[]
HA_A^_^][
hA_A^A]A\_^[]
HA_A^A]A\_^[]
hA_A^A]A\_^[]
hA_A^A]A\_^][
HA_A^A]A\_^][
hA_A^A]A\_^][
HA_A^A]A\_^][
hA_A^A]A\_^][
HA_A^A]A\_^][
hA_A^A]A\_^][
half_pixel
Hangul
Hanifi_Rohingya
Hanunoo
Hardmax
Hardmax inputs N, D and N * D must be < 
HardSigmoid
hardsigmoid
has output size 
has_starts && has_ends && attr_starts_.size() == attr_ends_.size()
HasDataType(dense_proto)
HasExclusiveSum
Hatran
Having memory pattern enabled is not supported while using the DML Execution Provider. 
Hc@ H;
Hc@ H9
Hc@ I9
Hc@ I9E
Hc@ J9
Hc@(H;
Hc\$8
Hc\$PH
Hc^ H
Hc^8H
Hc^PH
Hc{PH
Hc|$4
Hc|$dH
Hc|$x
HcA,H
HcB@H
HcB@H;
HcC(H
HcC(I;
HcC,H
HcC0H
HcC0HcK,H+
HcCxH;
HcD$ H
HcD$$H
HcD$(H
HcD$0H
HcD$pA
HcD$t
HcD$T
HcD$TH
HcE_H
HcE_L
HcEgH
HcEXH
HcExH
HcF H9
HcF I
HcF(L
HcG H
HcG H;
HcG I
HcG$H
HcG$L
HcG(H
HcG(H;
HcG(L
HcG@H;
HcG@L
HcG`L
HcGpH;
HcGTH
HcGXH
HcGXHcOTH
HcH H
HcH H;
HcH@H
HcH0H
HcH0H;
Hci H
HcI0I
HcI8I
HcIHI
HcIPI
HcJ H
HcJ@H
HcJ@I;
HcJ8H
HcJhH
HcJPH
HcK H
HcKPI
HcL$ H
HcL$(H
HcL$pH
HcL$xH
HcMHH
HcMPH
HcN H
HcN H9
HcN$H
HcNPH
HcNpH
HcNPH
HcNXH
HcO H
HcO$L
HcO@H
Hco8H
HcOPH
Hcp H
Hcp M
HcP,H
HcQ H
HcQ,H
HcQ0H
HcQHH
HcQPH
Hcr(L
Hcr@L
HcrpL
HcS H
HcS$H
HcS@H
HcS`H
HcS0H
HcSHH
HcSPH
HcT$ H
HcT$@H
HcT$0H
HcT$8H
HcT$xH
HcT$xHcL$|LcE
HcV$H
Hcw H
HcW H
HcW$L
HcW(H
HcW@H
HcW`H
HcW0H
HcWHH
HcWpH
HcWpI
HcWxH
Hcx D
Hcx H
Hcy H
Hcy,H
HeapAlloc
HeapFree
Hebrew
height_scale
helper.HaveTwoTensorInputs()
hFj(h
HHtfH
HiD$ %y
hidden
hidden_prev
hidden_size
hidden_size != num_heads * head_size
HiddenInitTensor
hipMalloc
Hiragana
hJj8h l
hl^nJr~t2v
hLXNBPfR<NJTNL.
host unreachable
However, the number of key is 
hPXRFTZVPRJXLP,
hResult
https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,
https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html
HUSVH
HUSVWATAUAVH
HUSVWATAUH
HUSVWAVATH
HUSVWAVAUATH
HUSVWAWAVAUATH
HUSVWAWH
HUSVWH
HXHcPPH
HZJjLdNBP`LZT4V~XjZd\B^~ZZb2d
HZJlLdNJP`LXT,V
I != nullptr
i < input_shape.NumDimensions()
i < tensors_.size()
I D9i(
I E8H t:I
I L9AP
I#H0I
I$D9B
I$E;B
i(+i0A
I(H9B
I.e. the output shape should be [C][0] or [N][C][0] if input shape was [N][C].
I;@(u]I
I;@8t^I
I;@8tGI
I;@8tpM
I;@8tqM
I;@8trM
I;@8tsM
I;@8u
I;@8uaI
I;@Ht
I;@Hu
I;BXt
I;C t
I;C u
I;Epu
I;EXt
I;F u
I;F0u
I;FXt
I;G@|
I;GXt
I;Gxu
I;H(u
I;H8tSH
I;H8u
I;I8u
I;JPuMA
I;N t=H
I;QHs
I;RHt
I;RPuIA
I;U8v
I;x0|
I;Y8|
I;zPuIA
I+<$H
I+4$H
I+C0H
I+CHH
I+EP3
I+F8H;
I+oPH
I+V`H
I+VHH
I9.tI
I9^ tGA
I9^(I
I9^(M
I9~0M
I9>tW
I90t%L
I90u1L
I9Jhs
I9Khs
I9OXt
I9p0~FH
I9t$8
I9vHs
I9x0~JI
I9Y8~
Iba|H
IbbeHQ
IbR=@
IbR=P
Ic@(H
Ic@@H
Ic@8H
Ic@pH
Ic^ I
Ic^P3
IcE H
IcEhL
IcEPH
IcETH
IcETI
IcETI9
IcF ;G t
IcF H
IcFTH
IcFXH
IcG H
IcG H;
IcGPL
IcH H
IcH(H;
IcH(I
IcH@H;
IcH@I
IcH8H
IcHhH
IcHPH
IcHpH;
IcHpI
IcJ I
IcK I
IcM H
IcM I
IcM8H
IcMhH
IcMPH
IcN H
IcN8H
IcNhH
IcNPH
IcO H
IcOP3
IcOPH
IcP(H
IcPhH
IcQ8I
IcT$ 
Icu H
Icu L
IcU8H
Icv(3
Icw H
id >= 0 && static_cast<size_t>(id) < ort_value_info_.size()
ID;H,tQH
identifier removed
Identity
IExecutionProvider constructor must be called with true for use_metadef_id_creator
IExecutionProvider::Compile with fused Node and dll path is not implemented by 
IExecutionProvider::Compile with fused Node is not implemented by 
IExecutionProvider::Compile with FusedNodeAndGraph is not implemented by 
If `axes` are omitted, they are set to `[0, ..., ndim-1]`.
If 0, normalize the mean only.  Default is 1.
If 1, mean and variance are computed across channels. Default is 0.
if coordinate_transformation_mode is "align_corners", <br/>
if coordinate_transformation_mode is "asymmetric", <br/>
if coordinate_transformation_mode is "half_pixel", <br/>
if coordinate_transformation_mode is "pytorch_half_pixel", <br/>
if coordinate_transformation_mode is "tf_crop_and_resize", <br/>
if coordinate_transformation_mode is "tf_half_pixel_for_nn", <br/>
If keepdims equal 0, then the resulting tensor have the reduced dimension pruned.
If necessary the right-hand-side argument will be broadcasted to match the
If node has 
'If' node has 
If scale is not provided, crop the borders as provided.
If set to 1 will perform the sums in reverse direction.
If set to 1 will return exclusive sum in which the top element is not included. In other terms, if set to 1, the j-th output element would be the sum of the first (j-1) elements. Otherwise, it would be the sum of the first j elements.
If set to 1, the weight of sampling locations outside the tensor will be set to 0 and the weight will be renormalized so that their sum is 1.0. The default value is 0.
If set to nonzero, run spatial batch normalization in test mode, default is 0.
If set, defines the broadcast dimensions.
If set, defines the broadcast dimensions. See doc for details.
If shape was concrete we shouldn't be using a custom allocator
If the tokenizer receives empty input of [0] then the output is [0] if empty input
If the value of map_form is 'SPARSE,' this attribute indicates the total length of the output tensor.
If tokenizer removes the entire content of [C]-input, it will produce [[]].
If true and category is not present, will return all zeros; if false and a category if not found, the operator will fail.
If true, compute the mean and variance across all spatial elements If false, compute the mean and variance across per feature.Default is 1.
If true, compute the mean and variance across per activation. If false, compute the mean and variance across per feature over each mini-batch.
If value is 1, output type is uint32_t, else int32_t. Default value is 1.
ignore_index
Ignoring unsupported session option in ORT config: 
IHHcB
IHL;IPu
illegal byte sequence
illegal input path:
ImageScaler
Imperial_Aramaic
impl_->max_gram_length_ >= impl_->min_gram_length_
impl_->max_skip_count_ >= 0
impl_->min_gram_length_ > 0
impl_->weighting_criteria_ != kNone
impl_->weights_.size() == impl_->ngram_indexes_.size()
imputed_value_floats
imputed_value_int64s
imputed_values_float_.empty() ^ imputed_values_int64_.empty()
Imputer
in a sequence-length aware fashion.
in onnx/defs/schema.h).
in the inclusive range [
in[idx]->IsTensor()
inappropriate io control operation
IncludePadding
Incompatible dimensions
Incompatible dimensions for matrix multiplication
Incompatible matrix dimensions for matMul
Inconsistent shape in loop output for output. 
Incorrect arena extend strategy.
Incorrect or missing attribute value for starts and ends
Incorrect or missing input value for starts and ends
index < data_.size()
index >= 0 && static_cast<size_t>(index) < inputs.size()
index >= 0 && static_cast<size_t>(index) < outputs.size()
index out of range
Index tensor shape should be same as that of the input data tensor to unpool.
IndexDimensions
IndexedSubGraph contains values not present in the Graph
Indicate up to which input dimensions (exclusive) should be flattened to the outer dimension of the output. The value for axis must be in the range [0, R], where R is the rank of the input tensor. When axis = 0, the shape of the output tensor is (1, (d_0 X d_1 ... d_n), where the shape of the input tensor is (d_0, d_1, ... d_n). 
Indicate up to which input dimensions (exclusive) should be flattened to the outer dimension of the output. The value for axis must be in the range [-r, r], where r is the rank of the input tensor. Negative value means counting dimensions from the back. When axis = 0, the shape of the output tensor is (1, (d_0 X d_1 ... d_n), where the shape of the input tensor is (d_0, d_1, ... d_n). 
Indicates the transform to apply to the regression output vector.<br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
Indicates the transform to apply to the score. <br> One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT.'
Indicates the transform to apply to the score. <br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
Indicates the transform to apply to the score. <br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT.'
Indicates the transform to apply to the scores vector.<br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
Indicates whether to do OvR or multinomial (0=OvR is the default).
Indicates whether to only output as many values as are in the input (dense), or position the input based on using the key of the map as the index of the output (sparse).<br>One of 'DENSE', 'SPARSE'.
indices
Indices
Indices and updates must have the same rank
Indices dim=
indices element out of data bounds, idx=
Indices must have the same rank as Input. Indices rank=
indices tensor data type not supported
indices tensor must has rank larger than 0
Indices tensor must have rank >= 1
Indices vs updates dimensions differs at position=
IndicesDimensionCount
IndicesTensor
Indicies expected to be INT64
ineIu
InferenceSession is null. Invalid ORT format model.
Inferred elem type differs from existing elem type: (
Inferred shape and existing shape differ in dimension 
Inferred shape and existing shape differ in rank: (
InfinityMode
info == nullptr
info.GetAttr("alpha", &alpha_).IsOK()
info.GetAttr("beta", &beta_).IsOK()
info.GetAttr("blocksize", &blocksize_).IsOK()
info.GetAttr("direction", &direction).IsOK()
info.GetAttr("direction", &direction_).IsOK()
info.GetAttr("hidden_size", &hidden_size_).IsOK()
info.GetAttr("hidden_size", &int64_value).IsOK() && int64_value > 0
info.GetAttr("keepdims", &keepdims).IsOK()
info.GetAttr("linear_before_reset", &int64_value).IsOK()
info.GetAttr("num_heads", &num_heads).IsOK() && num_heads > 0
info.GetAttr("scale", &scale_).IsOK()
info.GetAttr("storage_order", &storage_order).IsOK()
info.GetAttr<float>("alpha", &alpha_).IsOK()
info.GetAttr<float>("beta", &beta_).IsOK()
info.GetAttr<float>("high", &high_).IsOK()
info.GetAttr<float>("low", &low_).IsOK()
info.GetAttr<float>("mean", &mean_).IsOK()
info.GetAttr<float>("scale", &scale_).IsOK()
info.GetAttr<float>("spatial_scale", &spatial_scale_).IsOK()
info.GetAttr<int64_t>("across_channels", &across_channels_).IsOK()
info.GetAttr<int64_t>("axis", &axis_).IsOK()
info.GetAttr<int64_t>("batch_axis", &batch_axis).IsOK()
info.GetAttr<int64_t>("channels", &channels_).IsOK()
info.GetAttr<int64_t>("channels_last", &channels_last_).IsOK()
info.GetAttr<int64_t>("count_include_pad", &temp).IsOK()
info.GetAttr<int64_t>("default_int64", &default_int_).IsOK()
info.GetAttr<int64_t>("dtype", &dtype).IsOK()
info.GetAttr<int64_t>("max_map", &max_map_).IsOK()
info.GetAttr<int64_t>("normalize_variance", &normalize_variance_).IsOK()
info.GetAttr<int64_t>("num_scan_inputs", &num_scan_inputs_).IsOK()
info.GetAttr<int64_t>("p", &p_).IsOK()
info.GetAttr<int64_t>("sample_size", &num_samples_).IsOK()
info.GetAttr<int64_t>("size", &size).IsOK()
info.GetAttr<int64_t>("targets", &num_targets_).IsOK()
info.GetAttr<int64_t>("time_axis", &time_axis).IsOK()
info.GetAttr<int64_t>("transA", &temp).IsOK()
info.GetAttr<int64_t>("transB", &temp).IsOK()
info.GetAttr<int64_t>("upper", &temp).IsOK()
info.GetAttr<ONNX_NAMESPACE::GraphProto>("body", &proto).IsOK()
info.GetAttr<ONNX_NAMESPACE::GraphProto>("else_branch", &proto).IsOK()
info.GetAttr<ONNX_NAMESPACE::GraphProto>("then_branch", &proto).IsOK()
info.GetAttr<std::string>("auto_pad", &auto_padding).IsOK()
info.GetAttr<std::string>("cast_to", &attr).IsOK()
info.GetAttr<std::string>("default_string", &default_string_).IsOK()
info.GetAttr<std::string>("equation", &equation_).IsOK()
info.GetAttr<std::string>("map_form", &attr).IsOK()
info.GetAttr<std::string>("metric", &metric).IsOK()
info.GetAttr<std::string>("mode", &mode).IsOK()
info.GetAttr<std::string>("norm", &norm).IsOK()
info.GetAttrs("activations", activations_).IsOK()
info.GetAttrs("axes", axes_).IsOK()
info.GetAttrs(std::is_same<AttrType, std::string>::value ? "string_vocabulary" : "int64_vocabulary", vocabulary_).IsOK()
info.GetAttrs<float>("bias", bias_).IsOK()
info.GetAttrs<float>("coefficients", coefficients_).IsOK()
info.GetAttrs<float>("kernel_params", kernel_params).IsOK()
info.GetAttrs<float>("rho", rho_).IsOK()
info.GetAttrs<float>("scales", scales_).IsOK()
info.GetAttrs<int64_t>("cats_int64s", int_categories).IsOK()
info.GetAttrs<int64_t>("kernel_shape", kernel_shape).IsOK()
info.GetAttrs<int64_t>("kernel_shape", kernel_shape_).IsOK()
info.GetAttrs<int64_t>("pooled_shape", pooled_shape).IsOK()
info.GetAttrs<int64_t>("scales", scales_).IsOK()
info.GetAttrs<int64_t>("shape", shape).IsOK()
info.GetAttrs<std::string>("cats_strings", string_categories).IsOK()
info.GetAttrs<std::string>("classes_strings", string_classes).IsOK()
info.GetAttrs<std::string>("classlabels_strings", classlabels_strings_).IsOK() || info.GetAttrs<int64_t>("classlabels_ints", classlabels_ints_).IsOK()
info.GetAttrs<TKey>(_key_field_name, keys).IsOK()
info.GetAttrs<TValue>(_value_field_name, values).IsOK()
info_ == nullptr
Inherited
initial_c
initial_h
initial_state_and_scan_inputs
Initial_ZeroPoint_FP
initialize preallocated buffer failed
InitializeCriticalSectionAndSpinCount
InitializeCriticalSectionEx
Initialized tensor with unexpected type: 
Initializer 
Initializer tensor is missing. Invalid ORT format model.
Initializer with same name exists. Name:
InitializeSListHead
InitializeSRWLock
Initializing session.
InitOnceExecuteOnce
input
Input
Input 
input 
Input 0 and 1 shall have same shape
Input 0 and 7 (mask) shall have same shape
Input 0 dimension 2 should be divisiable by value of the num_heads attribute.
Input 0 is expected to have 1 or more dimensions, got 
Input 1 dimension 0 should have same length as dimension 2 of input 0
Input 1 dimension 0 should have same length as the last dimension of input 0
Input 1 is expected to have 1 dimensions, got 
Input and output types can be of any tensor type.
Input and target dimension value mismatch.
input and zero_point pair is expected to have be same type.
input and zero_point pair is expected to have same type.
input array doesn't equal tensor size
input array is too short
Input axes has incorrect length
Input axes has invalid data
Input axis is invalid: 
Input B must have shape {
Input B should not be null.
Input 'bias' dimension 0 should have same length as dimension 1 of input 'weights'
Input 'bias' is expected to have 1 dimension, got 
Input can be of any tensor type.
Input cannot be split evenly on selected axis. Input shape=
Input channels C is not equal to kernel channels * group.
Input channels is not divisible by group.
Input contains invalid utf8 chars at: 
input count mismatch
input count mismatch, expected 1 input - the tensor to be processed
input count mismatch, expected 2 inputs - the tensor to be processed and a tensor containing k value
Input count of Tile OP mismatch, the first one is empty
Input count of Tile OP mismatch, the second one is empty
Input data tensor from the previous layer.
Input data tensor from the previous operator; 4-D feature map of shape (N, C, H, W), where N is the batch size, C is the number of channels, and H and W are the height and the width of the data.
Input data type does not match the expected data type
Input data type does not match the expected data type. Current data type is 
Input data type is not int32 or int64
Input data with index: 
Input 'depth' must be a scalar or rank 1 tensor.
Input 'depth' must have exactly one element.
Input dim is zero but required output dim is non-zero. 
Input dimension cannot be less than 3.
Input dimensions are either [C] or [N][C] allowed
Input dimensions are either[C > 0] or [1][C > 0] allowed
input edges
Input element type of 
Input features_per_batch[
Input id is not valid. 
input index out of range
input index: 
Input initial_c must have shape {
Input initial_h must have shape {
Input 'input' is expected to have 3 dimensions, got 
Input is ether string UTF-8 or int32/int64
input is expected to have 3 dimensions, got 
Input is expected to have dim value in all dimensions.
Input is expected to have four dimensions corresponding to [N,C,H,W]
Input is expected to have four dimensions corresponding to [N,C,H,W], got 
Input is not of one of the supported map types.
Input is not of one of the supported sequence types.
Input is not of type sequence or map.
Input 'mask_index' is expected to have 1, 2 or 3 dimensions, got 
'input' must have rank >= 2
input name cannot be empty
Input of int64 must have output of string 
Input of reshape_before_gemm is not the input of subgraph
Input of string must have output of int64
Input of tensor(int64) must have output of tensor(string)
Input of tensor(string) must have output of tensor(int64)
Input P must have shape {
Input 'past' is expected to have 5 dimension, got 
Input R must have shape {
Input rank must be >= 2.
input scale must be a scalar or 1D tensor of size 1
Input 'scales' must have float element type.
Input Sequence and Tensor are expected to have the same elem type. Sequence=
Input Sequence and Tensor are expected to have type info. Current type is null.
Input sequence_lens must have shape {
Input shape dimensions mismatch:
Input shape had more than 2 dimension. Dims=
Input shape is unknown or not 2D, or data type unknown
Input shape must have either [C] or [1,C] dimensions where C > 0
Input shape must have either [C] or [B,C] dimensions with B > 0.
Input shape needs to be at least a single dimension.
input shape: 
Input 'sizes' must have int64 element type.
Input 'split' can not be empty.
Input steps has incorrect length
Input string contains invalid utf8 chars: 
input tensor
Input tensor
Input tensor A. The shape of A should be (M, K) if transA is 0, or (K, M) if transA is non-zero.
input tensor and indices tensor must has rank larger than 0. 
Input tensor B. The shape of B should be (K, N) if transB is 0, or (N, K) if transB is non-zero.
Input tensor C. The shape of C should be unidirectional broadcastable to (M, N).
Input tensor can be of arbitrary type.
Input tensor has no dimensions
Input tensor must be 2-dimensional
Input tensor must be 4-dimensional
Input tensor must have at least 2 dimensions
Input tensor must have atleast 2 dimensions
Input tensor must have rank 1 or 2
Input tensor must have rank 2
Input tensor of rank 2 or higher.
Input tensor of shape [N,C,H,W]
Input tensor should have a rank of at least 2
Input tensor to Unique op should be 1D
Input tensor X must have atleast 2 dimensions.
Input tensor.
Input tensor. Every matrix in the batch must be invertible.
Input tensors of wrong rank (0).
Input to be reduced is null
Input to 'Range' op should be scalars (Tensor with only one element and shape empty)
Input to set must exist.
Input type for input at index 
Input type for input at index 0 is null. Type info is expected.
Input type is not float tensor but keys_floats is set
Input type is not int64 tensor but keys_int64s is set
Input type is not string tensor but key_strings is set
Input type was null
Input 'values' must be rank 1 tensor.
Input 'values' must have exactly two elements.
Input W must have shape {
Input was expected to have either tensor or sequence type. Got 
Input was expected to have sequence type. Got 
Input was expected to have tensor type. Got 
Input 'weights' dimension 1 should be 3 times of dimension 0
Input 'weights' is expected to have 2 dimensions, got 
Input with name: 
Input X must have 3 dimensions only. Actual:
Input x_scale must be a scalar or 1D tensor of size 1
input x_zero_point must be a scalar or 1D tensor of size 1 if given
input y_scale must be a scalar or 1D tensor of size 1
input y_zero_point must be a scalar or 1D tensor of size 1 if given
input zero point must be a scalar or 1D tensor of size 1.
input.Shape().NumDimensions() == 4
Input/Output is a string tensor
input_0
input_1
input_1.DataType() == input_2.DataType()
input_arg->Type() != nullptr
input_as_shape
input_copy_needed != DeviceCopyCheck::Unknown && output_copy_needed != DeviceCopyCheck::Unknown
input_count >= 0 && static_cast<size_t>(input_count) == input_dimensions_.size()
input_count >= 1
input_depth % (blocksize_ * blocksize_) == 0
input_dims.size() >= 2
input_dims[rank - 2] == input_dims[rank - 1]
input_forget
input_gather_element
input_gather_element_transform
input_height % this->blocksize_ == 0
input_ids
Input_ids and segment id should have the same shape. 
input_ids is expected to have 2 dimensions, got 
input_indices.size() == expected_values.size() && input_indices.size() > 0
input_node.InputDefs().size() == 2 && scale_and_index.value().second < 2
input_num_bytes % 4 == 0
input_rank == permutation.size()
input_scale
input_sequence
input_shape.NumDimensions() == 4
input_shape.Size() > 0 || input_shape[0] == 0
input_shape.Size() > 0 || N == 0
input_shape[i] == 1
input_shape_1_override.size() == 3 && input_shape_2_override.size() == 3
input_shape_1_override[0] == input_shape_2_override[0]
input_shape_1_override[2] == input_shape_2_override[1]
input_size < std::numeric_limits<std::ptrdiff_t>::max()
input_tensor != nullptr && indices_tensor != nullptr
input_tensor_ptr != nullptr
input_tensor_ptr->Shape().Size() == input_shape_override->Size()
input_width % this->blocksize_ == 0
input_zero_point
input->Exists()
InputBroadcaster can only start at span boundary!
InputCount
inputCount >= 1
InputDimensionCount
inputdimensions
inputdimensions attribute must be provided
InputFirstMomentTensor
InputGradientTensor
InputParametersTensor
InputPixelOffsets
Inputs
inputs
Inputs 0 shall be 2 dimensions
Inputs 0 shall be 3 dimensions
Inputs 4 shall be 5 dimensions
inputs are expected to have tensor type and output type should not be null.
inputs are expected to have tensor type.
inputs by their magnitude, rather than gates inputs by their sign as in ReLUs.
Inputs have ellipses in them but the provided output subscript does not contain an ellipsis
Input's height (
Inputs 'mask_index' dimension 0 shall have length of batch_size or 2 * batch_size
Inputs 'mask_index' of 3d shall have shape batch_size x sequence_length x (past_sequence_length + sequence_length)
Inputs 'mask_index' with raw attention mask shall have shape batch_size x (past_sequence_length + sequence_length)
Inputs 'past' dimension 0 shall have length of 2
Inputs 'past' dimension 1 shall have same length as dimension 0 of input 0
Inputs 'past' dimension 2 shall have length of 
Inputs 'past' dimension 2 shall have length of num_heads
Input's shape must be 4-D
Input's shape should be 1D or 2D
Input's width (
inputs_n_rank == inputs_0_rank
InputScaleTensor
InputSecondMomentTensor
InputStateTensor
InputTensor
InputTensors
InputWindowOffsets
InputWindowSizes
InputWindowStrides
InputZeroPointTensor
Inscriptional_Pahlavi
Inscriptional_Parthian
Insert and concatenate on a new axis or not, default 0 means do not insert new axis.
InsertCastTransformer works on the assumption that `dtype` attribute holds an integer.
Inserted_Cast
InstanceNormalization
Insufficient dimensions to slice on 
int16
int32
int32_data
int64
Int64 tensor
int64_data
int64_vocabulary
Integer indicate the format of the box data. The default is 0. 0 - the box data is supplied as [y1, x1, y2, x2] where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Mostly used for TF models. 1 - the box data is supplied as [x_center, y_center, width, height]. Mostly used for Pytorch models.
Integer overflow
Integer representing the embedding vector size for each char.If not provide, use the char embedding size of embedding vector.
Integer representing the embedding vector size for each word.If not provide, use the fileter size of conv weight
inter_op_num_threads
inter_op_num_threads option in the model file must be an integer
intercepts
InterlockedFlushSList
InterlockedPushEntrySList
internal error
Internal error in BatchNormalizationMulFusion. BatchNormalization_B_tensor_proto is NULL
Internal error.
Internal error. The preallocated buffer is too small. Requires 
InternalName
InternalTestingExecutionProvider
inter-op
InterpolationMode
interrupted
intra_op_num_threads
intra_op_num_threads option in the model file must be an integer
intra-op
inv_std_var
Invalid activation function of 
Invalid allocation kind: 
Invalid arg_num of 
invalid argument
Invalid argument for depth; it's not a scalar.
Invalid argument for values; either it's rank is more than 1 or it has more than 2 elements
Invalid argument: input has empty dimensions.
Invalid argument: X input has empty dimensions.
Invalid assumption of output element size
Invalid attribute perm {
Invalid axes attribute, axes attribute (if present) should have the same size as starts/ends attributes
Invalid batch_axis of 
invalid BOM; must be 0xEF 0xBB 0xBF if given
Invalid CAST_TO value of 
invalid channel count
invalid character class
invalid character class range
Invalid data type for GRU operator of 
Invalid data type for LSTM operator of 
Invalid data type for split tensor 
Invalid data type of 
Invalid DataTypeImpl TypeProto definition
Invalid destination node arg slot specified when adding edge.
Invalid destination node arg slot specified when removing edge.
Invalid dim0_offset of 
Invalid dimension of 
Invalid dimension value: 
Invalid 'direction' argument of '
Invalid direction value of '
Invalid dtype of 
Invalid 'end'. Value is larger than 'start'.
Invalid entries in sequence_lens. Max sequence length was 
invalid escape sequence
Invalid ExecutionOrder
invalid expand shape
Invalid fd was supplied: 
Invalid Feed Input Name:
Invalid free dimension override.
Invalid GRU hidden gate activation function: 
Invalid GRU reset gate activation function: 
invalid hash bucket count
invalid index 
invalid index found, index = 
Invalid index requested for map type.
invalid indice found, indice = 
Invalid input B: 
Invalid input B: 0th dimension != 
Invalid input B: number of dimensions is not 1: 
Invalid input B: NumDimensions() != 
Invalid input data: number of dimensions is less than 3: 
Invalid input index for node 
Invalid input mean: 
Invalid input mean: 0th dimension != 
Invalid input mean: NumDimensions() != 
Invalid input scale: 
Invalid input scale: 0th dimension != 
Invalid input scale: number of dimensions is not 1: 
Invalid input scale: NumDimensions() != 
Invalid input shape. Only N can be zero. Got:
Invalid input shape: 
Invalid input type of value: 
Invalid input type:
Invalid input var: 
Invalid input var: 0th dimension != 
Invalid input var: NumDimensions() != 
Invalid input X: The rank of input X must be atleast 2. Got rank: 
invalid literal
invalid location range
Invalid LSTM merge activation function of 
invalid map<K, T> key
Invalid 'mode' attribute value
Invalid mode of value 
invalid named capture group
Invalid node indexes specified when adding edge.
Invalid node indexes specified when removing edge.
Invalid normalize value of 
invalid number; expected '+', '-', or digit after exponent
invalid number; expected digit after '-'
invalid number; expected digit after '.'
invalid number; expected digit after exponent sign
Invalid ORT format model.
invalid ort_value_index:
Invalid Output Name:
Invalid PACK_MAP value of 
Invalid 'pads' attribute value
invalid perl operator
Invalid position of 0
Invalid program_counter entries at index 
Invalid rank for input: 
Invalid RE2: 
invalid repetition size
Invalid roi input index.
Invalid run log severity level. Not a valid onnxruntime::logging::Severity value: 
invalid scales dimension
invalid scales value
Invalid scan input:
invalid seek
Invalid sequence index (
Invalid session log severity level. Not a valid onnxruntime::logging::Severity value: 
Invalid shape value: 
Invalid source node arg slot specified when adding edge.
Invalid source node arg slot specified when removing edge.
Invalid SparseTensor indices. Should be rank 0 or 1. Got:
Invalid SparseTensor indices. Should either have raw or int64 data
Invalid 'start'. Value is smaller than previous 'end'.
Invalid start/ending offset [
invalid stod argument
invalid stof argument
invalid stoll argument
invalid stoull argument
invalid string position
invalid string: '\u' must be followed by 4 hex digits
invalid string: control character U+0000 (NUL) must be escaped to \u0000
invalid string: control character U+0001 (SOH) must be escaped to \u0001
invalid string: control character U+0002 (STX) must be escaped to \u0002
invalid string: control character U+0003 (ETX) must be escaped to \u0003
invalid string: control character U+0004 (EOT) must be escaped to \u0004
invalid string: control character U+0005 (ENQ) must be escaped to \u0005
invalid string: control character U+0006 (ACK) must be escaped to \u0006
invalid string: control character U+0007 (BEL) must be escaped to \u0007
invalid string: control character U+0008 (BS) must be escaped to \u0008 or \b
invalid string: control character U+0009 (HT) must be escaped to \u0009 or \t
invalid string: control character U+000A (LF) must be escaped to \u000A or \n
invalid string: control character U+000B (VT) must be escaped to \u000B
invalid string: control character U+000C (FF) must be escaped to \u000C or \f
invalid string: control character U+000D (CR) must be escaped to \u000D or \r
invalid string: control character U+000E (SO) must be escaped to \u000E
invalid string: control character U+000F (SI) must be escaped to \u000F
invalid string: control character U+0010 (DLE) must be escaped to \u0010
invalid string: control character U+0011 (DC1) must be escaped to \u0011
invalid string: control character U+0012 (DC2) must be escaped to \u0012
invalid string: control character U+0013 (DC3) must be escaped to \u0013
invalid string: control character U+0014 (DC4) must be escaped to \u0014
invalid string: control character U+0015 (NAK) must be escaped to \u0015
invalid string: control character U+0016 (SYN) must be escaped to \u0016
invalid string: control character U+0017 (ETB) must be escaped to \u0017
invalid string: control character U+0018 (CAN) must be escaped to \u0018
invalid string: control character U+0019 (EM) must be escaped to \u0019
invalid string: control character U+001A (SUB) must be escaped to \u001A
invalid string: control character U+001B (ESC) must be escaped to \u001B
invalid string: control character U+001C (FS) must be escaped to \u001C
invalid string: control character U+001D (GS) must be escaped to \u001D
invalid string: control character U+001E (RS) must be escaped to \u001E
invalid string: control character U+001F (US) must be escaped to \u001F
invalid string: forbidden character after backslash
invalid string: ill-formed UTF-8 byte
invalid string: missing closing quote
invalid string: surrogate U+DC00..U+DFFF must be followed by U+DC00..U+DFFF
invalid string: surrogate U+DC00..U+DFFF must follow U+D800..U+DBFF
Invalid Target shape product of 0
Invalid tensor shape slice argument.
Invalid TensorProto
Invalid time_axis of 
Invalid type
invalid unordered_map<K, T> key
Invalid usage. Input 1 is a shape with no data.
invalid UTF-8
Invalid value for attribute axis
Invalid value for attribute k
Invalid value in scan_input_axes for input 
Invalid value in scan_output_axes for output 
Invalid value in 'split' attribute. All values must be > 0
Invalid value in 'split' input. All values must be >= 0
Invalid value of attribute 'axis'. Accepted range=[
Invalid value of attribute 'axis'. Rank=
Invalid value(
Invalid value/s in sequence_lens. All values must be > 0 and < seq_length. seq_length=
Invalid values in '
invalid vector<T> subscript
Invalid Y argument: index is out of range: Y[
Invalid Y argument: num_indices = 0
INVALID_ARGUMENT
INVALID_GRAPH
invalid_iterator
INVALID_PROTOBUF
Inverse
inverse_indices
io error
ios_base::badbit set
ios_base::eofbit set
ios_base::failbit set
iostream
iostream stream error
iou_threshold
iou_threshold must be in range [0, 1].
Irfft
irVersion
is a directory
is applied to the data tensor elementwise.
is applied to the tensor elementwise.
'is defined.
is not supported.
is_case_sensitive
is_concrete_shape_
is_model_proto_parsed
is_test
IsDebuggerPresent
IsInf
ISink must be provided.
IsNan
IsNaN
IsProcessorFeaturePresent
isRedist
IsScalarOr1ElementVector(a_offset)
IsScalarOr1ElementVector(a_scale)
IsScalarOr1ElementVector(a_scale_tensor)
IsScalarOr1ElementVector(a_zero_point)
IsScalarOr1ElementVector(a_zero_point_tensor)
IsScalarOr1ElementVector(b_offset)
IsScalarOr1ElementVector(b_scale)
IsScalarOr1ElementVector(b_scale_tensor)
IsScalarOr1ElementVector(b_zero_point)
IsScalarOr1ElementVector(b_zero_point_tensor)
IsScalarOr1ElementVector(k)
IsScalarOr1ElementVector(tensor_a_scale)
IsScalarOr1ElementVector(tensor_b_scale)
IsScalarOr1ElementVector(tensor_c_scale)
IsScalarOr1ElementVector(tensor_x_scale)
IsScalarOr1ElementVector(tensor_x_zero_point)
IsScalarOr1ElementVector(tensor_y_scale)
IsScalarOr1ElementVector(tensor_y_zero_point)
IsScalarOr1ElementVector(W_Zero_Point)
IsScalarOr1ElementVector(X_scale)
IsScalarOr1ElementVector(X_zero_point)
IsScalarOr1ElementVector(X_Zero_Point)
IsScalarOr1ElementVector(y_offset)
IsScalarOr1ElementVector(y_scale)
IsScalarOr1ElementVector(Y_scale)
IsScalarOr1ElementVector(Y_zero_point)
IsSparseTensor()
IsTensor()
IsTensorSequence()
it->i < (int64_t)predictions.size()
iteration_num_ < sequence_len_
iterator does not fit current value
iterator out of range
itr != node.InputDefs().end()
It's an extension of Gelu. It takes the sum of input A and bias input B as the input of Gelu activation. 
J D"\ l
j"J$T448
J$NL@
J(H;J0
J(H;J0tVH
J(L9B
J*`..0,
J,`0.2,
J,L N&L5
J`HcBXL
Javanese
jbljn
JD9H(tKH
JHI;JPu
JHL(N
JHL(NjJxLHN(L^JQ
JHL(NrJhLHN(L:JY
job_.size() = 
Json stored in the `ort_config` key cannot be parsed. Error message: 
jXlhndpXrTt6x
k and v are not from same Split node
k argument [
k D+k0D
K H+K
K H91u
K HcC,H
K input must be a one-dimensional tensor of size 1.
K input must be of type int64.
K L+K
K L91u
k root is not layer norm
k should be a 1-D or 0-D tensor.
K SUVWAVAWH
k tensor should be a 1D tensor of size 1
k VWAVH
K(9H(u
K,9H,u
k_matmul and k_add shape not matched
k_reshape const not matched
k_temp > 0
k_transpose perm attribute not matched
K+4/x#H
k0H+k(H
K0H9KHu
K0HcQ
K8Hcy
Kaithi
Kannada
Katakana
Kayah_Li
KbbeHQ
Keep the reduced dimension or not, default 1 mean keep reduced dimension.
Keep the split dimension or not. Default 1, which means we keep split dimension. If input 'split' is specified, this attribute is ignored.
keep_dims
keepdims
keepdims_
kernel != nullptr
Kernel create info hashes are null. Invalid ORT format model.
Kernel create info is null. Invalid ORT format model.
Kernel create info node indices are null. Invalid ORT format model.
kernel def can't be NULL
Kernel not found
kernel_params
kernel_shape
kernel_shape is not compatible with W shape.
kernel_shape num_dims is not compatible with W num_dims.
kernel_shape num_dims is not compatible with X num_dims.
kernel_shape[dim] > 0
kernel_shape_[dim] > 0
kernel_type
kernel32.dll
kernelbase.dll
key '
Key and value tensors have unequal number of elements.
Key type is not supported yet.
key_type
keys_floats
keys_int64s
keys_strings
Kharoshthi
kHH;kPu
KHL9C`u&
KHM;KPu
Khmer
Khojki
Khudawadi
known by the checker.
KPH+KHH
kRegexpCapture cap() == 0
KxHcCpL
L *"|$V&h(X*B,j(L0*2d4
L B"@$`(l,f0f4<6f:f>fBfFNJdNdRhVdZf^<`4b
L!|$(L!
L!|$@H
L!|$@L
L!|$h
L!}xI
L!B 3
L!d$(L!d$@D
L!d$03
L!d$HH
L!d$PH
L!mXH
L!mXI
L!t$@L
L!t$pI
L!t$XH
L#^0H
L$ 9H uSA
L$ E2
L$ E3
l$ E3
L$ E3
L$ fE#
L$ H;
L$ H;O
L$ H+
L$ H3
l$ I;
L$ I;
L$ I+
l$ I+
l$ Ic
l$ Icm H
L$ L;
L$ SUVWATAUAVAWH
L$ SUVWH
L$ SVWAVH
L$ UVWATAUAVAWH
l$ VWATAUAVAWI
l$ VWATAUAVAWL
l$ VWATAUAVAWM
l$ VWATAUAVH
l$ VWATAVAWH
l$ VWATAVAWI
l$ VWATAVAWM
l$ VWAUAVAWH
l$ VWAVH
l$ VWI
l$ WH
L$(9J(
L$(D;
l$(D8aX
L$(E3
l$(E3
L$(E3
L$(H!D$8H
L$(H!D$8L
l$(H;
L$(H;
l$(H;
L$(H;
l$(H;
L$(H;
l$(H;
L$(H;
l$(H;
L$(H+
L$(H3
L$(I!C
L$(L;
L$(L+
l$(M+
l$(Mc} H
L$,L;
L$@;|
L$@A+
l$@E3
L$@E3
l$@E3
L$@H!L$HH
L$@H;
L$@H;]ht
L$@H;Op
L$@H+
L$@H+L$8H
L$@H3
L$@I;
l$@I;
L$@I;
L$@I9Np
L$@Ic
l$@Ic
L$@L;
L$@L+<
l$@L9m
L$`;M
L$`A9O 
L$`A9v
L$`E3
l$`E3
L$`E3
L$`H;
L$`H+
L$`H3
L$`H9
L$`I;
L$`L;
l$`M9Y@
L$|L;
L$<Lc
l$0@8}
l$0@8u
l$0_^
L$09Q |
L$0A;
l$0D8
L$0D9A |
l$0D9h
L$0E3
l$0E9i(
l$0E9n(
L$0fD
L$0fH
l$0H!|$8
l$0H;
L$0H;
l$0H;
L$0H;
L$0H;L$hu
l$0H;V
L$0H+
L$0H3
L$0H9
L$0H9Q }
l$0Hc
l$0I;
l$0I;V
l$0L!L$(I
L$0L;
l$0L9
L$0L99t
l$0L9o
l$0M;
L$0tA
L$0tz
l$48\$<tJH
l$4D8d$2u
l$4E3
L$4L;
L$4Lc`
L$8D;G
l$8D;l$@
L$8D9(
l$8E3
L$8E3
l$8E3
l$8H;
L$8H;
l$8H;
L$8H;
L$8H;Aht
L$8H+
l$8H+
L$8H3
L$8Hc
l$8I#
L$8I+
L$8L;
L$8L+
L$8tEH
L$DHcP
L$H;C 
L$h;M
l$HA_A^A\_^
l$HE3
L$hE3
l$hE3
L$HE3
L$hfH
L$hH!D$xI
L$HH;
l$HH;
L$HH;
L$hH;L$pt
L$HH;L$pt)
L$hH+
l$hH+
L$HH+
L$hH+
L$HH+
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$hH3
L$HH3
L$HH9
L$hHc
L$hHcIXLc
l$HL;
L$HL9m
l$hL9n
L$hLcC D
l$HM;
l$huDM
l$huEM
l$huFM
l$huGM
l$huHM
l$huJM
l$huRM
l$LfD
L$P;M
L$p+M
l$PA_A^A]A\_^
l$pA8]
L$PD!d$X
l$pE3
L$PE3
l$PE3
L$PE3
l$pE3
L$PE3
l$pE3
L$PE3
l$pE8
l$pfD
L$pH;
L$PH;
L$pH;
L$PH;
L$PH;L$Xt,
L$PH+
L$pH+
l$PH+]
L$PH3
L$pH3
L$PH3
L$pH3
L$PH3
L$pH3
L$PH3
L$pH3
L$PH3
L$pH3
L$PH3
L$pH3
L$PH3
L$pH3
L$PH3
L$pH3
L$PH3
L$pH3
L$PH3
L$pH3
L$PH3
L$pH3
L$PH3
l$pHc
l$pI+
L$pL;
l$PM;
l$pM;Q`s7I
L$T HcMHH
L$X9xT
L$XD!e
l$xD9i 
L$XE3
L$xE3
L$XE3
L$XE9STu1H
L$XfH
L$xfI
L$XH!D$hM
L$xH!M
L$xH;
L$XH;
L$xH+
L$XH+
L$xH+
L$XH+
L$XH3
L$xH3
L$XH3
L$xH3
L$XH3
L$xH3
L$XH3
L$xH3
L$XH3
L$xH3
L$XH3
L$xH3
L$XH3
L$xH3
L$XH3
L$xH3
L$XH3
L$xH3
L$XH3
L$xH3
L$XH3
L$xH9
L$XHc
l$XHk
l$xI+
l$XIc
l$xL;
l$XL;
l$XL;|$P
l$xL;|$p
L$xMc,
l$xu]M
l$xu^M
l$xu`M
l$xuaM
l$xumM
l$xunM
l$xupM
L)h u
L)t$h
l*n$p,r"p6tVv4xZzZtJ|`~ZtJ
L;@ rOH
L;[h|
L;|$ s
L;|$(
L;|$`
L;|$h
L;|$P
L;|$x
L;|$Xr
L;A H
L;A s
L;A(tVL
L;A(u
L;B r_I
L;B s
L;B saH
L;B sz
L;B(t#L;
L;B(t7L;
L;B(u
L;BPuOD
L;C(t
L;D$(uYH
L;d$@
L;D$`
L;D$0|
L;d$8u
L;d$H
L;D$H|
L;d$hL
L;D$PuLH
L;d$XL
L;G0t
L;I }^H
L;J(t!L;
L;J(u
L;K(t
L;L$(A
L;L$(t
L;L$H|
L;L$P|
L;l$Pt
L;O8r
L;q s
L;R s
L;t$@
L;t$@s
L;t$`
L;t$`t9
L;t$0
L;T$H
L;T$HI
L;t$P
L;t$pH
L;t$X
L;uHt
L;wHt>M+
L;wHu
L;y s
L;y sjH
L;Z s
L;z s
l?4?~?
l?4?~?4?~?4?~?4?~?
l^ntp`r^tJvZxt|4~8
l^ntp`rdtJv`xt|4~8
L+,$L
L+|$P
L+CxH;
L+F0I
L+g L
L+Q H
L+QxL
L+t$HO
L+t$PL
L>2?~>
L>2?~>2?~>2?~>2?~>
L9 spH
L9#uyH
L9%(lc
L9%|Zc
L9%>oc
L9%7nc
L9%'bc
L9%Wac
L9&u4H
L9)~zH
L9)t*H
L9)t?H
L9)t'H
L9:t[H
L9@ s
L9-@dc
L9[h~eH
L9[h~iH
L9-[hc
L9^(H
L9`0H
L9|$@t4
L9|$`t
L9|$0
L9|$8t4
L9|$X
L9} u
L9}8t}
L9-}ic
L9}pu
L9}'t-
L9}'t*
L9>tb
L90u"H
L90uY
L92uQL
L97t{L
L9A s
L9A t
L9A(L
L9a`u
L9B0H
L9b0H
L9C0H
L9cht
L9D$(
L9D$X
L9d$Xu
L9e`u
L9e`u]L
L9e0t1
L9e0uGH
L9e8t-
L9eH@
L9Ewu
L9g uP
L9H }
L9h0H
L9I0H
L9i0L
L9J }
L9j(H
L9kHu
L9L$`
L9l$8w
L9l$X
L9M0u
L9n(L
L9o(v_I
L9O@t
L9P }
L9p s
L9P s
L9Q s
L9qHs
L9t$@t4
L9t$`
L9t$`t5
L9t$0t/
L9t$h
L9t$Ht4
L9t$hupH
L9T$huUD
L9T$p
L9T$purI
L9t$X
L9u@u
L9u0u
L9u7t1
L9upu
L9u't*
L9vHs
L9X s
L9x s
L9x(H
L9X0H
L9y s
L9y8E
L9z0H
L9-Zfc
Label encoder has only one input.
Label encoder has only one output.
LabelEncoder
labels
lambd
largest
largest <= 1
Last dimension of `indices` input tensor in GatherND op must not be larger than the rank of `data` tensor
Last dimension of beta and input does not match
Last dimension of bias and input does not match
Last dimension of gamma and input does not match
last dimension of indices must not be larger and rank of data tensor
last dimension of indices must not be larger than rank of input tensor
last_outputs[j + 1].IsTensor()
last_results.last_loop_red_size > 0
last_results.last_loop_size > 0
last_results.projected_index.size() > 0
Latin
LayerNormalization
LayerNormFusion
LbNrP
Lc@@E
Lc^P3
Lc` H
Lc|$ H
Lc|$0E+
Lc|$4H
Lc} M
Lc~ I;
LcA H
Lca L
LcA<E3
LcC$H
LcC@H
LcD$0E
LcD$DI
Lcd$hI;
LcD$pD
Lce I
LcE(HcU HcM
LcF(I
LcF@I
LcFpI
LcG E
LcG(I
LcG@I
LcG0A
LcGpI
LcGTM
LcGXI
Lch H
Lch L
LcI0L
LcJ A
Lcl$(H
Lcl$DL
LcL$tM
Lcl$x
LCMapStringEx
LcMwI
LcO0HcO(
Lcp,M
Lcr@I
Lcs(I
Lcs@I
Lcs@K
LcspI
LcT$(I
LcT$(M
LcT$@H
Lct$4H
LcT$4LcD$8H
LcT$TE;
Lcu H
Lcv I
Lcx H
lda >= K && ldb >= K && ldc >= N
ldn`ppr
leakyrelu
LeakyRelu
LearningRate
LeaveCriticalSection
left operand cannot broadcast on dim 
Left shape: 
left.NumDimensions() == 2 || left.NumDimensions() == 1
left.Shape().Size() == left_shape_override.Size()
left_dim == right_dim
left_num_dims and right_num_dims must be >= 1
left_rank == right_rank
legacy optimization attribute.
LegalCopyright
len <= op_schema.inputs().size()
len >= 0 && static_cast<uint64_t>(len) < std::numeric_limits<size_t>::max()
length
length > buffer.size()
length of each output
length of each output. Values should be >= 0.
Length of permutation must match the rank of the input to be permutated
length overflow
lengths allocation failed
Lepcha
LessOrEqual
Level
LHN(L
Limbu
limit
limit in Range operator should be scalar like tensor, yet got shape:
linear
Linear
LINEAR
'Linear' mode only support 2-D inputs or 3-D inputs ('Bilinear', 'Trilinear') or 4-D inputs or 5-D inputs with the corresponding outermost 2 scale values being 1 in the 
Linear_A
Linear_B
linear_before_reset
LinearBeforeReset
LinearClassifier
LinearRegressor
list count 
List of 3 elements containing gamma, coef0, and degree, in that order. Zero if unused for the kernel.
List of categories, ints.<br>One and only one of the 'cats_*' attributes must be defined.
List of categories, strings.<br>One and only one of the 'cats_*' attributes must be defined.
list of floats. This attribute stores the weight of each n-gram in pool. The i-th element in weights is the weight of the i-th n-gram in pool. Its length equals to the size of ngram_indexes. By default, weights is an all-one tensor.This attribute is used when mode is "IDF" or "TFIDF" to scale the associated word counts.
List of int64 n-grams learned from the training set. Either this or pool_strings attributes must be present but not both. It's an 1-D tensor starting with the collections of all 1-grams and ending with the collections of n-grams. The i-th element in pool stores the n-gram that should be mapped to coordinate ngram_indexes[i] in the output vector.
list of int64s (type: AttributeProto::INTS). This list is parallel to the specified 'pool_*' attribute. The i-th element in ngram_indexes indicate the coordinate of the i-th n-gram in the output tensor.
List of integers indicate the padding element count at the beginning and end of each axis, for 2D it is the number of pixel. `paddings` rank should be double of the input's rank. `paddings` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded).
List of integers indicating the dimensions to squeeze. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
List of integers indicating the number of padding elements to add or remove (if negative) at the beginning and end of each axis. For 2D it is the number of pixels. `pads` rank should be double of the input's rank. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
List of non-negative integers, indicate the dimensions to be inserted
List of non-negative integers, indicate the dimensions to squeeze.
List of stop words. If not set, no word would be removed from X.
List of strings n-grams learned from the training set. Either this or pool_int64s attributes must be present but not both. It's an 1-D tensor starting with the collections of all 1-grams and ending with the collections of n-grams. The i-th element in pool stores the n-gram that should be mapped to coordinate ngram_indexes[i] in the output vector.
List of tensors for 
list<T> too long
LNN8L P
LNN8L Pi
Load model 
Load model from 
loadedFrom
LoadLibraryExA
LoadNodeArgsFromOrtFormat: Node [
Local\SM0:%d:%d:%hs
locale
LocalSize
localtime_s(&local_tm, &in_time_t) == 0
location
location dimensions do not match shape size
log_prob
LogHr
LOGISTIC
LogSoftmax
long 
LongformerAttention
Loop 'body' subgraph outputs should all be tensors but output 
Loop 'body' subgraph outputs should all be tensors or sequences but output 
Loop 'body' subgraph scan outputs should all be tensors but output 
Loop had zero iterations and the shape of subgraph output 
'Loop' input 'cond' should be a scalar tensor. Got shape of 
'Loop' input 'M' should be a scalar tensor. Got shape of 
'Loop' node has 
loop_body_attribute
loss_N1dd
loss_NCdd
loss_Ndd
loss_sum
loss_unweighted
LOWER
Lower boundary of the output values.
LPN.P,>
LPN.P,>(BPH.J
LpNormalization
LpPool
LSTM operator does not support double yet
ltr^thvrxPzh|d~P
LVNfP^RVTfVdXVZf\
Lycian
Lydian
M A+M0+
M H!E0M
M H1E
m HcE
M I+M
M#_0M
M(H+M H
M(HcS 
M(L9 
M(L9 ukH
M/H+M'H
M;ghH
M;H }
M;H I
M;H(t7L;
M;H(u
M;nXH
M;Q8|
M;YpsvH
M?H;MGs H
M@H9A }
M_ == 1 && N_ == 1 was false
M_ >= 0 && K_ > 0 && N_ >= 0
M`H;Mhs
M+,$I
M+}PH
M+<$H
M+<$I
M+4$I
M7H!EGE3
M8H+M0H
M9&t;
M9/tA
M9}Hs
M9>t3I
M94$t(I
M9E0I
M9H s
M9n(M
M9nXt3I
M9o(I
M9o(t
M9Q8~iI
M9Q8~tI
Mahajani
Main Graph instance should have populated all subgraphs when being resolved.
Makasar
Malayalam
Malformed repeat 
Mandaic
Manichaean
Map is missing type entry for its value
map(int64, double)
map(int64, float)
map(int64, string)
map(string, double)
map(string, float)
map(string, int64)
map/set too long
map_form
map_form_ != PACK_MAP::SPARSE || max_map_ > 0
map_type
MapFileIntoMemory is not implemented on Windows.
Marchen
Masaram_Gondi
Mask data type is not int32 or int64 or float32
Mask is neither unidirectional nor all ones
Mask shape is unknown or not 2D, or data type unknown
mask_index
Mask_Int32
mask_mul const input not matched
mask_sub const input not matched
mask_unsqueeze_1 axes not matched. Expect: 1
mask_unsqueeze_2 axes not matched. Expect: 2
MaskCast
Match contains invalid utf8 chars: 
MatchInputMaskSubgraph returns false
MatchPastSubgraph returns false
MatchUnidirMaskSubgraph returns NULL
MatMul
MatMul dimension mismatch
MatMulAddFusion
MatMulInteger
MatmulInteger : input1 A_scale must be a scalar or 1D tensor of size 1
MatmulInteger : input1 A_zero_point must be a scalar or 1D tensor of size 1 if given
MatmulInteger : input1 B_scale must be a scalar or 1D tensor of size 1
MatmulInteger : input1 B_zero_point must be a scalar or 1D tensor of size 1 if given
MatmulInteger : input1 C_scale must be a scalar or 1D tensor of size 1
MatmulInteger : input1 C_zero_point must be a scalar or 1D tensor of size 1 if given
MatmulInteger : input1 zero point must be a scalar or 1D tensor of size 1
MatmulInteger : input2 zero point must be a scalar or 1D tensor of size 1
MatMulInteger16
MatMulIntegerToFloat
MatMulIntegerToFloat : input A scale must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
MatMulIntegerToFloat : input A zero point must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
MatMulIntegerToFloat : input B scale must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
MatMulIntegerToFloat : input B zero point must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
MatMulIntegerToFloatFusion
MatMulScaleFusion
Matrix dimensions are not equal. Square matrix is expected
Matrix multiply results
Matrix multiply results from A * B
Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html
Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html.
max should be a scalar.
max_gram_length
max_gram_length must be inbounds of ngram_counts: 
max_map
max_map must be > 0 if map_form is SPARSE
max_output_boxes_per_class
max_skip_count
max_skip_count is required
max_skip_count must be non-negative: 
max->Shape().NumDimensions() == 0
Maximum n-gram length. If this value is 3, 3-grams will be used to generate the output.
Maximum number of events reached, could not record profile event.
Maximum number of items (integers/strings) to be skipped when constructing an n-gram from X. If max_skip_count=1, min_gram_length=2, max_gram_length=3, this operator may generate 2-grams with skip_count=0 and skip_count=1, and 3-grams with skip_count=0 and skip_count=1
Maximum value, above which element is replaced by max
MaximumSamplesPerOutput
MaxPool
MaxpoolWithMask
MaxRoiPool
MaxUnpool
MaxUnpool op must have either two or three inputs.
Mc@ I
Mc`(E3
Mc} H
Mc~ D;
Mc~ H
Mce D;
Mce H
McE I
Mcf M
Mcf@M
McF@M
Mcf`M
McFpH
McFTL
Mcg H
Mcn(M
Mcn8K
Mcu D
McU N
Mcv H
Mcw H
McWPL
ME9gX
MeanTensor
MeanVarianceNormalization
Medefaidrin
Meetei_Mayek
Mem pattern for initializer 
Mem pattern should be disabled when using DML execution provider.
mem_steps <= max_memory_steps_ && mem_steps > 0
Memcpy
MemcpyFromHost
MemcpyToHost
MemcpyTransformer
Memory pattern planner is not enabled on this execution framework.
Memory type 
memory_seq_lens
Mende_Kikakui
Meroitic_Cursive
Meroitic_Hieroglyphs
message size
metadef_id_generator_
Method IncrementIndexAndComputeOffset assumes this value is strictly positive.
metric
MGHcG I;
mH+)I
MHD;B
MhH!Ex
MHH;MPt2L;
MHL9u
Microsoft
Microsoft Corporation
Microsoft.ML.ONNXRuntime
MIGraphXExecutionProvider
min should be a scalar.
min_ <= max_
min_gram_length
min_gram_length >= max_gram_length required: 
min_gram_length is required
min_gram_length must be inbounds of ngram_counts: 
Min_Scaled
min->Shape().NumDimensions() == 0
mincharnum
mincharnum is too big for char level tokenezation
mincharnum_ > 0
Minimum n-gram length. If this value is 2 and max_gram_length is 3, output may contain counts of 2-grams and 3-grams.
Minimum number of characters allowed in the output. For example, if mincharnum is 2, tokens such as "A" and "B" would be ignored
Minimum value, under which element is replaced by min
MinimumSamplesPerOutput
Mismatch between expected shape and shape from first output
Mismatch between Graph and IndexedSubGraph. Input not found:
Mismatch between Graph and IndexedSubGraph. Node not found: 
Mismatch between Graph and IndexedSubGraph. Output not found:
Mismatch between input data and B: size of B != input channel count 
Mismatch between input data and scale: size of scale != input channel count 
Mismatch between number of source and target dimensions. Source=
Mismatch between number of splits (
Mismatch between the sum of 'split' (
Mismatched attribute type in '
Mismatched data types between input and output Tensors. 
Mismatched tensor element type for output 
Mismatched tensor element type:
Mismatched type for output 
Mismatched type:
missing )
missing ]
Missing case in Compiler: 
Missing dimensions for initializer. Invalid ORT format model.
Missing dims for sparse initializer: 
Missing 'equation' attribute
Missing indicies for sparse initializer: 
Missing Input: 
Missing Model. Invalid ORT format model.
Missing name for SparseTensor initializer. Invalid ORT format model.
Missing opset in the model. All ModelProtos MUST have at least one entry that specifies which version of the ONNX OperatorSet is being imported.
Missing or invalid starts and ends attribute
Missing raw data for initializer. Invalid ORT format model.
Missing session state for subgraph. Node:'
Missing string data for initializer. Invalid ORT format model.
Missing values for sparse initializer. Invalid ORT format model.
Missing/Invalid 'axes' attribute value
Missing/Invalid 'axis' attribute value
Misuse of LoopStateVariable. Attempt to move beyond end of sequence
ml_type != nullptr
MLDataType for: 
mlvalue.Fence() == nullptr
mode attribute is 
mode is required
mode: 
Model file not found!
model format error!
model format error! Missing 'location'
model format error! Need a key for the external data info
model format error! Need a value for the external data info
Model must have opset imports. Invalid ORT format model.
Model was not loaded
Model was not loaded.
MODEL_LOADED
model_loading_array
model_loading_from_saved_proto
model_loading_proto
model_loading_uri
model_path must not be empty. Ensure that a path is provided when the model is created or loaded.
model_run
modelDomain
modelGraphName
modelMetaData
modelProducerName
modelProducerVersion
ModelProto corresponding to the model to be loaded has already been parsed. Invoke Load().
ModelProto corresponding to the model to be loaded has not been parsed yet. This API should be called in conjunction with a ctor that takes a model abstraction.
ModelProto does not have a graph.
ModelProto needs to be parsed to check for ORT config within it
momentum
Mongolian
Move it out of graph inputs if there is no need to override it, 
MPH!]@L
MPH!E`H
mPL;U`
Msg:[%ws] 
mul_B_tensor_proto
mul_inputs.size() == 2
MulInteger
Multani
multi_class
MultiByteToWideChar
Multinomial
Multiple errors were found.
multiplication
Multiplicative spatial scale factor to translate ROI coordinates from their input scale to the scale used when pooling.
Multiplicative spatial scale factor to translate ROI coordinates from their input spatial scale to the scale used when pooling, i.e., spatial scale of the input feature map X relative to the input image. E.g.; default is 1.0f. 
MurmurHash3
Must be a scalar or 1D tensor or size 1.
Must have 1 or more inputs
Must have a single dimension
Must have a single dimension of 1
Must have a valid data type
Must have a valid input shape.
Must have valid 'axis' attribute
Must provide classlabels_strings or classlabels_int64s but not both.
Must provide imputed_values_float_ or imputed_values_int64_ but not both.
Must use Function based fusion when exporting compiled nodes to dll.
mutually equal shape is specified by the argument "axis", and if it is not set,
MxH+MpH
MXI;w
Myanmar
n >= 0
n >= 0 && static_cast<size_t>(n) < ort_value_info_.size()
n >= 0 && static_cast<size_t>(n) < plan_.allocation_plan.size()
N 9H0
N A+N0+
n H"( r
N H9Z
N HcF
N IcF
n j$4"l$V"T
N$9H0
N(A+N0+
n(D+n0D
N*L~@
N:0^E
n_supports
n_targets
n_targets_or_classes > 0
N`X2Z
N0I;N8t=H
N8E8~XtSI
N8H+N0H
Nabataean
name:
naxes > 0
NBP(N
NchwcTransformer
N-dimensional matrix A
N-dimensional matrix B
nearest
NEAREST
nearest_mode
nearest_mode:[
Negative index values are not permitted. First entry in map has index value of 
Negative ngram_indexes values are not allowed
Negative values are not allowed in a shape specification
NegativeLogLikelihoodLoss
Nested parallelism not supported
network down
network reset
network unreachable
New shape
new_axis
new_axis must be either 0 or 1
New_Tai_Lue
n-gram counts out of bounds for 
ngram_counts
ngram_indexes
ngram_indexes must be non-empty with no negative values
NHH;NPu
nHp(n
nHp(nVl
NHP(R
NHP(RhNhPHR(PzNnX4\
nhwc_permutated_pads
NhwcMaxPool
NhwcTransformer
njob_ = 
NnapiExecutionProvider
nNp8n r
no argument for repetition operator
No attribute with name:'
No attribute with name: 
No attribute with this name is defined.
no buffer space
no child process
no error
No Graph instance was found for attribute 
No graph was found in the protobuf.
No kernel shape is set.
no link
no lock available
No matching 'start' entry.
no message
no message available
No Op registered for 
No opset import for domain '
no protocol option
No provider specified.
No ranges in char class
No requested allocator available
no space on device
no stream resources
no such device
no such device or address
no such file or directory
no such process
NO_MODEL
NO_SUCHFILE
Node 
Node (
Node [
Node id for each node. Ids may restart at zero for each tree, but it not required to.
Node id for each node. Node ids must restart at zero for each tree and increase sequentially.
node id that this weight is for.
Node index is out of range
Node is missing. Invalid ORT format model.
Node must only have one used output
Node placements
node.MutableOutputDefs().size() == subgraph.GetOutputs().size()
Node:
Node::LoadEdgesFromOrtFormat, edge is missing for 
Node::LoadFromOrtFormat, input_arg_counts is missing
node_arg
node_arg_name cannot be null
node_index < nodes_.size()
node_index_info and ort_value_idx_map are out of sync and cannot be used
node_index_info_
node_index_info_.GetMaxMLValueIdx() == ort_value_idx_map.MaxIdx()
node_offsets_index < node_offsets_size_
node->GetOutputEdgesCount() == 0
nodearg
NodeArg is missing. Invalid ORT format model.
NodeArg Name is missing. Invalid ORT format model.
NodeEdge is missing. Invalid ORT format model.
NodeProto (name: 
Nodes in a graph must be topologically sorted, however input '
nodes_.size() < static_cast<unsigned int>(std::numeric_limits<int>::max())
nodes_falsenodeids
nodes_falsenodeids.size() == nodes_featureids.size()
nodes_falsenodeids.size() == nodes_modes.size()
nodes_falsenodeids.size() == nodes_nodeids.size()
nodes_falsenodeids.size() == nodes_treeids.size()
nodes_falsenodeids.size() == nodes_truenodeids.size()
nodes_falsenodeids.size() == nodes_values.size()
nodes_featureids
nodes_hitrates
nodes_missing_value_tracks_true
nodes_modes
nodes_nodeids
nodes_treeids
nodes_truenodeids
nodes_values
Non concat axis dimensions must match: Axis 
Non per-tensor quantization is not supported now.
non_tensor_base != nullptr
Non-empty ngram_counts is required
Non-empty ngram_indexes is required
non-empty pool_int64s is required if pool_strings not provided
NonMaxSuppression
NonZero
Non-zero status code returned while running 
noop_with_empty_axes
normalize_variance
normalized
Normalizer
NormalizeVariance
not a directory
not a socket
not a stream
Not all dimensions to be reduced have been reduced in the candidate output. Candidate output dims: 
not connected
Not eliminating output 
Not enough elements in dilations. Expected: 
Not enough elements in kernel shape. Expected: 
Not enough elements in pads. Expected: 
Not enough elements in strides. Expected: 
not enough memory
not enough space: expected 
Not implemented
not implemented
not support normalize yet.
Not supported
not supported
Not supported with filtered graph.
NOT_IMPLEMENTED
NOT_SET
NOTSET
NPH+NHH
NPH9NHt
Nr^``pbtd`fTh6lQ
ntelA
nteltd
Null batch_indices_ptr
Null crop_size_ptr
Null entry in dimensions. Invalid ORT format model.
Null floats attribute. Invalid ORT format model.
Null graph attribute. Invalid ORT format model.
Null input ptr
Null input X ptr
Null ints attribute. Invalid ORT format model.
null literal
Null map type info. Invalid ORT format model.
Null rois_ptr
Null sequence type info. Invalid ORT format model.
NULL state in RunStateOnByte
Null string attribute. Invalid ORT format model.
Null string in strings attribute. Invalid ORT format model.
Null strings attribute. Invalid ORT format model.
Null tensor attribute. Invalid ORT format model.
Null tensor in tensors attribute. Invalid ORT format model.
Null tensor type info. Invalid ORT format model.
Null tensors attribute. Invalid ORT format model.
Null type info for 
Null value type info in fbs::MapType. Invalid ORT format model.
Null value type info in fbs::SequenceType. Invalid ORT format model.
nullptr != func_meta_def
nullptr != p.output_tensor
nullptr != tensor_type_base
nullptr != type_proto
nullptr == p_data
num_axes > 0
num_broadcasted_indices < num_of_ellipsis_dims_
num_categories_ > 0
num_classes is < 1
num_dims_with_pad - 1 != num_output_dims
num_dims_with_pad - 2 != num_output_dims
num_dims_with_pad != num_output_dims
num_entries == int_categories.size()
num_explicit_inputs == static_cast<size_t>(target_input_idx)
num_features == feature_count_
num_heads
num_inputs >= 1
num_keys == num_values
num_samples is < 1
num_scan_inputs
num_subgraph_outputs - 1 == num_outputs
num_subgraph_outputs == static_cast<size_t>(num_outputs)
num_variadic_inputs == num_subgraph_inputs
number
number literal
Number of attention heads
Number of dimensions for batch indices should be exactly 1
Number of dimensions for crop size should be exactly 1
Number of dimensions for rois should be exactly 
number of elements in this dimension), it represents `n`. For slicing to the
Number of elements of attribute 'scales' must be same as rank of input 'X'
Number of elements of input 'scales' must be same as rank of input 'X'
Number of elements of input 'sizes' must be same as rank of input 'X'
Number of entries in '
Number of entries in 'scan_input_axes' was 
Number of entries in 'scan_output_axes' was 
number of groups input channels and output channels are divided into.
number of groups input channels and output channels are divided into. default is 1.
Number of input tensors does not match the operands in the equation.
Number of inputs (
Number of items must compose whole 
Number of neurons in the hidden layer
Number of neurons in the hidden layer.
Number of sampling points in the interpolation grid used to compute the output value of each pooled output bin. If > 0, then exactly sampling_ratio x sampling_ratio grid points are used. If == 0, then an adaptive number of grid points are used (computed as ceil(roi_width / output_width), and likewise for height). Default is 0.
Number of scan input axes specified (
Number of scan output axes specified (
Number of subscripts in the input equation does not match number of input tensors
Number of times to sample.
Number of top elements to retrieve
Number of values should be at least 1.
number overflow parsing '
NumCapturesWalker::ShortVisit called
NupharExecutionProvider
Nushu
NXH;i
NXI;NXu
O 9K t
O A+O0+
O D9q
O H;G0w
O H;O 
O H+O
O H91u
O I+O
O(H+O H
o\$PH
o|$0J
o=G>7
O0HcG(L
O0LcQ
object
object key
object separator
oD$ f
of [N, 0] then [N, 0].
Offset
offset
offset % span_size_ == 0
offset + size <= size_t(span.size())
offset < 0
offset >= 0 && static_cast<size_t>(offset) < node_values_size_
Offsets
offsets buffer is not equal to tensor size
Ogham
OhH;Opt
OHH+O@L
OhL;O
oL$0f
Ol_Chiki
Old_Hungarian
Old_Italic
Old_North_Arabian
Old_Permic
Old_Persian
Old_Sogdian
Old_South_Arabian
Old_Turkic
OLEAUT32.dll
One (or two if bidirectional) activation function for input gate. The activation function must be one of the activation functions specified above. Optional: Default `Tanh` if not specified.
One and only one of the attributes 'value', 'value_*' or 'sparse_value' must be specified for a Constant node.
One and only one of the 'cats_*' attributes must be defined
One falsenode is pointing either to itself, either to another tree.
One float, indicates the value to be filled, default is 0
One float, indicates the value to be filled.
One of 'MAX,' 'L1,' 'L2'
One of the attributes 'value' or 'sparse_value' must be specified for a Constant node.
One sided attention windows length W, or half of total window length
one_class
OneHot
OneHot node must have three inputs.
OneHotEncoder
onesided
Only 1 batch dimension is allowed for MatMul
Only bool
Only CPU devices are supported for now.
Only one instance of LoggingManager created with InstanceType::Default can exist at any point in time.
Only one node should produce an output. Existing entry for 
Only one of keys_*'s can be set in label encoder.
Only one of scales or sizes must be provided as input.
Only one of the attributes 'value' or 'sparse_value' must be specified for a Constant node.
Only one of values_*'s can be set in label encoder.
Only one thread was configured for parallel execution. Hence will use sequential execution.
Only ONNX MLDataType can be registered
Only supports `int32_t` or `int64_t` inputs for split
Only supports `int32_t` or `int64_t` inputs for starts/ends/axes/steps
Only tensors or sequence of tensors are suppported
ONNX model does not support multiple opset versions for a domain. Model imports opset version 
ONNX models don't support multiple opset version imports for a domain. Function 
ONNX Runtime
ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 
ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain 'ai.onnx'. Please upgrade your model to opset 7 or higher. For now, this opset 
ONNX Schema 
onnx.AttributeProto
onnx.FunctionProto
onnx.GraphProto
onnx.ModelProto
onnx.NodeProto
onnx.OperatorSetIdProto
onnx.SparseTensorProto
onnx.StringStringEntryProto
onnx.TensorAnnotation
onnx.TensorProto
onnx.TensorProto.Segment
onnx.TensorShapeProto
onnx.TensorShapeProto.Dimension
onnx.TrainingInfoProto
onnx.TypeProto
onnx.TypeProto.Map
onnx.TypeProto.Opaque
onnx.TypeProto.Sequence
onnx.TypeProto.SparseTensor
onnx.TypeProto.Tensor
onnx.ValueInfoProto
ONNX_NAMESPACE::TensorProto::DataType_IsValid(dtype_) && dtype_ != ONNX_NAMESPACE::TensorProto::UNDEFINED
ONNX_NAMESPACE::TensorProto::DataType_IsValid(output_dtype_) && output_dtype_ != ONNX_NAMESPACE::TensorProto::UNDEFINED
ONNX_NAMESPACE::TensorProto::DataType_IsValid(t_proto.data_type())
onnxruntime
onnxruntime.dll
onnxruntime.pdb
onnxruntime::`anonymous-namespace'::Cast::Cast
onnxruntime::`anonymous-namespace'::CastToString
onnxruntime::`anonymous-namespace'::ConstantOfShape::Compute
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<__int64>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<double>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<float>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<int>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<struct onnxruntime::BFloat16>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<struct onnxruntime::MLFloat16>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<unsigned __int64>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<unsigned int>::operator ()
onnxruntime::`anonymous-namespace'::GetClipConstantMinMax::<lambda_a019ee9e596263da7ea81c5951dc580e>::operator ()
onnxruntime::`anonymous-namespace'::GetCurrentTimeString
onnxruntime::`anonymous-namespace'::GetInputNodeMerges
onnxruntime::`anonymous-namespace'::GetIntermediateMLFloat16ToFloatTensor
onnxruntime::`anonymous-namespace'::GetOutputNodeMerges
onnxruntime::`anonymous-namespace'::GetRatioOrDefault
onnxruntime::`anonymous-namespace'::GetScalarConstantInitializer
onnxruntime::`anonymous-namespace'::GetScaleFromNode
onnxruntime::`anonymous-namespace'::ParsePathRoot
onnxruntime::`anonymous-namespace'::TraverseFormalParametersWithTypeProto
onnxruntime::`anonymous-namespace'::WindowsEnv::DeleteFolder
onnxruntime::`anonymous-namespace'::WindowsEnv::FormatLibraryFileName
onnxruntime::`anonymous-namespace'::WindowsEnv::GetCanonicalPath
onnxruntime::`anonymous-namespace'::WindowsEnv::GetNumCpuCores
onnxruntime::`anonymous-namespace'::WindowsEnv::ReadFileIntoBuffer
onnxruntime::AllocateSparseTensor
onnxruntime::AllocatorManager::InsertAllocator
onnxruntime::AllocPlanPerValue::ProgramCounter::AddEnd
onnxruntime::AllocPlanPerValue::ProgramCounter::AddStart
onnxruntime::AttentionFusion::ApplyImpl
onnxruntime::AttentionFusion::FuseSubGraph
onnxruntime::AttentionFusionHelper::CheckDistilBertReshapeShape
onnxruntime::AttentionFusionHelper::CheckNodesInPathK
onnxruntime::AttentionFusionHelper::CheckNodesInPathQ
onnxruntime::AttentionFusionHelper::CheckNodesInPathV
onnxruntime::AttentionFusionHelper::CheckSliceParameters
onnxruntime::AttentionFusionHelper::FuseGptAttention
onnxruntime::AttentionFusionHelper::MatchGemmSubgraph
onnxruntime::AttentionFusionHelper::MatchInputMaskSubgraph
onnxruntime::AttentionFusionHelper::MatchPastSubgraph
onnxruntime::AttentionFusionHelper::MatchUnidirMaskSubgraph
onnxruntime::AttentionFusionHelper::ValidateGemmInitializer
onnxruntime::AttentionFusionHelper::ValidateUnidirMask
onnxruntime::BatchNorm<double>::BatchNorm
onnxruntime::BatchNorm<double>::Compute
onnxruntime::BatchNorm<float>::BatchNorm
onnxruntime::BatchNorm<float>::Compute
onnxruntime::BFCArena::AllocateRawInternal
onnxruntime::BFCArena::AllocationRegion::AllocationRegion
onnxruntime::BFCArena::AllocationRegion::IndexFor
onnxruntime::BFCArena::BFCArena
onnxruntime::BFCArena::ChunkFromHandle
onnxruntime::BFCArena::DeallocateRawInternal
onnxruntime::BFCArena::Extend
onnxruntime::BFCArena::Extend::<lambda_0b6f6e94b9ace871745a369732fe3ba7>::operator ()
onnxruntime::BFCArena::FindChunkPtr
onnxruntime::BFCArena::FreeAndMaybeCoalesce
onnxruntime::BFCArena::InsertFreeChunkIntoBin
onnxruntime::BFCArena::Merge
onnxruntime::BFCArena::RegionManager::RegionFor
onnxruntime::BFCArena::RemoveFreeChunkFromBin
onnxruntime::BFCArena::RemoveFreeChunkIterFromBin
onnxruntime::BFCArena::Reserve
onnxruntime::BFCArena::SplitChunk
onnxruntime::BiasGeluFusion::ApplyImpl
onnxruntime::BiasSoftmaxFusion::ApplyImpl
onnxruntime::BitShift<unsigned __int64>::BitShift
onnxruntime::BitShift<unsigned __int64>::Compute::<lambda_e00ccdfdad5b99e85d66c8cdcbf3ecd6>::operator ()
onnxruntime::BitShift<unsigned char>::BitShift
onnxruntime::BitShift<unsigned char>::Compute::<lambda_91d3ab33d6c7496b510fc503102ea468>::operator ()
onnxruntime::BitShift<unsigned int>::BitShift
onnxruntime::BitShift<unsigned int>::Compute::<lambda_11fb99886600c0969d3276473b32067e>::operator ()
onnxruntime::Broadcaster::Broadcaster
onnxruntime::BroadcastIterator::Append
onnxruntime::BroadcastIterator::Init
onnxruntime::BroadcastLooper
onnxruntime::CheckInput
onnxruntime::Clip::ComputeImpl<__int64>::operator ()
onnxruntime::Clip::ComputeImpl<double>::operator ()
onnxruntime::Clip::ComputeImpl<float>::operator ()
onnxruntime::Clip::ComputeImpl<signed char>::operator ()
onnxruntime::Clip::ComputeImpl<unsigned __int64>::operator ()
onnxruntime::Clip::ComputeImpl<unsigned char>::operator ()
onnxruntime::clip_internal::Clip_6Base<float>::Clip_6Base
onnxruntime::common::Status::Status
onnxruntime::CommonReduce
onnxruntime::CommonSubexpressionElimination::ApplyImpl
onnxruntime::Compress::Compute
onnxruntime::ComputePadAndOutputShape
onnxruntime::ConcatBase::ConcatBase
onnxruntime::ConcatBase::PrepareForCompute
onnxruntime::ConcatFromSequence::Compute
onnxruntime::concurrency::ThreadPool::ParallelFor
onnxruntime::concurrency::ThreadPool::ParallelSection::ParallelSection
onnxruntime::ConstantFolding::ApplyImpl
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<struct onnxruntime::MLFloat16,float,double,signed char,short,int,__int64,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::ConstantOfShapeBase
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<struct onnxruntime::MLFloat16,float,double,signed char,short,int,__int64,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::PrepareCompute
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<struct onnxruntime::MLFloat16,float,double,signed char,short,int,__int64,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::SetValue
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<struct onnxruntime::MLFloat16,float,double,signed char,short,int,__int64,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::SetValueFromTensorProto
onnxruntime::ConstPointerContainer<class std::vector<class onnxruntime::NodeArg *,class std::allocator<class onnxruntime::NodeArg *> > >::at
onnxruntime::contrib::`anonymous-namespace'::QLinearImpl
onnxruntime::contrib::Affine<float>::Affine
onnxruntime::contrib::Attention<float>::Compute
onnxruntime::contrib::AttentionBase::AttentionBase
onnxruntime::contrib::AttentionBase::GetPresent
onnxruntime::contrib::AttentionCPUBase::ApplyAttention
onnxruntime::contrib::BahdanauAttention<float>::BahdanauAttention
onnxruntime::contrib::BahdanauAttention<float>::PrepareMemory
onnxruntime::contrib::BiasGelu<float,0>::Compute
onnxruntime::contrib::BiasGelu<float,1>::Compute
onnxruntime::contrib::CDist<double>::CDist
onnxruntime::contrib::CDist<float>::CDist
onnxruntime::contrib::Crop<float>::Compute
onnxruntime::contrib::CropAndResize<float>::CropAndResize
onnxruntime::contrib::DeepCpuAttnLstmOp::Compute
onnxruntime::contrib::DeepCpuAttnLstmOp::ComputeImpl
onnxruntime::contrib::DeepCpuAttnLstmOp::DeepCpuAttnLstmOp
onnxruntime::contrib::DeepCpuAttnLstmOp::ValidateInputs
onnxruntime::contrib::DynamicQuantizeMatMul::Compute
onnxruntime::contrib::EmbedLayerNorm<float>::Compute
onnxruntime::contrib::EmbedLayerNorm<float>::EmbedLayerNorm
onnxruntime::contrib::ExpandDims::Compute
onnxruntime::contrib::FusedConvFloat::FusedConvFloat
onnxruntime::contrib::FusedGemm<float>::FusedGemm
onnxruntime::contrib::ImageScaler<float>::ImageScaler
onnxruntime::contrib::LayerNorm<double,0>::Compute
onnxruntime::contrib::LayerNorm<double,0>::LayerNorm
onnxruntime::contrib::LayerNorm<double,1>::Compute
onnxruntime::contrib::LayerNorm<double,1>::LayerNorm
onnxruntime::contrib::LayerNorm<float,0>::Compute
onnxruntime::contrib::LayerNorm<float,0>::LayerNorm
onnxruntime::contrib::LayerNorm<float,1>::Compute
onnxruntime::contrib::LayerNorm<float,1>::LayerNorm
onnxruntime::contrib::MatMulInteger16<short,short,int>::Compute
onnxruntime::contrib::MatMulIntegerToFloat::Compute
onnxruntime::contrib::MatMulIntegerToFloatBase::ComputeCommon
onnxruntime::contrib::MaxpoolWithMask::Compute
onnxruntime::contrib::MurmurHash3::Compute
onnxruntime::contrib::NchwcConv::Compute
onnxruntime::contrib::NchwcConv::NchwcConv
onnxruntime::contrib::NchwcPoolBase::NchwcPool
onnxruntime::contrib::NchwcPoolBase::NchwcPoolBase
onnxruntime::contrib::NchwcUpsample::Compute
onnxruntime::contrib::NchwcUpsample::NchwcUpsample
onnxruntime::contrib::NhwcMaxPool::Compute
onnxruntime::contrib::QAttention<float>::Compute
onnxruntime::contrib::QlinearBuildLookupTable
onnxruntime::contrib::QLinearGlobalAveragePool::Compute
onnxruntime::contrib::RegisterCpuContribKernels
onnxruntime::contrib::RegisterNchwcKernels
onnxruntime::contrib::RegisterQuantizationKernels
onnxruntime::contrib::ReorderInput::Compute
onnxruntime::contrib::ReorderOutput::Compute
onnxruntime::contrib::ReorderOutput::ReorderOutput
onnxruntime::contrib::Scale<float>::Scale
onnxruntime::contrib::SkipLayerNorm<double>::SkipLayerNorm
onnxruntime::contrib::SkipLayerNorm<float>::SkipLayerNorm
onnxruntime::contrib::Tokenizer::Tokenizer
onnxruntime::contrib::Trilu::Compute
onnxruntime::contrib::Trilu::Trilu
onnxruntime::contrib::WordConvEmbedding::Compute
onnxruntime::Conv<float>::Compute
onnxruntime::ConvActivationFusion::ApplyImpl
onnxruntime::ConvAddFusion::Apply
onnxruntime::ConvAttributes::ConvAttributes
onnxruntime::ConvAttributes::InferOutputShape
onnxruntime::ConvBNFusion::Apply
onnxruntime::ConvertMaskToInt32
onnxruntime::ConvInteger::Compute
onnxruntime::ConvMulFusion::Apply
onnxruntime::ConvTranspose<float>::DoConvTranspose
onnxruntime::ConvTransposeAttributes::ComputePadsAndOutputShape
onnxruntime::ConvTransposeAttributes::ComputeTransposePadAndOutputShape
onnxruntime::ConvTransposeAttributes::PrepareForCompute
onnxruntime::core_impl::<lambda_8091d3521700e7dceb5661842990062f>::operator ()
onnxruntime::core_impl::<lambda_b566ac763982c43c14b91bb93c010ccf>::operator ()
onnxruntime::core_impl::<lambda_dadaf9d3b277ad7eb0671685783b45e2>::operator ()
onnxruntime::core_impl::<lambda_e46ce45d0b65f683023369b5522f0aa5>::operator ()
onnxruntime::CPUDataTransfer::CopyTensor
onnxruntime::CPUExecutionProvider::GetKernelRegistry
onnxruntime::CreateAllocator
onnxruntime::CreateCopyAndAppendCpuTensor
onnxruntime::CreateCustomRegistry
onnxruntime::CreateOpsetImportsForFunction
onnxruntime::CreateSchema
onnxruntime::CumSum<__int64>::Compute
onnxruntime::CumSum<double>::Compute
onnxruntime::CumSum<float>::Compute
onnxruntime::CumSum<int>::Compute
onnxruntime::CustomOpKernel::CustomOpKernel
onnxruntime::data_types_internal::DataTypeRegistry::RegisterDataType
onnxruntime::data_types_internal::IsCompatible
onnxruntime::data_types_internal::SetMapTypes<__int64,__int64>::Set
onnxruntime::data_types_internal::SetMapTypes<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::Set
onnxruntime::data_types_internal::SetMapTypes<__int64,double>::Set
onnxruntime::data_types_internal::SetMapTypes<__int64,float>::Set
onnxruntime::data_types_internal::SetMapTypes<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::Set
onnxruntime::data_types_internal::SetMapTypes<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::Set
onnxruntime::data_types_internal::SetMapTypes<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,double>::Set
onnxruntime::data_types_internal::SetMapTypes<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float>::Set
onnxruntime::data_types_internal::SetSequenceType<__int64>::Set
onnxruntime::data_types_internal::SetSequenceType<bool>::Set
onnxruntime::data_types_internal::SetSequenceType<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::Set
onnxruntime::data_types_internal::SetSequenceType<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > > >::Set
onnxruntime::data_types_internal::SetSequenceType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > > >::Set
onnxruntime::data_types_internal::SetSequenceType<double>::Set
onnxruntime::data_types_internal::SetSequenceType<float>::Set
onnxruntime::data_types_internal::SetSequenceType<int>::Set
onnxruntime::data_types_internal::SetSequenceType<short>::Set
onnxruntime::data_types_internal::SetSequenceType<signed char>::Set
onnxruntime::data_types_internal::SetSequenceType<struct onnxruntime::BFloat16>::Set
onnxruntime::data_types_internal::SetSequenceType<struct onnxruntime::MLFloat16>::Set
onnxruntime::data_types_internal::SetSequenceType<unsigned __int64>::Set
onnxruntime::data_types_internal::SetSequenceType<unsigned char>::Set
onnxruntime::data_types_internal::SetSequenceType<unsigned int>::Set
onnxruntime::data_types_internal::SetSequenceType<unsigned short>::Set
onnxruntime::DataTransferManager::CopyTensors
onnxruntime::DataTypeImpl::GetType<T>() == type_
onnxruntime::DeepCpuGruOp::Compute
onnxruntime::DeepCpuGruOp::ComputeImpl
onnxruntime::DeepCpuGruOp::DeepCpuGruOp
onnxruntime::DeepCpuLstmOp::Compute
onnxruntime::DepthToSpace<float>::Compute
onnxruntime::DepthToSpace<float>::DepthToSpace
onnxruntime::DequantizeLinear<int>::Compute
onnxruntime::Det<float>::Compute
onnxruntime::DoTransposeEltWise
onnxruntime::DoTransposeImpl
onnxruntime::Dropout<double,double>::Compute
onnxruntime::Dropout<double,float>::Compute
onnxruntime::Dropout<float,double>::Compute
onnxruntime::Dropout<float,float>::Compute
onnxruntime::DynamicQuantizeLinear<unsigned char>::Compute
onnxruntime::DynamicQuantizeMatMulFusion::ApplyImpl
onnxruntime::Einsum::DeviceCompute
onnxruntime::Einsum::Einsum
onnxruntime::EinsumComputePreprocessor::PostProcessBroadcastedDims
onnxruntime::EinsumComputePreprocessor::Run
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::DataCopy
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::Diagonal
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::DiagonalInnermostDims
onnxruntime::EinsumOp::IsTransposeRequired
onnxruntime::EinsumOp::MatMul
onnxruntime::EinsumOp::Transpose
onnxruntime::EinsumTypedComputeProcessor<__int64>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<__int64>::PairwiseOperandProcess
onnxruntime::EinsumTypedComputeProcessor<double>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<double>::PairwiseOperandProcess
onnxruntime::EinsumTypedComputeProcessor<float>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<float>::PairwiseOperandProcess
onnxruntime::EinsumTypedComputeProcessor<int>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<int>::PairwiseOperandProcess
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<__int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<__int64> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<int> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<short> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<short> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<signed char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<signed char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned __int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned __int64> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned int> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned short> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned short> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Ceil<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Ceil<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Elu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Elu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Floor<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Floor<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::HardSigmoid<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::HardSigmoid<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::LeakyRelu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::LeakyRelu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<__int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<__int64> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<int> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<signed char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<signed char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ParametricSoftplus<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ParametricSoftplus<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ScaledTanh<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ScaledTanh<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Selu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Selu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sigmoid<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sigmoid<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Softplus<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Softsign<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Tanh<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Tanh<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ThresholdedRelu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ThresholdedRelu<float> >::ElementWiseKernel
onnxruntime::EmbedLayerNormFusion::ApplyImpl
onnxruntime::ExecutionFrame::{ctor}::<lambda_824046d022ae5e6c84964671b22cc5ee>::operator ()
onnxruntime::ExecutionFrame::AllocateAsPerAllocationPlan
onnxruntime::ExecutionFrame::AllocateMLValueTensorPreAllocateBuffer
onnxruntime::ExecutionFrame::AllocateMLValueTensorSelfOwnBufferHelper
onnxruntime::ExecutionFrame::ExecutionFrame
onnxruntime::ExecutionFrame::GetAllocationPlan
onnxruntime::ExecutionFrame::ReleaseMLValueImpl
onnxruntime::ExecutionFrame::TraceAllocate
onnxruntime::ExecutionFrame::TraceFree
onnxruntime::ExecutionProviders::Add
onnxruntime::ExLibLoader::{dtor}::<lambda_373253f8ea5b3b2c86014ec843a05d77>::operator ()
onnxruntime::ExLibLoader::~ExLibLoader
onnxruntime::ExLibLoader::LoadExternalLib
onnxruntime::Expand_8<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::Compute::<lambda_5529023aebb045f88423212810b9b883>::operator ()
onnxruntime::ExpandBroadcastLooper
onnxruntime::experimental::utils::LoadAttributeOrtFormat
onnxruntime::experimental::utils::LoadInitializerOrtFormat
onnxruntime::experimental::utils::LoadMapTypeOrtFormat
onnxruntime::experimental::utils::LoadOpsetImportOrtFormat
onnxruntime::experimental::utils::LoadSequenceTypeOrtFormat
onnxruntime::experimental::utils::LoadSparseInitializerOrtFormat
onnxruntime::experimental::utils::LoadTensorDimensionOrtFormat
onnxruntime::experimental::utils::LoadTensorShapeOrtFormat
onnxruntime::experimental::utils::LoadTensorTypeAndShapeOrtFormat
onnxruntime::experimental::utils::LoadTypeInfoOrtFormat
onnxruntime::experimental::utils::LoadValueInfoOrtFormat
onnxruntime::experimental::utils::SaveAttributeOrtFormat
onnxruntime::experimental::utils::SaveInitializerOrtFormat
onnxruntime::experimental::utils::SaveMapTypeOrtFormat
onnxruntime::experimental::utils::SaveSequenceTypeOrtFormat
onnxruntime::experimental::utils::SaveSparseInitializerOrtFormat
onnxruntime::experimental::utils::SaveTensorTypeAndShapeOrtFormat
onnxruntime::experimental::utils::SaveTypeInfoOrtFormat
onnxruntime::experimental::utils::SaveValueInfoOrtFormat
onnxruntime::EyeLike::Compute
onnxruntime::FastGeluFusion::ApplyImpl
onnxruntime::FeedsFetchesInfo::FeedsFetchesInfo
onnxruntime::FeedsFetchesInfo::MapNamesToMLValueIdxs
onnxruntime::FeedsFetchesManager::SetDeviceCopyChecks
onnxruntime::FinalizeSessionOptions
onnxruntime::Flatten::Compute
onnxruntime::Flatten::Flatten
onnxruntime::FreeDimensionOverrideTransformer::ApplyImpl
onnxruntime::FreeDimensionOverrideTransformer::FreeDimensionOverrideTransformer
onnxruntime::FuncManager::GetFuncs
onnxruntime::FunctionImpl::FunctionImpl
onnxruntime::FunctionKernel::FunctionKernel
onnxruntime::functors::HardSigmoid<float>::Init
onnxruntime::functors::ParametricSoftplus<float>::Init
onnxruntime::functors::ScaledTanh<float>::Init
onnxruntime::functors::Selu<float>::Init
onnxruntime::FuseReluClip::Apply
onnxruntime::FuseSubGraph
onnxruntime::FuseSubGraphDistilBert
onnxruntime::FuseSubGraphQK
onnxruntime::FuseSubGraphQKDistilBert
onnxruntime::FuseSubGraphQKImpl
onnxruntime::Gather::Compute
onnxruntime::GatherBase::GatherBase
onnxruntime::GatherElements::GatherElements
onnxruntime::GatherND::Compute
onnxruntime::GeluApproximation::ApplyImpl
onnxruntime::GeluFusion::ApplyImpl
onnxruntime::Gemm<double>::Gemm
onnxruntime::Gemm<float>::Gemm
onnxruntime::GemmActivationFusion::ApplyImpl
onnxruntime::GemmBroadcastBias
onnxruntime::GemmHelper::GemmHelper
onnxruntime::GetKernelCreateInfo
onnxruntime::GetScalarSplitInput
onnxruntime::GetSeqIdx
onnxruntime::GetSplitSizesInput
onnxruntime::GetSubGraphSessionStatesOrtFormat
onnxruntime::Graph::AddEdge
onnxruntime::Graph::AddInitializedTensor
onnxruntime::Graph::AllocateNode
onnxruntime::Graph::BuildConnections
onnxruntime::Graph::CleanUnusedInitializers
onnxruntime::Graph::CreateFusedSubGraphNode
onnxruntime::Graph::FinalizeFuseSubGraph
onnxruntime::Graph::ForThisAndAllSubgraphs
onnxruntime::Graph::Graph
onnxruntime::Graph::InferAndVerifySubgraphTypes
onnxruntime::Graph::InferAndVerifyTypeMatch
onnxruntime::Graph::InitializeStateFromModelFileGraphProto
onnxruntime::Graph::InitInputsInitializersOutputs
onnxruntime::Graph::InlineFunction
onnxruntime::Graph::KahnsTopologicalSort
onnxruntime::Graph::LoadFromOrtFormat
onnxruntime::Graph::LoadFromOrtFormat::<lambda_3d5d1fa0a04e6ac81bdd7d349241e290>::operator ()
onnxruntime::Graph::NodeAtIndexImpl
onnxruntime::Graph::PerformTypeAndShapeInferencing
onnxruntime::Graph::RemoveEdge
onnxruntime::Graph::RemoveInitializedTensor
onnxruntime::Graph::RemoveNode
onnxruntime::Graph::Resolve
onnxruntime::Graph::SaveToOrtFormat
onnxruntime::Graph::SetInputs
onnxruntime::Graph::SetOuterScopeNodeArgs
onnxruntime::Graph::ToGraphProto
onnxruntime::Graph::ToGraphProtoInternal
onnxruntime::Graph::VerifyNodeAndOpMatch
onnxruntime::graph_utils::AddInitializer
onnxruntime::graph_utils::AddNodeInput
onnxruntime::graph_utils::CanUpdateImplicitInputNameInSubgraphs
onnxruntime::graph_utils::FindPath
onnxruntime::graph_utils::GetNodeInputIndexFromInputName
onnxruntime::graph_utils::GetNodeInputName
onnxruntime::graph_utils::GetNodeOutputName
onnxruntime::graph_utils::RemoveNode
onnxruntime::graph_utils::RemoveNodeWithSingleNodeInSingleUsedOutput
onnxruntime::graph_utils::ReplaceNodeInput
onnxruntime::graph_utils::UpdateImplicitInputNameInSubgraph
onnxruntime::GraphPartitioner::Partition
onnxruntime::GraphPartitioner::PartitionOnnxFormatModel
onnxruntime::GraphPartitioner::PartitionOrtFormatModel
onnxruntime::GraphTransformer::Apply
onnxruntime::GraphTransformer::Recurse
onnxruntime::GraphTransformerManager::ApplyTransformers
onnxruntime::GraphViewer::GetNodesInTopologicalOrder
onnxruntime::GraphViewer::GetRootNodes
onnxruntime::GraphViewer::GraphViewer
onnxruntime::HandleNegativeAxis
onnxruntime::Hardmax<float>::Compute
onnxruntime::IAllocator::CalcMemSizeForArrayWithAlignment::<lambda_a348a9c2c76669be2f8a12b76169aedc>::operator ()
onnxruntime::IDataTransfer::CopyTensors
onnxruntime::IdentityOp<0>::Compute
onnxruntime::IdentityOp<1>::Compute
onnxruntime::IExecutionFrame::GetMLValue
onnxruntime::IExecutionFrame::GetOrCreateNodeOutputMLValue
onnxruntime::IExecutionFrame::IExecutionFrame
onnxruntime::IExecutionFrame::Init
onnxruntime::IExecutionProvider::GenerateMetaDefId
onnxruntime::IExecutionProvider::InsertAllocator
onnxruntime::IExecutionProvider::TryInsertAllocator
onnxruntime::If::Compute
onnxruntime::If::If
onnxruntime::If::Info::Info
onnxruntime::If::SetupSubgraphExecutionInfo
onnxruntime::IfImpl::Execute
onnxruntime::IfImpl::Initialize
onnxruntime::IncrementIndexAndComputeOffsetSetup
onnxruntime::inference_session_utils::JsonConfigParser::ParseOrtConfigJsonInModelProto
onnxruntime::inference_session_utils::JsonConfigParser::ParseOrtConfigJsonInModelProto::<lambda_ea5d7836a5bb894f88820e1a4fdbc849>::operator ()
onnxruntime::inference_session_utils::JsonConfigParser::ParseSessionOptionsFromModelProto
onnxruntime::InferenceSession::{dtor}::<lambda_134f39f9919384d4426f2a6715f2ecbd>::operator ()
onnxruntime::InferenceSession::~InferenceSession
onnxruntime::InferenceSession::AddCustomOpDomains
onnxruntime::InferenceSession::AddPredefinedTransformers
onnxruntime::InferenceSession::ConstructorCommon
onnxruntime::InferenceSession::ConstructorCommon::<lambda_cc4dcf6fbba5e78e278178a94b784c45>::operator ()
onnxruntime::InferenceSession::CreateLoggerForRun
onnxruntime::InferenceSession::EndProfiling
onnxruntime::InferenceSession::GetModelInputs
onnxruntime::InferenceSession::GetModelMetadata
onnxruntime::InferenceSession::GetModelOutputs
onnxruntime::InferenceSession::GetOverridableInitializers
onnxruntime::InferenceSession::GetSessionState
onnxruntime::InferenceSession::InferenceSession
onnxruntime::InferenceSession::Initialize
onnxruntime::InferenceSession::Initialize::<lambda_b7531013926ef980719fc5779920cd36>::operator ()
onnxruntime::InferenceSession::Initialize::<lambda_bcf2216679d92719f42db3370b3cd665>::operator ()
onnxruntime::InferenceSession::InitLogger
onnxruntime::InferenceSession::Load
onnxruntime::InferenceSession::LoadOrtModel
onnxruntime::InferenceSession::LoadOrtModel::<lambda_7dcae2be4577512cc32ab12b37bbe9bb>::operator ()
onnxruntime::InferenceSession::NewIOBinding
onnxruntime::InferenceSession::PartitionOrtFormatModel
onnxruntime::InferenceSession::RegisterExecutionProvider
onnxruntime::InferenceSession::RegisterGraphTransformer
onnxruntime::InferenceSession::Run
onnxruntime::InferenceSession::SaveToOrtFormat
onnxruntime::InferenceSession::TransformGraph
onnxruntime::InferenceSession::ValidateInputs
onnxruntime::Initializer::Initializer
onnxruntime::Initializer::ReadExternalRawData
onnxruntime::Initializer::ToProto
onnxruntime::InlineNodes
onnxruntime::InputBroadcaster::AdvanceBy
onnxruntime::InsertCastTransformer::ApplyImpl
onnxruntime::InstanceNorm<float>::Compute
onnxruntime::InstanceNorm<float>::InstanceNorm
onnxruntime::IOBinding::BindInput
onnxruntime::IsInf::IsInf
onnxruntime::KernelRegistry::TryCreateKernel
onnxruntime::LayerNormFusion::ApplyImpl
onnxruntime::LoadOrtModelBytes
onnxruntime::logging::LoggingManager::CreateDefaultLogger
onnxruntime::logging::LoggingManager::DefaultLogger
onnxruntime::logging::LoggingManager::LoggingManager
onnxruntime::Loop::Compute
onnxruntime::Loop::Info::Info
onnxruntime::Loop::Loop
onnxruntime::Loop::SetupSubgraphExecutionInfo
onnxruntime::LoopDir
onnxruntime::LoopImpl::ConcatenateLoopOutput
onnxruntime::LoopImpl::Execute
onnxruntime::LoopImpl::Execute::<lambda_4d9c7a23d4f86fb1950fdc4ef7e95544>::operator ()
onnxruntime::LoopImpl::SaveOutputsAndUpdateFeeds
onnxruntime::LpNorm<double>::LpNorm
onnxruntime::LpNorm<float>::LpNorm
onnxruntime::LRN<float>::Compute
onnxruntime::LRN<float>::LRN
onnxruntime::LSTMBase::ComputeImpl
onnxruntime::LSTMBase::LSTMBase
onnxruntime::MatchInputToConcatSubgraph
onnxruntime::MatchPositionEmbeddingSubgraphsFromGather
onnxruntime::math::NextPosition
onnxruntime::MatMul<__int64>::Compute
onnxruntime::MatMul<double>::Compute
onnxruntime::MatMul<float>::Compute
onnxruntime::MatMul<int>::Compute
onnxruntime::MatMulAddFusion::ApplyImpl
onnxruntime::MatMulComputeHelper::Compute
onnxruntime::MatMulInteger::Compute
onnxruntime::MatMulIntegerToFloatFusion::ApplyImpl
onnxruntime::MatMulScaleFusion::ApplyImpl
onnxruntime::Max_6<float>::Compute
onnxruntime::MaxPoolV8::ComputeImpl
onnxruntime::MaxUnpool::Compute
onnxruntime::MaxUnpool::MaxUnpool
onnxruntime::Mean_6<float>::Compute
onnxruntime::MeanVarianceNormalization_0<float>::MeanVarianceNormalization_0
onnxruntime::MemcpyTransformer::ApplyImpl
onnxruntime::MemPatternPlanner::TraceAllocation
onnxruntime::MergeShapeInfo
onnxruntime::Min_6<float>::Compute
onnxruntime::ml::batched_update_scores_inplace
onnxruntime::ml::CastInputToFloat
onnxruntime::ml::CastMap::CastMap
onnxruntime::ml::CastMap::ComputeImpl
onnxruntime::ml::CategoryMapper::CategoryMapper
onnxruntime::ml::detail::TreeAggregator<double,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregator<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorAverage<double,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorAverage<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<__int64,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<__int64,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<double,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<double,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<float,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<int,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<int,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorMax<double,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorMax<float,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorMin<double,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorMin<float,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<__int64,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<__int64,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorSum<double,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<double,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorSum<float,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<float,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorSum<int,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<int,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeEnsembleCommon<__int64,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommon<double,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommon<double,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommon<float,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommon<float,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommon<int,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<__int64,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<double,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<float,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<int,float>::compute
onnxruntime::ml::DictVectorizerOp<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<__int64,double>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<__int64,float>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,double>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float>::DictVectorizerOp
onnxruntime::ml::FeatureVectorizer::Compute
onnxruntime::ml::FeatureVectorizer::FeatureVectorizer
onnxruntime::ml::ImputerOp::Compute
onnxruntime::ml::ImputerOp::ImputerOp
onnxruntime::ml::LabelEncoder::LabelEncoder
onnxruntime::ml::LabelEncoder_2<__int64,__int64>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<__int64,float>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<float,__int64>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<float,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::LabelEncoder_2
onnxruntime::ml::LinearClassifier::ComputeImpl
onnxruntime::ml::LinearClassifier::LinearClassifier
onnxruntime::ml::LinearRegressor::LinearRegressor
onnxruntime::ml::MakeCast
onnxruntime::ml::MakeNormalize
onnxruntime::ml::MakePack
onnxruntime::ml::Normalizer::Normalizer
onnxruntime::ml::OneHotEncoderOp<__int64>::OneHotEncoderOp
onnxruntime::ml::OneHotEncoderOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::OneHotEncoderOp
onnxruntime::ml::OneHotEncoderOp<double>::OneHotEncoderOp
onnxruntime::ml::OneHotEncoderOp<float>::OneHotEncoderOp
onnxruntime::ml::RegisterOnnxMLOperatorKernels
onnxruntime::ml::ScalerOp<__int64>::ScalerOp
onnxruntime::ml::ScalerOp<double>::ScalerOp
onnxruntime::ml::ScalerOp<float>::ScalerOp
onnxruntime::ml::ScalerOp<int>::ScalerOp
onnxruntime::ml::SVMClassifier::Compute
onnxruntime::ml::SVMClassifier::SVMClassifier
onnxruntime::ml::SVMCommon::SVMCommon
onnxruntime::ml::SVMRegressor<float>::Compute
onnxruntime::ml::SVMRegressor<float>::SVMRegressor
onnxruntime::ml::ZipMapOp::ZipMapOp
onnxruntime::Mod::Compute
onnxruntime::Mod::Mod
onnxruntime::Model::Load
onnxruntime::Model::LoadFromOrtFormat
onnxruntime::Model::Model
onnxruntime::Model::Save
onnxruntime::Model::SaveToOrtFormat
onnxruntime::model_load_utils::IsAllowReleasedONNXOpsetsOnlySet
onnxruntime::model_load_utils::ValidateOpsetForDomain
onnxruntime::Multinomial::Multinomial
onnxruntime::MultinomialCompute
onnxruntime::NchwcTransformer::ApplyImpl
onnxruntime::ngram_details::PopulateGrams
onnxruntime::NhwcTransformer::ApplyImpl
onnxruntime::Node::ForEachWithIndex
onnxruntime::Node::LoadEdgesFromOrtFormat
onnxruntime::Node::LoadEdgesFromOrtFormat::<lambda_e4dedc1faed72ddce2da2907a3c42b9a>::operator ()
onnxruntime::Node::LoadFromOrtFormat
onnxruntime::Node::LoadFromOrtFormat::<lambda_54fc2e4e560d16157abc4bc5a9a94a86>::operator ()
onnxruntime::Node::SaveToOrtFormat
onnxruntime::NodeArg::UpdateTypeAndShape
onnxruntime::NodeIndexInfo::GetMLValueIndex
onnxruntime::NodeIndexInfo::GetNodeOffset
onnxruntime::NodeIndexInfo::Init::<lambda_bc47dcc73653c7f9cb7b31e06f49f967>::operator ()
onnxruntime::NodeIndexInfo::Init::<lambda_d084e33f090b9a2ef4ac7752a4304e52>::operator ()
onnxruntime::NonMaxSuppression::Compute
onnxruntime::NonMaxSuppressionBase::GetThresholdsFromInputs
onnxruntime::NonMaxSuppressionBase::NonMaxSuppressionBase
onnxruntime::NonMaxSuppressionBase::PrepareCompute
onnxruntime::NonTensorTypeBase::FromDataContainer
onnxruntime::NonTensorTypeBase::IsMapCompatible
onnxruntime::NonTensorTypeBase::IsSequenceCompatible
onnxruntime::NonTensorTypeBase::ToDataContainer
onnxruntime::NonZero<__int64>::Compute
onnxruntime::NonZero<bool>::Compute
onnxruntime::NonZero<float>::Compute
onnxruntime::NonZero<int>::Compute
onnxruntime::NonZero<unsigned char>::Compute
onnxruntime::NoTransposeReduce
onnxruntime::OneHotOp<__int64,__int64,__int64>::Compute
onnxruntime::OneHotOp<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::Compute
onnxruntime::OneHotOp<__int64,float,__int64>::Compute
onnxruntime::OneHotOp<__int64,float,float>::Compute
onnxruntime::OneHotOp<__int64,float,int>::Compute
onnxruntime::OneHotOp<__int64,int,float>::Compute
onnxruntime::OneHotOp<float,__int64,__int64>::Compute
onnxruntime::OneHotOp<float,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::Compute
onnxruntime::OneHotOp<float,float,float>::Compute
onnxruntime::OneHotOp<int,float,float>::Compute
onnxruntime::OneHotOp<int,float,int>::Compute
onnxruntime::OnnxRuntimeOpSchemaRegistry::RegisterOpSchemaInternal
onnxruntime::OnnxRuntimeOpSchemaRegistry::RegisterOpSet
onnxruntime::OpKernel::ComputeAsync
onnxruntime::OpKernelContext::GetOrCreateOutputMLValue
onnxruntime::OpKernelContext::Input
onnxruntime::OpKernelContext::NumVariadicInputs
onnxruntime::OpKernelContext::OpKernelContext
onnxruntime::OpKernelContext::Output
onnxruntime::OpKernelContext::OutputMLValue
onnxruntime::OpKernelContextInternal::OpKernelContextInternal
onnxruntime::OpKernelInfo::GetMemoryInfo
onnxruntime::OpNodeProtoHelper<class onnxruntime::ProtoHelperNodeContext>::GetAttrs
onnxruntime::OpNodeProtoHelper<struct onnx::InferenceContext>::GetAttrs
onnxruntime::optimizer_utils::GenerateRewriteRules
onnxruntime::optimizer_utils::GenerateTransformers
onnxruntime::OptimizerExecutionFrame::Info::{ctor}::<lambda_beb16572edee23057a5875bc228caa9d>::operator ()
onnxruntime::OptimizerExecutionFrame::Info::Info
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Create
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Iterator::Iterator
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Iterator::operator *
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Create
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Iterator::Iterator
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Iterator::operator *
onnxruntime::OutputBroadcaster::OutputBroadcaster
onnxruntime::Pad::Compute
onnxruntime::PadBase::PadBase
onnxruntime::PadImpl
onnxruntime::PadInputWithDimValueOfZero
onnxruntime::PadValueFromFloat
onnxruntime::ParallelExecutor::Execute
onnxruntime::ParallelExecutor::RunNodeAsync
onnxruntime::PartitionOnnxFormatModelImpl
onnxruntime::PartitionOrtFormatModelImpl
onnxruntime::Path::Parse
onnxruntime::PlaceNode
onnxruntime::PlannerImpl::AllocPlan
onnxruntime::PlannerImpl::Buffer
onnxruntime::PlannerImpl::ComputeReusePlan
onnxruntime::PlannerImpl::ComputeUseCounts
onnxruntime::PlannerImpl::CreatePlan
onnxruntime::PlannerImpl::GenerateDeallocationPlan
onnxruntime::PlannerImpl::GeneratePlanForWeights
onnxruntime::PlannerImpl::GetElementSize
onnxruntime::PlannerImpl::GetLocationForNodeInput
onnxruntime::PlannerImpl::Index
onnxruntime::PlannerImpl::ProcessDef
onnxruntime::PlannerImpl::Reuse
onnxruntime::PlannerImpl::UseCount
onnxruntime::PlannerImpl::VerifyMemoryTimeSchedule
onnxruntime::Pool<float,class onnxruntime::LpPool>::Compute
onnxruntime::PoolAttributes::ComputeSizePadDilations
onnxruntime::PoolAttributes::InferOutputSize
onnxruntime::PoolAttributes::PoolAttributes
onnxruntime::PoolAttributes::SetOutputSize
onnxruntime::PoolBase::Compute
onnxruntime::PoolProcessContext::init
onnxruntime::PrepareForQDQ
onnxruntime::profiling::Profiler::EndProfiling
onnxruntime::profiling::Profiler::EndTimeAndRecordEvent
onnxruntime::profiling::Profiler::Initialize
onnxruntime::profiling::Profiler::StartProfiling
onnxruntime::profiling::Profiler::StartTime
onnxruntime::ProviderLibrary::Get
onnxruntime::ProviderSharedLibrary::Ensure
onnxruntime::QLinearConv::Compute
onnxruntime::QLinearMatMul::Compute
onnxruntime::RandomNormal::RandomNormal
onnxruntime::RandomNormalCompute
onnxruntime::RandomNormalLike::RandomNormalLike
onnxruntime::RandomUniform::RandomUniform
onnxruntime::RandomUniformCompute
onnxruntime::RandomUniformLike::RandomUniformLike
onnxruntime::ReduceKernelBase<0>::ReduceKernelBase
onnxruntime::ReduceKernelBase<1>::ReduceKernelBase
onnxruntime::ReduceSum<__int64>::Impl
onnxruntime::ReduceSum<double>::Impl
onnxruntime::ReduceSum<float>::Impl
onnxruntime::ReduceSum<int>::Impl
onnxruntime::RegisterCPUKernels
onnxruntime::RegisterOnnxOperatorKernels
onnxruntime::ReleaseNodeMLValues
onnxruntime::RemoveDuplicateCastTransformer::ApplyImpl
onnxruntime::Reshape::Compute
onnxruntime::Reshape_1::Reshape_1
onnxruntime::ReshapeFusion::ApplyImpl
onnxruntime::ReshapeFusion::Fuse_Subgraph
onnxruntime::ReshapeHelper::ReshapeHelper
onnxruntime::ReverseSequenceOp::Compute
onnxruntime::ReverseSequenceOp::ReverseSequenceOp
onnxruntime::rnn::detail::ComputeGemm
onnxruntime::rnn::detail::deepcpu::ActivationFuncByName
onnxruntime::rnn::detail::deepcpu::GruOutputGateFuncByName
onnxruntime::rnn::detail::deepcpu::GruResetGateFuncByName
onnxruntime::rnn::detail::deepcpu::LstmMergeGatesFuncByName
onnxruntime::rnn::detail::MakeDirection
onnxruntime::rnn::detail::NormalizeActivationArgumentAndGetAlphaBetaCount
onnxruntime::rnn::detail::SafeRawConstPointer
onnxruntime::rnn::detail::SafeRawPointer
onnxruntime::RNN<float>::Compute
onnxruntime::RNN<float>::RNN
onnxruntime::RoiAlignBase::RoiAlignBase
onnxruntime::RoiPool<float>::Compute
onnxruntime::RoiPool<float>::RoiPool
onnxruntime::RuleBasedGraphTransformer::ApplyImpl
onnxruntime::RuleBasedGraphTransformer::ApplyRulesOnNode
onnxruntime::SaveModel
onnxruntime::scan::detail::CreateFeedsFetchesManager
onnxruntime::scan::detail::Info::Info
onnxruntime::scan::detail::IterateSequence
onnxruntime::scan::detail::IterateSequence::<lambda_df18081ccc34b5e376ea2f099a6c86d1>::operator ()
onnxruntime::scan::detail::LoopStateVariable::Next
onnxruntime::scan::detail::OutputIterator::AllocateFinalBuffer
onnxruntime::scan::detail::OutputIterator::AllocateFinalOutput
onnxruntime::scan::detail::OutputIterator::GetOutput
onnxruntime::scan::detail::OutputIterator::Initialize
onnxruntime::scan::detail::OutputIterator::operator *
onnxruntime::scan::detail::OutputIterator::operator ++
onnxruntime::scan::detail::ReadDirections
onnxruntime::Scan<8>::Compute
onnxruntime::Scan<8>::Scan
onnxruntime::Scan<8>::SetupSubgraphExecutionInfo
onnxruntime::Scan<9>::Compute
onnxruntime::Scan<9>::Scan
onnxruntime::Scan<9>::SetupSubgraphExecutionInfo
onnxruntime::Scan8Impl::AllocateOutputTensors
onnxruntime::Scan8Impl::CreateLoopStateVariables
onnxruntime::Scan8Impl::Execute
onnxruntime::Scan8Impl::Initialize
onnxruntime::Scan8Impl::ValidateInput
onnxruntime::ScanImpl::AllocateOutputTensors
onnxruntime::ScanImpl::CreateLoopStateVariables
onnxruntime::ScanImpl::Execute
onnxruntime::ScanImpl::Initialize
onnxruntime::ScanImpl::SetupInputs
onnxruntime::ScanImpl::TransposeOutput
onnxruntime::ScanImpl::ValidateInput
onnxruntime::Scatter::Compute
onnxruntime::Scatter::Scatter
onnxruntime::ScatterND::Compute
onnxruntime::ScatterNDBase::PrepareForCompute
onnxruntime::SequenceAt::Compute
onnxruntime::SequenceConstruct::Compute
onnxruntime::SequenceEmpty::Compute
onnxruntime::SequenceErase::Compute
onnxruntime::SequenceInsert::Compute
onnxruntime::SequenceLength::Compute
onnxruntime::SequenceTensorTypeBase::GetElementType
onnxruntime::SequenceTensorTypeBase::IsCompatible
onnxruntime::SequentialExecutor::Execute
onnxruntime::session_state_utils::DeserializeTensorProto
onnxruntime::session_state_utils::SaveInitializedTensors
onnxruntime::session_state_utils::SaveInitializedTensors::<lambda_1ddec5f12f5870e921dc03552d928e7f>::operator ()
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping::<lambda_04d93a296e8e245be45f1b71871c71c4>::operator ()
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping::<lambda_7fe593f35b7bf5ce6d3de3d99005ffa9>::operator ()
onnxruntime::SessionOptions::AddConfigEntry
onnxruntime::SessionState::AddOutputNameToNodeInfoMapping
onnxruntime::SessionState::AddSubgraphSessionState
onnxruntime::SessionState::CreateGraphInfo
onnxruntime::SessionState::CreateSubgraphSessionState
onnxruntime::SessionState::FinalizeSessionState
onnxruntime::SessionState::FinalizeSessionStateImpl
onnxruntime::SessionState::GetNodeIndexInfo
onnxruntime::SessionState::GetNodeKernelCreateInfo
onnxruntime::SessionState::LoadFromOrtFormat
onnxruntime::SessionState::LoadFromOrtFormat::<lambda_c43b6425f7359047907aabe5dc5d16ef>::operator ()
onnxruntime::SessionState::PopulateKernelCreateInfo
onnxruntime::SessionState::PrepackConstantInitializedTensors
onnxruntime::SessionState::SaveToOrtFormat
onnxruntime::SessionState::SetupAllocators
onnxruntime::SessionState::UpdateToBeExecutedNodes
onnxruntime::SetEnableProfiling
onnxruntime::SetExecutionMode
onnxruntime::SetGraphOptimizationLevel
onnxruntime::SetInterOpNumThreads
onnxruntime::SetIntraOpNumThreads
onnxruntime::SetupForReduce
onnxruntime::Shrink::Shrink
onnxruntime::SimpleTensorAllocator::GetPreallocatedBuffer
onnxruntime::SimplifiedLayerNormFusion::ApplyImpl
onnxruntime::SkipLayerNormFusion::ApplyImpl
onnxruntime::SliceBase::Compute
onnxruntime::SliceBase::FillVectorsFromInput
onnxruntime::SliceBase::SliceBase
onnxruntime::SliceImpl::<lambda_2d7b68cf081f6020a40f9cadb759259b>::operator ()
onnxruntime::SliceImpl::<lambda_80a346a00f94cfca5c0040adb0ff506d>::operator ()
onnxruntime::SliceImpl::<lambda_cb3dd282566b6182e311d28218bafb8e>::operator ()
onnxruntime::SliceImpl::<lambda_d9d95929f6d08c9006145e05b9c64f30>::operator ()
onnxruntime::SliceImpl::<lambda_e14984825aef9cc23303ec6a29437c1b>::operator ()
onnxruntime::SliceIteratorBase::CopyInnermostAxisNonSolitaryInnerStep
onnxruntime::SliceIteratorBase::Init
onnxruntime::SliceSkips::SliceSkips
onnxruntime::Softmax<double>::ComputeImplOpset13
onnxruntime::Softmax<float>::ComputeImplOpset13
onnxruntime::SpaceDepthBase::SpaceDepthBase
onnxruntime::SpaceToDepth<float>::Compute
onnxruntime::SparseTensorTypeBase::GetElementType
onnxruntime::SparseTensorTypeBase::IsCompatible
onnxruntime::Split::Compute
onnxruntime::Split::ComputeImpl
onnxruntime::SplitBase::SplitBase
onnxruntime::SplitToSequence::Compute
onnxruntime::SplitToSequence::ComputeImpl
onnxruntime::Squeeze::Compute
onnxruntime::SqueezeBase::ComputeOutputShape
onnxruntime::string_normalizer::Locale::Locale
onnxruntime::StringNormalizer::StringNormalizer
onnxruntime::StringToAutoPadType
onnxruntime::Sum_6<double>::Compute
onnxruntime::Sum_6<float>::Compute
onnxruntime::Tensor::Data
onnxruntime::Tensor::DataAsSpan
onnxruntime::Tensor::DataRaw
onnxruntime::Tensor::Init
onnxruntime::Tensor::MutableData
onnxruntime::Tensor::MutableDataAsSpan
onnxruntime::Tensor::MutableDataRaw
onnxruntime::Tensor::Reshape
onnxruntime::Tensor::SizeInBytes
onnxruntime::Tensor::Tensor
onnxruntime::TensorAllocator::TensorAllocator
onnxruntime::TensorAllocatorWithMemPattern::FinalizePlan
onnxruntime::TensorAllocatorWithMemPattern::Trace
onnxruntime::TensorSeq::Get
onnxruntime::TensorSeq::SetType
onnxruntime::TensorShape::SizeFromDimension
onnxruntime::TensorShape::SizeToDimension
onnxruntime::TensorShape::Slice
onnxruntime::TensorTypeBase::GetElementType
onnxruntime::TensorTypeBase::IsCompatible
onnxruntime::TfIdfVectorizer::TfIdfVectorizer
onnxruntime::Tile::Compute
onnxruntime::ToMBString
onnxruntime::TopkOpset10ConstructorCommon
onnxruntime::TopkOpset11ConstructorCommon
onnxruntime::TopkOpset9ConstructorCommon
onnxruntime::ToWideString
onnxruntime::TransformerMemcpyImpl::ProcessDefs
onnxruntime::TransformerMemcpyImpl::ProcessInitializers
onnxruntime::TransformerMemcpyImpl::ProcessInitializers::<lambda_86b0d88e3b26101a7a3a4da61749b425>::operator ()
onnxruntime::Transpose::Compute
onnxruntime::TransposeBase::TransposeBase
onnxruntime::TypedDoTransposeEltWise
onnxruntime::Unsqueeze::Compute
onnxruntime::UnsqueezeBase::PrepareCompute
onnxruntime::UnsqueezeBase::UnsqueezeBase
onnxruntime::UnsqueezeElimination::Apply
onnxruntime::UntypedExpand
onnxruntime::update_subgraphs_within_function_body
onnxruntime::Upsample<float>::BaseCompute
onnxruntime::Upsample<float>::Compute
onnxruntime::Upsample<int>::BaseCompute
onnxruntime::Upsample<int>::Compute
onnxruntime::Upsample<unsigned char>::BaseCompute
onnxruntime::Upsample<unsigned char>::Compute
onnxruntime::UpsampleBase::ParseScalesData
onnxruntime::UpsampleBase::ParseScalesDataFromOutputSize
onnxruntime::UpsampleBase::ScalesValidation
onnxruntime::UpsampleBase::StringToCoordinateTransformationMode
onnxruntime::UpsampleBase::StringToNearestMode
onnxruntime::UpsampleBase::StringToUpsampleMode
onnxruntime::UpsampleBase::UpsampleBase
onnxruntime::utils::BatchOrCopyMLValue
onnxruntime::utils::CalculateStaticCopyInfoForFeed
onnxruntime::utils::CalculateStaticCopyInfoForFeeds
onnxruntime::utils::ConstantNodeProtoToTensorProto
onnxruntime::utils::ContainerChecker::ContainerChecker
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,__int64,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,__int64> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,double,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,double> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,__int64> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,double,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,double> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::vector<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > >,class std::allocator<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > > > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::vector<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > >,class std::allocator<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > > > > >::check
onnxruntime::utils::CopyInputsAcrossDevices
onnxruntime::utils::CopyOneInputAcrossDevices
onnxruntime::utils::CopyOutputsAcrossDevices
onnxruntime::utils::CopySparseData
onnxruntime::utils::DenseTensorToSparseTensorProto
onnxruntime::utils::detail::CopyLittleEndian
onnxruntime::utils::ExecuteGraph
onnxruntime::utils::ExecuteGraphImpl
onnxruntime::utils::FinalizeCopyInfoForFeeds
onnxruntime::utils::FinalizeCopyInfoForFetches
onnxruntime::utils::FindMemoryInfoForValue
onnxruntime::utils::GetFileContent
onnxruntime::utils::GetMLDataType
onnxruntime::utils::InitializeFeedFetchCopyInfo
onnxruntime::utils::mltype_dispatcher_internal::CallableDispatchableHelper::CheckCalledOnce
onnxruntime::utils::mltype_dispatcher_internal::UnsupportedTypeDefaultPolicy<class onnxruntime::common::Status>::operator ()
onnxruntime::utils::SparseTensorProtoToDenseTensorProto
onnxruntime::utils::TensorProtoToMLValue
onnxruntime::utils::UnpackInitializerData
onnxruntime::utils::UnpackTensorWithExternalDataImpl
onnxruntime::ViewerFunctionImpl::Body
onnxruntime::WritableSliceIterator<__int64>::Init
onnxruntime::WritableSliceIterator<double>::Init
onnxruntime::WritableSliceIterator<float>::Init
onnxruntime::WritableSliceIterator<int>::Init
onnxruntime_profile_
onnxruntime_providers_dnnl.dll
onnxruntime_providers_openvino.dll
onnxruntime_providers_shared.dll
onnxruntime_providers_tensorrt.dll
Op registered for 
Op with name (
op_kernel_info.GetAttr("axis", &axis_).IsOK()
op_kernel_info.GetAttr<float>("bias", &bias_temp).IsOK()
op_kernel_info.GetAttr<float>("epsilon", &epsilon_).IsOK()
op_kernel_info.GetAttr<float>("lambd", &lambd_temp).IsOK()
op_kernel_info.GetAttr<int64_t>("axis", &axis_).IsOK()
op_kernel_info.GetAttr<int64_t>("axis", &axis_temp).IsOK()
op_kernel_info.GetAttr<int64_t>("k", &k_temp).IsOK()
op_kernel_info.GetAttr<int64_t>("largest", &largest_temp).IsOK()
op_kernel_info.GetAttr<int64_t>("p", &p_).IsOK()
op_kernel_info.GetAttr<int64_t>("sorted", &sorted_temp).IsOK()
op_name
op_type
opaque
Opaque type is not a non_tensor type!!!
opaque(
opaque_type
open file 
OpenSemaphoreW
OpenVINOExecutionProvider
operation canceled
operation in progress
operation not permitted
operation not supported
operation would block
operator
Operator '
operator "" 
operator co_await
operator<=>
OpH+OhH
OPI;OP
OpKernel was null
opset id is null. Invalid ORT format model.
opset import domain is null. Invalid ORT format model.
Optional position subgraph nodes number of outputs unexpected.
Optional position subgraph nodes Where node is expected to be the parent of Reshape.
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM.
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM. Default values are the same as of corresponding ONNX operators.
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM. Default values are the same as of corresponding ONNX operators.For example with LeakyRelu, the default alpha is 0.01.
or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
Order
OriginalFilename
Oriya
ORT config json from the model: 
ORT model verification failed.
ort_config
ORT_LOAD_CONFIG_FROM_MODEL
ort_value.Fence() == nullptr
ort_value.IsAllocated()
ort_value.IsTensor()
ort_value_idx >= 0 && static_cast<size_t>(ort_value_idx) < alloc_plan.size()
ort_value_index >= 0 && static_cast<size_t>(ort_value_index) < all_values_size_
ort_value_index >= 0 && static_cast<size_t>(ort_value_index) < alloc_plan.size()
ort_value_name_idx_map.MaxIdx() > -1
OrtApis::CreateOpaqueValue
OrtApis::GetOpaqueValue
OrtApis::GetTensorTypeAndShape
OrtCreateMapMLValue
OrtCreateValueImplMapHelper
OrtCreateValueImplSeqHelper
OrtEnv::Release
OrtGetApiBase
OrtGetWinMLAdapter
OrtMemoryInfo is null
OrtMemoryInfo:[
ORTMH
OrtProgrammingProjection
OrtSessionOptionsAppendExecutionProvider_CPU
OrtSessionOptionsAppendExecutionProvider_DML
OrtSessionOptionsAppendExecutionProviderEx_DML
OrtValue has not been allocated so can't be sliced.
OrtValue indexes should have been populated.
OrtValue is not a Tensor
OrtValue is TensorSequence type but has no element Tensor DataType.
OrtValue shape verification failed. Current shape:
OrtValue::Get
OrtValue::GetMutable
Osage
Osmanya
oT$@f
other_error
out of index
Out of memory
out_of_range
Outer scope node arg name '
outer_scope_node_args_consumed.empty()
OutOfBoundsInputValue
output
Output
output 
Output 
output != nullptr
output == output_end
output and sum shape must match
Output buffer allocation failed
output buffer is too small
Output case #1: Y, mean, var, saved_mean, saved_var (training mode)
Output case #2: Y (test mode)
Output channels M is not divisible by group.
output count mismatch, expected 2 outputs to be present for TopK operator
Output data tensor.
Output edge count not expected for Add or MatMul in path v
Output edge count not expected for mask nodes
Output edge count not expected for nodes in gemm gather path
Output edge count not expected for nodes in gemm path
Output edge count not expected for nodes in past subgraph
Output edge count not expected for nodes in path 1 of position shape.
Output edge count not expected for nodes in path 1 of unidirectional mask
Output edge count not expected for nodes in path 2 of position shape.
Output edge count not expected for nodes in path v
Output edge count not expected for nodes in path1.
Output edge count not expected for Softmax
Output edge count not expected for squeeze_2/slices2/shape2 of unidirectional mask
Output edge count not expected for unsqueeze2 of unidirectional mask
Output edge count not expected for unsqueeze3 of unidirectional mask
output edges
output name cannot be empty
Output OrtValue has not been created for loop state variable output 
Output subscript contains letters not seen in the inputs
Output subscript contains repeated letters
output tensor
Output tensor must have at least 2 dimensions
Output tensor of shape (M, N).
Output tensor of the same type and shape as the input tensor.
Output type is determined by the specified 'values_*' attribute.
Output type must be int32 or int64
Output vector incorrectly sized: output_names.size(): 
Output vector pointer is NULL
Output was expected to have tensor type. Got 
output.SizeInBytes() == input.SizeInBytes()
Output:
output_dims.size() == dims.size()
output_dims[i] == 0
output_height
output_mlvalue
output_names_to_nodeinfo.empty()
output_node.OutputDefs().size() == 1
output_padding
output_sequence
output_shape
output_shape is smaller than minimum required. output_shape:
'output_shape' must be rank 1 tensor.
'output_shape' must have same number of elements as the shape of input tensor X.
output_size
output_width
OutputCellSingleTensor
OutputCoordinatesTensor
OutputCount
OutputCountTensor
OutputDebugStringW
OutputFirstMomentTensor
OutputGradientTensor
OutputIndexTensor
OutputIndicesTensor
OutputPadding
OutputParametersTensor
OutputPixelOffsets
outputs
Outputs from Scan are not optional and should never be null.
outputs...
OutputScaleTensor
OutputSecondMomentTensor
OutputSequenceTensor
OutputSingleTensor
OutputStateTensor
OutputTensor
OutputTensors
OutputValueTensor
OutputZeroPointTensor
owner dead
OXI;OX
OXI+OPH
p AWH
p H+p
P H+P
p H9^
P Hc@
P HcH
p L+p
p L9f
p p t y 
p UWATAVAWH
p UWAUAVAWH
p UWAVH
p UWAWH
p V"6$6
p value of the Lp norm used to pool over the input data, default is 2.0.
p value of the Lp norm used to pool over the input data.
p WATAUAVAWH
p WATAWH
p WAVAWH
P(H;P0tFL
p.first->second->id_ == 0
p.second
P@I+P8H
p_ == 1 || p_ == 2
p_fetches->size(): 
p_int < base_int + memory_size_
p_int >= base_int
p_kernel_def
p_ml_value
p_mlvalue
p_op_kernel
p_provider
p_type != nullptr
p>ZzL
P08@H
P08~T
P08NT
P0Hc@(H
p2\RF
p20^E
P4D~E
p6(6L
P6p+M
p7M}6p7M
P8@HP^)
p8@HPX`hp
P8@HPX`hpx
p8H9w
PA^_]
pA^_^
PA^_^
pA^_^
PA^_^
pA^_^[]
PA^_^[]
pA^_^[]
pA^_^][
PA^_^][
pA^_^][
PA^_^][
pA^_^][
PA^A\_^]
pA^A\_^]
PA^A\_^]
PA^A]A\_^
PA_A]A\_^[]
PA_A^_][
pA_A^_^[
PA_A^_^[
pA_A^_^[
PA_A^_^[
PA_A^_^]
pA_A^_^]
PA_A^_^]
pA_A^_^]
PA_A^_^]
pA_A^_^]
PA_A^_^]
pA_A^_^]
PA_A^_^]
pA_A^_^]
PA_A^_^]
pA_A^A\_^
pA_A^A\_^[]
PA_A^A\_^[]
pA_A^A\_^[]
PA_A^A\_^[]
PA_A^A\_^][
pA_A^A\_^][
PA_A^A\_^][
pA_A^A\_^][
PA_A^A\_^][
PA_A^A]_^[]
PA_A^A]_^][
pA_A^A]A\_^[
PA_A^A]A\_^[
pA_A^A]A\_^[
PA_A^A]A\_^[
pA_A^A]A\_^[
PA_A^A]A\_^[
pA_A^A]A\_^[
PA_A^A]A\_^[
pA_A^A]A\_^[
PA_A^A]A\_^[
pA_A^A]A\_^[
PA_A^A]A\_^[
PA_A^A]A\_^]
pA_A^A]A\_^]
PA_A^A]A\_^]
pA_A^A]A\_^]
PA_A^A]A\_^]
pA_A^A]A\_^]
PA_A^A]A\_^]
pA_A^A]A\_^]
PA_A^A]A\_^]
pA_A^A]A\_^]
PA_A^A]A\_^]
pA_A^A]A\_^]
PA_A^A]A\_^]
pA_A^A]A\_^]
PA_A^A]A\_^]
pA_A^A]A\_^]
PA_A^A]A\_^]
pA_A^A]A\_^]
PA_A^A]A\_^]
pA_A^A]A\_^]
PA_A^A]A\_^]
pA_A^A]A\_^]
PA_A^A]A\_^]
Pad should be smaller than kernel.
pad_value
Padding for the beginning and ending along each axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute.
Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaults to 0 along start and end of each spatial axis.
Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0.The value represent the number of pixels added to the beginning and end part of the corresponding axis.`pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number ofpixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaultsto 0 along start and end of each spatial axis.
PaddingMode
paddings
PaddingValue
Pads has incorrect number of values
'pads' has wrong number of values
'pads' input must be a 1D (shape: [2 * input_rank]) tensor of type int64
'pads' input must be a 1D (shape: [2 * n_input_dims]) tensor of type int64
'pads' input must be a 1D (shape: [input_rank]) or 2D tensor (shape: [1, input_rank]) of type int64
Pads tensor should be a 1D tensor of shape [2 * input_rank] or a 2D tensor of shape [1, 2 * input_rank]
Pads tensor should be an INT64 tensor
Pads tensor size should be equal to twice the input dimension count 
pads[dim] < kernel_shape[dim] && pads[dim + kernel_shape.size()] < kernel_shape[dim]
pads_[dim] < kernel_shape_[dim] && pads_[dim + kernel_shape_.size()] < kernel_shape_[dim]
pads_size == 2 * data_rank
pads_tensor.IsDataType<int64_t>()
pads_tensor_dims.size() == 1 || (pads_tensor_dims.size() == 2 && pads_tensor_dims[0] == 1)
Pahawh_Hmong
Palmyrene
Parallel execution mode does not support the CUDA Execution Provider. 
Parallel execution mode does not support the DML Execution Provider. 
Parallel mode
ParallelExecutor::Execute
parameter_size
ParametricSoftplus
ParametricSoftplus takes one input data (Tensor<T>) and produces one output data
parse
parse error
parse_error
parsing 
PartA_PrivTags
Pass 1 to enable broadcasting
Pass CheckNodesInPathK
Pass CheckNodesInPathQ
Pass CheckNodesInPathV
Pass MatchGemmSubgraph
Pass MatchInputMaskSubgraph
Pass MatchInputMaskSubgraphDistilBert
Pass MatchPastSubgraph
Pass MatchUnidirMaskSubgraph
Pass ValidateGemmInitializer
past_k_gather indices != 0
past_k_transpose perm attribute not matched
past_v_gather and past_k_gather does not have same past input
past_v_gather indices != 1
PathCchRemoveBackslash
PathCchRemoveFileSpec
pattern too large - compile failed
pattern too large - reverse compile failed
Pau_Cin_Hau
pbrrt|vbxj|
PeepholeTensor
Perform mean variance normalization.
Performs element-wise binary {name} on 8 bit data types (with Numpy-style broadcasting support).
perm: 
permission denied
PH;w(r
Phags_Pa
Phoenician
PHR(P
PHR(PvNnT:V
PHT(V
PHT(V|P:X
Please fetch output tensor with specified shape.
pool_attrs_.kernel_shape.size() == 2
pool_int64s
pool_strings
pool_strings must not be empty if specified
pooled_height_ > 0
pooled_shape
pooled_shape.size() == 2
pooled_width_ > 0
PooledSize
Popularity of each node, used for performance and may be omitted.
position
Position embedding data type shall be float or float16.
Position embedding shape is not expected.
Position embedding shape not matched.
position_ >= 0 && position_ < sequence_length_
position_embedding
position_embedding is expected to have 2 dimensions, got 
position_embeddings
positive
post_transform
Pow takes input data (Tensor<T>) and exponent Tensor, and
predictions.size() == (size_t)n_targets_or_classes_
predictions.size() == 2
predictions.size() == predictions2.size()
PRelu
present
present_k_transpose perm attribute not matched
present_k_unsqueeze axes value not expected
present_v_unsqueeze axes value not expected
Previous entry was not terminated.
private: 
prob_a
prob_b
proba_.size() == probb_.size()
Processed_STD
ProcessInfo
Produces a slice of the input tensor along multiple axes. Similar to numpy:
produces one output data (Tensor<T>) where the function `f(x) = x^exponent`,
ProductName
ProductVersion
Profiler is disabled.
protected: 
proto != nullptr
Protobuf parsing failed.
Protobuf serialization failed.
protocol error
protocol not supported
provided axis. The resulting tensor has the same rank as the input if keepdims equal 1.
provider
Provider 
Provider_SetHost
Psalter_Pahlavi
PtRTTXVJXTZl^*`8b
public: 
PXRhTpVXXZZ.^
pXrrtdvBx`tT|,~~
pytorch_half_pixel
PzRTTXVbXTZr^*`8b
Q +Q0
q and v are not from same Split node
Q L9APL
q root should be layer normalization
Q(+Q0
q(H;]
q(Hci 3
q(Hci A
q_matmul and q_add shape not matched
Q_Max
Q_Min
q_reshape const not matched
q_transpose perm attribute not matched
Q0H;P
q0L+q(I
q8H+q0H
QAttention
Qbr}X
QbreH
qE9f(
qE9n(
QHH+Q@H
qHI;qPu
qk_div const not matched.
qkv_bias
qkv_weights
qLcL$t
QLinearAdd
QLinearAveragePool
QLinearConv
QLinearConv : filter scale shape invalid
QLinearConv : filter zero point must be constant
QLinearConv : filter zero point shape invalid
QLinearConv : input scale must be a scalar or 1D tensor of size 1
QLinearConv : input zero point must be a scalar or 1D tensor of size 1
QLinearConv : result scale must be a scalar or 1D tensor of size 1
QLinearConv : result zero point must be a scalar or 1D tensor of size 1
QLinearGlobalAveragePool
QLinearGlobalAveragePool ImageSize too large!
QLinearGlobalAveragePool parameter out of computation range!
QLinearLeakyRelu
QLinearLeakyRelu : input X_scale must be a scalar or 1D tensor of size 1
QLinearLeakyRelu : input X_zero_point must be a scalar or 1D tensor of size 1
QLinearLeakyRelu : input Y_scale must be a scalar or 1D tensor of size 1
QLinearLeakyRelu : input Y_zero_point must be a scalar or 1D tensor of size 1
QLinearMatMul
QLinearMatmul : input scale must be a scalar or 1D tensor of size 1
QLinearMatmul : input zero point must be a scalar or 1D tensor of size 1
QLinearMatmul : result scale must be a scalar or 1D tensor of size 1
QLinearMatmul : result zero point must be a scalar or 1D tensor of size 1
QLinearMatmul : weight scale must be a scalar or 1D tensor of size 1
QLinearMatmul : weight zero point must be a scalar or 1D tensor of size 1
QLinearMul
QLinearReduceMean
QLinearSigmoid
QpD;Q`
QPH+QHH
Quantized GEMM only support alpha equal to 1.0f and beta equal to 0.0f or 1.0f
QuantizeLinear
QueryPerformanceCounter
QueryPerformanceFrequency
R B">
R D8RHu
R D8ZHu
r l"b$n&l(b*j,z.d0v2
R$<&J(
r(H;]
R(TrR
R:0^E
r;Ic8H
R_scale
R_zero_point
r`HcS$H
R`THV(T
r~LcF
R->Shape()[1] == 5
r0@2.4
r0HcM
r5w,H
raB3G
RaiseException
RaiseFailFastException
RandomNormal
RandomNormalLike
RandomUniform
RandomUniformLike
range
Range
rank == dims.size() && rank > 0
rank >= 2 && dim_1 != dim_2 && input_dims[dim_1] == input_dims[dim_2]
rank must be greater than axis
Rank of input 
Rank of input and output tensor should be same.
Rank of input to Normalized must be less than 2. Got 
Rank of the input must match number of subscript labels corresponding to the input
Ranks inferred (
Ranks of input data are different, cannot concatenate them. expected rank: 
Ranks of pair-wise operands must be equal. 
ratio
ratio input should have a single value.
ratio must be in the range [0, 1)
Ratio of Dropout must be a scalar.
ratio_tensor->Shape().Size() == 1
raw_data
rbb}H
RbTrVtXbZr\z^b`nbrdbfjh
RE2: invalid startpos, endpos pair. [
RE2: unexpected op: 
read only file system
ReadExternalRawData() failed: 
ReadFile
ReadFile 
Reading the provided model for the ORT config
Real memory steps 
Received invalid value for allow_spinning. Valid values are 0 or 1
Received invalid value for arena extend strategy. Valid values can be either 0, 1 or -1.
Received invalid value of arena_extend_strategy 
Received negative size from stat call
Received null OrtThreadingOptions
Received nullptr for custom registry
Received nullptr for exec provider
Received nullptr for graph transformer
Received nullptr for name.
Received nullptr for OrtValue.
Received OrtValue is not a tensor. Only tensors are supported.
Reciprocal
RecurrenceTensor
Recurrent
reduced
reduced_scale
reduced_zero_point
ReduceL1
ReduceL2
ReduceLogSum
ReduceLogSumExp
ReduceMax
ReduceMean
ReduceMin
ReduceProd
ReduceSum
ReduceSumInteger
ReduceSumSquare
reduction
Reduction on all axes, output size should be 1.
ReductionFunction
reflect
Regexp not destroyed.
RegisterCustomOps
RegisterCustomOpsLibrary: Entry point RegisterCustomOps not found in library
RegisterCustomOpsLibrary: Failed to load library
Rejang
Release_State_
ReleaseMutex
ReleaseSemaphore
ReleaseSRWLockExclusive
RemoveDirectory() failed - path: 
RemoveDirectoryW
RemoveDuplicateCastTransformer
Removing initializer '
reorder
ReorderInput
ReorderOutput
'repeat' input tensor must be 1 dimensional
'repeat' input tensor must have the same length as the 'input' tensor
Repeats
repeats
'Repeats' input has incorrect number of values. The number of values in 'repeats' must be equal to the number of input dimensions.
'Repeats' input must be 1D tensor of type int64
RepeatsCount
RepetitionWalker::ShortVisit called
replaced_value_float
replaced_value_int64
representative.output_index != kInvalidOutputIndex
Requested attribute: 
Requested size is too large to fit into size_t.
requested_shape[i] >= -1
Required attribute '
Required attribute axis is missing
Required min_gram_length must be positive: 
reserved_chunks_.find(ptr) == reserved_chunks_.end()
ResetEvent
Reshape
reshape initializer value is not expected
reshaped
ReshapeFusion
Resize
Resize operator
Resize: input shape needs to be at least a single dimension
Resize: input tensor's dimension does not match the scales.
Resize: input tensor's rank does not match the output tensor's rank.
Resize: input/output value is nullptr
Resize: input/output value's dimension mismatch
Resize: size of roi array should be 2 * N where N is the rank of input tensor X.
Resize: unexpected mode
Resolve subgraph failed:
ResolveDelayLoadedAPI
resource deadlock would occur
resource unavailable try again
result
Result buffer is not large enough
result out of range
Result, has same shape and type as input
Result, has same type as input, with H and W dimensions reduced.
result.second
ReturnHr
reused != reused_for
reverse
ReverseSequence
r-H;Q
RHLcJ@I
rHt(v
rHt(vlRHT(X
ri9O vdH
RIGHT
right operand cannot broadcast on dim 
Right shape: 
right.NumDimensions() == 2
right.Shape().Size() == right_shape_override.Size()
RknpuExecutionProvider
rLH9Z
RLTZVZXJZ$\
RNN op: Invalid activation attribute - 
ROCMExecutionProvider
ROI pool output shape (height, width).
RoI pooled output, 4-D tensor of shape (num_rois, C, crop_height, crop_width). The r-th batch element Y[r-1] is a pooled feature map corresponding to the r-th RoI X[r-1].
roi_batch_id < batch_size
roi_batch_id >= 0
roi_input_idx_ > 0
RoiAlign
RoIs (Regions of Interest) to pool over; rois is 2-D input of shape (num_rois, 4) given as [[y1, x1, y2, x2], ...]. The RoIs' coordinates are normalized in the coordinate system of the input image. Each coordinate set has a 1:1 correspondence with the 'batch_indices' input.
rois input tensor has wrong dimension
RoIs tensor must have 2 dimensions
ROITensor
RoOriginateLanguageException
Round
round_prefer_ceil
round_prefer_floor
Rounded_ZeroPoint_FP
RoundingMode
RtlCaptureContext
RtlLookupFunctionEntry
RtlPcToFileHeader
RtlUnwindEx
RtlVirtualUnwind
run_options.run_log_severity_level >= 0 && run_options.run_log_severity_level <= static_cast<int>(logging::Severity::kFATAL)
Runic
Running with tag: 
RunStateOnByteUnlocked failed after Reset
RunStateOnByteUnlocked failed after ResetCache
RUNTIME_EXCEPTION
RuntimeError
RuntimePerf
runtimeVersion
S != nullptr
s D9s8~!H
S H;S(t@
S H;S(tH
s H+s
S H+S
s H+s
S H+S
s IcC
's number of inputs is different from function body graph's number of input.
's number of outputs is different from function body graph's number of outputs.
s WATAUAVAWH
s WATAVH
s WAVAWH
s(+s0
s(HcD$
S@H9)u
s@Hck83
s@Hck8I
s~H;_
s8D9sP~!H
S8H;S@t=
S8H;S@tH
SafeIntExceptionHandler<class onnxruntime::OnnxRuntimeException>::SafeIntOnDivZero
SafeIntExceptionHandler<class onnxruntime::OnnxRuntimeException>::SafeIntOnOverflow
Samaritan
SAME_LOWER
SAME_UPPER
Sample echo operator.
sample_size
SampleOp
Sampling ratio should be >=0, but it was 
sampling_ratio
sampling_ratio_ >= 0
Saurashtra
SaveAttributeOrtFormat: Unsupported attribute type: 
Saved inverse standard variance used during training to speed up gradient computation.
Saved mean used during training to speed up gradient computation
saved_mean
saved_var
SaveMLValueNameIndexMapping
SaveValueInfoOrtFormat: value_info_proto for 
Saving initialized tensors.
Sbad variant access
sbetu9
SbreH
Scalar multiplier for input tensor C, the default value is 1.0.
Scalar multiplier for input tensor C.
Scalar multiplier for the product of input tensors A * B, the default value is 1.0.
Scalar multiplier for the product of input tensors A * B.
Scalar multiplier for the product of the input tensors.
Scale
scale
scale > 0
scale >= 1
Scale and bias the input image. Bias values are stored in
Scale and Zero-point must be a scalar
scale must be 1D tensor with size 
'Scale' must contain exactly 2 values - (height, width)
Scale size: (
Scale tensor.
Scale value should be greater than 0.
Scale value should be greater than or equal to 1.
scale.Shape().NumDimensions() == 1 && scale.Shape()[0] == broadcast_dim
scale_.size() == offset_.size()
ScaleBias
ScaleCount
scaledtanh
ScaledTanh
Scaler
Scales
scales
scales size should be greater than 0.
scales.size() == 2 || (scales.size() == 4 && scales[0] == 1 && scales[1] == 1)
scales.size() == 2 || (scales.size() == 4 && scales[0] == 1 && scales[1] == 1) || scales.size() == 3 || (scales.size() == 5 && scales[0] == 1 && scales[1] == 1)
scales_.size() == 4
scales_[0] == 1 && scales_[1] == 1 && scales_[2] >= 1 && scales_[3] >= 1
scales_size > 0
ScaleSize
ScaleTensor
Scaling parameter.
Scaling value
Scan 'body' subgraph outputs should all be tensors but output 
Scan input 
Scan inputs have inconsistent batch size. Previous value was 
Scan inputs have inconsistent sequence lengths. Previous value was 
scan_input_axes
scan_input_directions
scan_output_axes
scan_output_directions
Scan<8> spec does not support transpose of output. This should never be called.
Scatter
ScatterElements
ScatterND
Schema error: 
schemaVersion
score_threshold
scores
scores must be a 3D tensor.
Scores output is incorrect size. Expected:
scores.size() == static_cast<size_t>(expected_num_scores)
scores_output_data.length() >= scores_output_size
scores_tensor
SearchBitState inconsistency
SearchDFA inconsistency
SearchNFA inconsistency
SearchOnePass inconsistency
Second dimension for rois should be exactly 
Second input does not have rank 2
Second input of Gather in path 1 of position shape should be a constant with value 0.
Second input of Gather in path 2 of position shape should be a constant with value 1.
Second input of Gather should be a constant with value 1. 
Second input tensor has wrong dimension
Second set of probability coefficients. This array must be same size as prob_a.<br>If these are provided then output Z are probability estimates, otherwise they are raw scores.
Second, multiply by this.<br>Can be length of features in an [N,F] tensor or length 1, in which case it applies to all features, regardless of dimension count.<br>Must be same length as 'offset'
sED;|$@}
Seed for the hashing algorithm, unsigned 32-bit integer, default to 0.
Segment id is not valid. 
segment_embedding
segment_embedding is expected to have 2 dimensions, got 
segment_ids
select_last_index
selected_indices
separators
separators must not be empty
seq(map(int64, float))
seq(map(string, float))
seq(tensor(bool))
seq(tensor(complex128))
seq(tensor(complex64))
seq(tensor(double))
seq(tensor(float))
seq(tensor(float16))
seq(tensor(int16))
seq(tensor(int32))
seq(tensor(int64))
seq(tensor(int8))
seq(tensor(string))
seq(tensor(uint16))
seq(tensor(uint32))
seq(tensor(uint64))
seq(tensor(uint8))
seq_lengths
Sequence
Sequence is missing type entry for its element
sequence_lens
sequence_lens length of 
'sequence_lens' must have rank of 1
sequence_lens shape must be {batch_size}. Got:
sequence_type
SequenceAt
SequenceAt: Got nullptr for output tensor
SequenceConstruct
SequenceConstruct is expected to have at least 1 input.
SequenceConstruct: Got nullptr for output sequence
SequenceEmpty
SequenceEmpty: Got nullptr for output sequence
SequenceErase
SequenceErase: Got nullptr for output sequence
SequenceInsert
SequenceInsert: Got nullptr for output sequence
SequenceLength
SequenceLength: Got nullptr for output tensor
SequenceLengthsTensor
Sequences must have tensors of the same data type. There was at least one tensor in the input that was different.
Sequential execution should be enabled when using DML execution provider.
Sequential mode
SequentialExecutor::Execute
Serialization error. Graph attribute was serialized without Graph instance
Serialization of fused function body is not currently supported, 
Serialized version info is null. Invalid ORT format model.
Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED. The generated model may contain hardware and execution provider specific optimizations, and should only be used in the same environment the model was optimized for.
Session
Session Config with key [
Session has already been initialized.
Session must be initialized to create session state.
Session not initialized.
Session successfully initialized.
Session was not initialized
session.disable_prepacking
session.enable_quant_qdq
session.load_model_format
session.save_model_format
session.set_denormal_as_zero
session.use_env_allocators
session_env.EnvCreatedWithGlobalThreadPools()
session_initialization
session_logger != nullptr
session_options
session_options_.session_log_severity_level >= 0 && session_options_.session_log_severity_level <= static_cast<int>(logging::Severity::kFATAL)
session_state
session_state_ != nullptr
SessionCreation
SessionCreationStart
sessionId
SessionOptionsAppendExecutionProvider_OpenVINO: Failed to load shared library
SessionOptionsAppendExecutionProvider_Tensorrt: Failed to load shared library
SessionState for subgraphs is null. Invalid ORT format model.
SessionState is null. Invalid ORT format model.
SessionState should have saved the KernelCreateInfo prior to this running. NodeIndex:
SetEvent
SetFilePointerEx
SetFilePointerEx 
SetGraphAndCreateKernels must be called prior to GetExecutionInfo.
SetLastError
SetRestrictedErrorInfo
SetThreadAffinityMask
SetThreadDescription
setting data_type field (tensor name: 
Setting enable_profiling to 
Setting execution_mode to 
Setting graph_optimization_level to ORT_DISABLE_ALL
Setting graph_optimization_level to ORT_ENABLE_ALL
Setting graph_optimization_level to ORT_ENABLE_BASIC
Setting graph_optimization_level to ORT_ENABLE_EXTENDED
Setting inter_op_num_threads to 
Setting intra_op_num_threads to 
SetUnhandledExceptionFilter
SetupSubgraphExecutionInfo should only be called once for each subgraph.
Shape
shape
shape && tensor.Shape() == *shape
shape as a contiguous subset of the first tensor's shape. The starting of the
'shape' input must be 1D tensor of type INT64
Shape input must be a one-dimensional tensor.
shape is invalid
Shape mismatch attempting to re-use buffer. 
Shape must be 1 dimensional as it's tensor data of a shape
shape of layer norm bias tensor not expected
shape of left-hand-side argument. When broadcasting is specified, the second
shape.Size() must >=0
shape_.Size() == new_shape.Size()
shape_data_tensor.Shape().GetDims().size() == 1
shape_size == out.length()
shapeTensor->Shape().NumDimensions() == 1
ShapeToInitializer
Sharada
Shavian
SHH;SPu
sHI;sPu
sHL;sPu
short
short 
Should be unreachable if CanRemoveNodeAndMergeEdges is in sync with the logic here.
should never happen
Should not have entry in kernel create info with nullptr for kernel_def
Shouldn't be possible to have NodeArgs that haven't been handled already.
Shrink
Siddham
Sigmoid
sigmoid
signal_ndim
signed 
SignWriting
SimplifiedLayerNormalization
SimplifiedLayerNormFusion
Simplify case not handled: 
SimplifyWalker::ShortVisit called
Single dimension value must be greater than 0
single_node_compute_func should have 1 elements
Sinhala
size != 0 && (input_shape.Size() % size) == 0
size is different
Size mismatch for kernel create info node indexes and hashes. Invalid ORT format model.
Size mismatch validating subgraph inputs. Got 
Size mismatch: feed_names has 
size overflow
size_ % 2 == 1
size_ == size
size_ > 0
size_t(impl_->max_gram_length_) <= impl_->ngram_counts_.size()
size_t(impl_->min_gram_length_) <= impl_->ngram_counts_.size()
sizeof(uint32_t) == output_element_bytes
sizes
Sizes
sizes != nullptr && sizes->Shape().Size() != 0
sizes == nullptr
skip is expected to have same shape as input
SkipLayerNormalization
SkipLayerNormFusion
Sleep
SleepConditionVariableCS
SleepConditionVariableSRW
Slice
Slice does not have enough number of inputs
Slice ends is less than INT_MAX
Slice op must have either three, four or five inputs.
Slice parameter is not expected. Input index:
slice the input `data` tensor. If a negative value is passed for any of the
Sliced data tensor.
slices of `data` into an output tensor of rank q - 1 + r - indices[-1].
Slices uses `axes`, `starts` and `ends` inputs to specify the start and end
slope
SlopeTensor
snprintf() failed with return value: 
snprintf_result > 0
snprintf_result > 0 && gsl::narrow_cast<size_t>(snprintf_result) == buffer_span.size() - 1
So disabling it for this session since it uses the DML Execution Provider.
So making the execution mode sequential for this session since it uses the CUDA Execution Provider.
So making the execution mode sequential for this session since it uses the DML Execution Provider.
SOFTMAX
Softmax
Softmax attribute axis is expected to be 3
softmax_axis
SOFTMAX_ZERO
SoftmaxCPU inputs N, D and N * D must be < 
SoftmaxCrossEntropyLoss
softplus
Softplus
Softsign
softsign
Sogdian
Some nodes are not included in the topological sort, graph have a cycle.
Sora_Sompeng
sorted
source and destination buffer size mismatch
source sequence type missing element type.
Soyombo
SpaceToDepth
SPARSE
Sparse Indicies raw data size does not match expected.
Sparse initializer must have a name. This model is invalid
Sparse Initializer tensor is missing. Invalid ORT format model.
Sparse tensor (
Sparse tensor indices (
Sparse tensor initializers must have a non-empty name
Sparse tensor values (
sparse_tensor
SPARSE_TENSOR
sparse_tensor(
sparse_tensor_names_ not in sync with name_to_initial_tensor_
sparse_tensor_names_.count(tensor_name) == 0
sparse_tensor_proto
sparse_tensor_type
SPARSE_TENSORS
sparse_value
SparseTensor element type mismatch. 
Spatial
spatial
spatial_scale
spatial_scale_ > 0
SpatialScale
SpatialScaleX
SpatialScaleY
sPD9sh~!H
Specified axis to insert a dimension
Specified device is not supported.
Specified domain and type names combination does not refer to a registered opaque type
Specifies a target value that is ignored and does not contribute to the input gradient. It's an optional value.
Specify batchs of sequence words to embedding
Specify bias of conv
Specify embedding vector of char
Specify if the RNN is forward, reverse, or bidirectional. Must be one of forward (default), reverse, or bidirectional.
Specify weights of conv
spHckh3
spHckhI
Split
split
Split operator does not support 
Split should be > 0
split_scalar > 0
split_size_sum (
split_tensor->Shape().NumDimensions() == 1
SplitToSequence
SplitToSequence operator does not support 
sqeuclidean
Squeeze
squeeze_mask
squeezed
src.SizeInBytes() == dst.SizeInBytes()
st.IsOK()
Stack not empty.
Stacktrace:
start
Start CheckNodesInPathK
Start CheckNodesInPathQ
Start CheckNodesInPathV
Start FuseGptAttention
start in Range operator should be scalar like tensor, yet got shape:
Start MatchGemmSubgraph
Start MatchInputMaskSubgraph
Start MatchInputMaskSubgraphDistilBert
Start MatchPastSubgraph
Start MatchUnidirMaskSubgraph
start or end indices, it represent number of elements before the end of that
Start ValidateGemmInitializer
start_offset % span_size == 0 && real_end % span_size == 0
start_offset >= 0 && real_end >= 0 && start_offset <= real_end && real_end <= len
Starting indices of corresponding axis in `axes`
StartPadding
startpos: 
starts
Starts and axes shape mismatch
Starts and ends shape mismatch
Starts and steps shape mismatch
Starts must be a 1-D array
starts.size()=
starts_.empty() || start > ends_.back()
starts_.size() == ends_.size()
starts_.size() == ends_.size() + 1
stash_type
state (NxD), and the sequence lengths (N), computes the GRU
state not recoverable
StateSaver failed to restore state.
static 
static_activation_memory_sizes_in_byte_.find(location.name) == static_activation_memory_sizes_in_byte_.end()
static_cast<int>(activation_func_names.size()) == num_directions_ * 3
static_cast<size_t>(num_subgraph_inputs) == subgraph_inputs.size()
status.IsOK()
status.IsOK() && !impl_->ngram_counts_.empty()
status.IsOK() && !impl_->ngram_indexes_.empty()
status.IsOK() && !input_dimensions_.empty()
status.IsOK() && !pool_int64s.empty()
std::all_of(impl_->ngram_indexes_.cbegin(), impl_->ngram_indexes_.cend(), [](int64_t i) { return i >= 0; })
std::all_of(output_edges.cbegin(), output_edges.cend(), [&src_idx](const GraphEdge& edge) { return edge.src_arg_index == src_idx; })
std::all_of(split_sizes.cbegin(), split_sizes.cend(), [](int64_t value) { return value >= 0; })
std::all_of(split_sizes_.cbegin(), split_sizes_.cend(), [](int64_t value) { return value >= 0; })
std::count_if(subgraph_node.InputEdgesBegin(), subgraph_node.InputEdgesEnd(), [input_slot_index](const Node::EdgeEnd& entry) { return entry.GetDstArgIndex() == input_slot_index; }) == 0
std::exception: %hs
std::nullptr_t
std::nullptr_t 
Steepness
'step' cannot be 0
'step' value cannot be 0
steps
steps.size()=
stod argument out of range
stof argument out of range
stoll argument out of range
Stopword contains invalid utf8 chars
stopwords
storage_order
stoull argument out of range
strcmp
strcspn
stream timeout
Stride along each axis.
Stride along each spatial axis.
Stride along each spatial axis. If not present, the stride defaults is 1 along each spatial axis.
Stride along each spatial axis. If not present, the stride defaults to 1 along each axis.
Stride along each spatial axis. If not present, the stride defaults to 1 along each spatial axis.
Strides
strides
strides.size() == kernel_shape.size()
strides_.size() == kernel_shape_.size()
string
STRING
string buffer allocation failed
STRING data (tensor name: 
string enum that cases output to be lowercased/uppercases/unchanged. Valid values are "LOWER", "UPPER", "NONE". Default is "NONE"
string literal
string tensor can not have raw data
string tensor is not supported for copying between allocators
string too long
string_data
string_vocabulary
StringFileInfo
StringNormalizer
STRINGS
Strings to tokenize
strncmp
struct 
sub_result
sub_result_casted
subgraph
Subgraph
Subgraph in 'body' produces 
Subgraph input missing type.
Subgraph must have the shape set for all outputs but 
Subgraph SessionState entry for 
Subgraph SessionState for 
Subgraph SessionState was not found for '
Subgraph SessionState was not found for 'body' attribute.
SUCCESS
suffix matching is assumed. 1-dim expansion doesn't work yet.
Sum of split values not equal to 'input' dim size on 'axis'. 'axis' dim size=
Sundanese
Support vector coefficients.
support_vectors
Supported modes: `constant`(default), `reflect`, `edge`
SUVWATAUAVAWH
SUVWATAVAWH
SUVWAVH
SUVWH
SVMClassifier
SVMRegressor
SVWATAUAVAWH
SVWATAUAWH
SVWATAVAWH
SVWAVAWH
SVWAVH
sxHckpA
sXHckPA
sXHckPI
Syloti_Nagri
syntax error 
Syriac
system
system error number 
SystemError
t @8x
t 9K@
t L;K(t
t t"`$t&|(`*T,:0
t!D8B8t
T":$(
t"H9)u
t"H91u
t"I;H(t
t"L;C(t
t#D8=Be]
t$ A^
T$ D+
t$ E;
t$ E3
T$ E3
t$ E3
T$ E3
t$ E3
T$ E3
t$ E3
T$ E3
t$ E3
T$ H;
t$ H;
T$ H;
t$ H;
T$ H;
t$ H+
T$ Hi
t$ Hk
t$ I;
t$ L;
t$ Lc
t$ UAVAWH
t$ UWATAUAVH
t$ UWATAUAWH
t$ UWATAVAWH
t$ UWATH
t$ UWAUAVAWH
t$ UWAVH
t$ UWAWH
t$ WATAUAVAWH
t$ WAVAWH
t$ WH
T$$fD
t$(8\$0u@H
t$(8\$0uBH
T$(E3
t$(H;
t$(H;t$0
T$(H+
t$(I;
t$(I+
T$(L;
t$(L;
t$(L+t$ I
t$(M;
t$@A_A^A]A\_
t$@D8
T$@E3
t$@E3
T$@E3
T$@H;
t$@H;
T$@H;
t$@H;
t$@H;S
T$@H;T$Ht
t$@H;U
T$@H+
t$@H+
T$@H+
T$@H+T$8H
t$@I;
T$@Ic
t$@L!
T$@L;
t$@L+
t$@L+t$8I
t$@L9
t$`@8p t4H
T$`A9r
t$`D8p t4H
t$`E3
T$`H;
t$`H;\$H
t$`H;]
T$`H;T$ht
T$`H+
t$`H+
T$`H+
t$`Hc
T$`I;
T$`I+
t$`Ic
T$`Ic
t$`L;u
t$`L+
T$`L+
t$`L+
t$`L9
t$<;t$H
t$<E;
T$0@8j
T$08MwI
t$0A_A^_
T$0E3
t$0E3
T$0E3
T$0f9w
t$0fD
T$0H;
t$0H;
T$0H;
T$0H+
t$0H+
T$0H+
t$0H+
T$0H+
t$0H+
T$0H+
t$0H+
T$0H+
t$0H+
T$0H+
t$0H+
T$0H+
t$0H+
T$0H+T$ H
t$0H9|$Ptg
T$0Hi
t$0I;
T$0I;
T$0I+
T$0Ic
T$0L;
T$0L9
T$0M;
T$4Lc
T$89T$@
t$8A_A^A\_
t$8D8hH
T$8D8P
t$8E3
T$8E3
T$8H!\$8
t$8H;
T$8H;
t$8H;
t$8H;\$0t
t$8H;s 
T$8H+
t$8H+
T$8H+
t$8H+
T$8H+
t$8Hc
t$8I;
T$8I;
t$8I;
T$8I+
t$8IcMhH
T$8L;
t$8L;
T$8L+
t$8L9e
t$8uNH
t$9X 
t$D;X
T$DfD
t$h!L$
t$h@8{
t$HA_A^A]A\_
t$HcS
t$HE3
t$hE3
t$HE3
t$hE3
T$HE3
t$HE3
t$HfD
T$HH;
T$hH;
T$HH;
t$HH;
T$HH;
T$hH;
T$HH;
T$HH;AHt^H
T$HH;APt^H
T$HH;T$Pt
T$hH;T$pt
T$HH;T$Pt
T$hH;T$pt!
T$hH+
T$HH+
T$hH+
T$HH+
T$hH+
T$HH+
T$hH+
T$HH+
T$hH+
T$HH+
T$hH+
T$HH+
T$hH+
T$HH+
t$hH9]ht1
t$HHc
T$HI;
t$hI;
T$hI+
T$HI+
t$HIc
T$HL;
t$HL;
t$HL;|$P}
t$HL+t$@I
T$hL9\$ t
t$HM;
T$htQI
T$p+U
T$PA;V |
T$PA;V8|
t$PD8
t$pD9v 
T$pE2
t$PE3
t$pE3
T$pE3
t$pE3
t$PE3
T$PE3
t$PE3
t$PfD
T$pH;
T$PH;
T$pH;
t$pH;
T$pH;
t$PH;]
T$PH;T$Xt
T$pH;T$xt
T$PH;T$Xt
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
t$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
t$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$pH+
t$PH+
T$pH+
T$PH+
T$pH+
t$PH+
T$pH+
T$PH+
T$pH+
T$PH+
T$PH+T$@H
T$pH+T$`H
t$PH+z
t$PH9
t$pH9\$ u
T$PH9|$hH
t$pH98tUH
T$pHc
T$pI;
T$PI;
T$pI;
T$pL;
t$pL9
t$PM+
t$pN;,
T$ptTH
t$x@8p
t$X@8p
t$x@8p
t$xD8p
t$XE3
T$XE3
t$xE3
T$XE3
t$XfA
t$xH;
T$XH;
t$xH;
T$xH;
T$XH;
t$xH;
t$xH;]
T$XH;T$`
T$XH;T$`t
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
T$xH+
T$XH+
t$XH9
t$XHc
T$xI;
T$XI;
t$XI;
T$xI;
t$XL;
t$xL;f
t$xL;u
t$XL+
t$XL9(u
T$xL9\$0t
T$xM9#
t$xM9Y@
t$xu|M
t%A9X H
t%ba|H
t%LcF
t&A88t
t&I;B(t
t)I9}
t)IcV
t*ba|H
T.JDXF\H^JZFLLRD.
t/HcH0I
t/L93t
t:fA9(t4H
t;9)u
t?L9~Hs
t@33{@
t@HcS
t@L9oHs
t^vnxnzB|jxL
t_HcC
t_HcH 
t_M9o
t_proto.dims()[0] == 1
t_proto.dims_size() == 1
t`vxx
t+HcC
t<X>R@RBBDB@FFL>4
t=E;p
t>8J8t9H
t>L9~Hs
T1 != nullptr
t1ba|H
t2A;u },I
t2D9}
t2fD;"s
t4E88t/H
t4L9)
t5Ich
t6D9}
t6L;K(t+
t7H9Q
t7HcP
t8\$`tvA
t8ba|H
t8fD;
t9I;S(t.
tA8J8t<H
Tagalog
Tagbanwa
Tai_Le
Tai_Tham
Tai_Viet
Takri
Tamil
Tangut
target
Target rank must be 1 less than the input rank.
target sequence type missing element type.
Target shape may not have multiple -1 dimensions
target_class_ids.size() == target_class_nodeids.size()
target_class_ids.size() == target_class_treeids.size()
target_ids
target_nodeids
target_treeids
target_weights
targets
tb@8=gS]
TB`F.H,
tbH;D$Xs[H
tbvNxRzj|B~`
tcba|H
tCL9~Hs
tCL9oHs
tCL9vHs
tDH9>t
tdHcL$0
tDI9>t
teH9Khu
tEHcR
Telugu
template-parameter-
TempSpace allocator not found
tensor
TENSOR
Tensor after padding.
tensor can either be of element size 1 (including a scalar tensor and any
tensor can't contain negative dims
Tensor does not have external data to read from.
Tensor element type mismatch. 
tensor failed memory size calculation
Tensor initializers must have a non-empty name
Tensor is expected to contain one of the primitive data types. Got: 
Tensor must always contain primitive types. Found: 
tensor of bool, which should be a scalar.
Tensor of data to extract slices from.
tensor of int64, which should be a scalar.
Tensor of integers indicating the number of padding elements to add or remove (if negative) at the beginning and end of each axis. For 2D input tensor, it is the number of pixels. `pads` should be a 1D tensor of shape [2 * input_rank] or a 2D tensor of shape [1, 2 * input_rank]. `pads` format (1D example) should be as follow [x1_begin, x2_begin,...,x1_end, x2_end,...], where xi_begin is the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
Tensor of rank q >= 1.
Tensor of rank q-1+r-indices[-1].
Tensor of rank r >= 1.
Tensor proto with external data for value attribute is not supported.
tensor rank too small
Tensor sequence must contain only primitive types
Tensor shape cannot contain any negative value
Tensor size (
Tensor size mismatch
tensor size overflow
tensor type 
Tensor type mismatch.
Tensor type mismatch. 
Tensor types should have been handled already
tensor with rank equal to or smaller than the first tensor), or having its
Tensor with shape information must be 1 dimensional.
tensor(
tensor(bfloat16)
tensor(bool)
tensor(complex128)
tensor(complex64)
tensor(complext128)
tensor(complext64)
tensor(double)
tensor(float)
tensor(float16)
tensor(int16)
tensor(int32)
tensor(int64)
tensor(int8)
tensor(string)
tensor(string) expected as input
tensor(uint16)
tensor(uint32)
tensor(uint64)
tensor(uint8)
tensor_a_zero_point == nullptr || IsScalarOr1ElementVector(tensor_a_zero_point)
tensor_b_zero_point == nullptr || IsScalarOr1ElementVector(tensor_b_zero_point)
tensor_c_zero_point == nullptr || IsScalarOr1ElementVector(tensor_c_zero_point)
tensor_type
tensor_x_zero_point == nullptr || IsScalarOr1ElementVector(tensor_x_zero_point)
tensor_y_zero_point == nullptr || IsScalarOr1ElementVector(tensor_y_zero_point)
TensorProto ( tensor name: 
TensorProto (tensor name: 
TensorProto external data size mismatch. 
TensorProto external data size mismatch. Computed size: 
TensorrtExecutionProvider
TENSORS
ter!u13
TerminateProcess
TerminateThread
text file busy
text size: 
tf_crop_and_resize
tf_half_pixel_for_nn
tFI9]Hs
TFIDF
TfIdfVectorizer
tFM9fHs
tFM9wHs
tG8_&tGD
tg9X ubH
Thaana
than the operator set version 
t'HcV
The Alpha value in Celu formula which control the shape of the unit. The default value is 1.0.
The axis along which same quantization parameters are applied. It's optional.If it's not specified, it means per-tensor quantization and input 'x_scale' and 'x_zero_point' must be scalars.If it's specified, it means per 'axis' quantization and input 'x_scale' and 'x_zero_point' must be 1-D tensors.
The axis in which to compute the arg indices.
The axis in which to compute the arg indices. Accepted range is [-r, r-1] where r = rank(data).
The axis on which to apply normalization, -1 mean last axis.
The bias (or mask) as Tensor.
The bias input data that is a 1D tensor.
The bias value added to output. Default is 0.
The broadcasted dimensions of the inputs are incompatible
The buffer planner is not consistent with tensor buffer size, expected 
The coefficient 'a' used in cubic interpolation. Two common choice are -0.5 (in some cases of TensorFlow) and -0.75 (in PyTorch). Check out Equation (4) in https://ieeexplore.ieee.org/document/1163711 for the details. This attribute is valid only if "mode" is "cubic".
The coordinate of each dimension is transformed individually. Let's describe a case using axis x as an example.
The data type for the elements of the output tensor. Default is TensorProto::FLOAT.
The data type for the elements of the output tensor. If not specified, default is TensorProto::FLOAT.
The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto
The dimension with value zero exceeds the dimension size of the input tensor.
The distance metric to use. If a string, the distance function can be "braycurtis", "canberra", "chebyshev", "cityblock", "correlation", "cosine", "dice", "euclidean", "hamming", "jaccard", "jensenshannon", "kulsinski", "mahalanobis", "matching", "minkowski", "rogerstanimoto", "russellrao", "seuclidean", "sokalmichener", "sokalsneath", "sqeuclidean", "wminkowski", "yule".
The environment variable contained the value: 
The epsilon value to use to avoid division by zero, default is 1e-5f.
The epsilon value to use to avoid division by zero.
the expected transformation of a stochastic regularizer which randomly applies
The exponent.
The first input of CDist kernel has wrong shape: 
The first input of Range should be a constant with value 0.
The first normalization dimension: normalization will be performed along dimensions axis : rank(inputs).
The fused convolution operator schema is the same as Conv besides it includes an attribute
The FusedGemm operator schema is the same as Gemm besides it includes attributes
The given version [%u] is not supported, only version 1 to %u is supported in this build.
The graph run each iteration. It has 2+N inputs: (iteration_num, condition, loop carried dependencies...). It has 1+N+K outputs: (condition, loop carried dependencies..., scan_outputs...). Each scan_output is created by concatenating the value of the specified output value at the end of each iteration of the loop. It is an error if the dimensions or data type of these scan_outputs change across loop iterations.
The graph run each iteration. It has N+M inputs: (loop state variables..., scan_input_elts...). It has N+K outputs: (loop state variables..., scan_output_elts...). Each scan_output is created by concatenating the value of the specified scan_output_elt value at the end of each iteration of the loop. It is an error if the dimensions of these values change across loop iterations.
The id of the tree that each node is in.
The id of the tree that this node is in.
the identity or zero map to a neuron's input. The GELU nonlinearity weights
The index of the class list that each weight is for.
The index of the target that each weight is for
The inner-most 2 dimensions must have the same size (mat_w:
The innermost dims should have the same dim value to parse the diagonal elements
The input data as Tensor.
The input is not evenly splittable
The input must be a map from strings or integers to either strings or a numeric type. The key and value types cannot be the same.
The input must be a tensor of a numeric type or string. The output will be of the same tensor type.
The input must be a tensor of a numeric type, and of of shape [N,C] or [C]. In the latter case, it will be treated as [1,C]
The input must be a tensor of a numeric type, either [C] or [N,C].
The input must be a tensor of a numeric type.
The input must be a tensor of a numeric type. The output will be of the same tensor type.
The input must be a tensor of strings or integers, either [N,C] or [C].
The input must be an integer map to either string or float.
The input shape override's size does not match the input tensor's shape size
The input tensor cannot be reshaped to the requested shape. Input shape:
The input type is a tensor of any shape.
The input type must be a tensor of a numeric type, either [C] or [N,C].
The input type must be a tensor of a numeric type, either [N,C] or [C]. The output type will be of the same tensor type and shape.
The input type must be a tensor of a numeric type.
The input type must be a tensor of integers or strings, of any shape.
The integers of the map. This sequence must be the same length as the 'cats_strings' sequence.
The kernel type, one of 'LINEAR,' 'POLY,' 'RBF,' 'SIGMOID'.
The keys when using int keys.<br>One and only one of the 'classlabels_*' attributes must be defined.
The keys when using string keys.<br>One and only one of the 'classlabels_*' attributes must be defined.
The lambd value for the Shrink formulation. Default is 0.5.
The mean of the normal distribution.
The model contains a 16-bit input (
The model has input '
The Model Proto has already been checked for the ORT config json.
The Model Proto hasn't been checked for the ORT config json.
The most inner dimension in boxes must have 4 data.
The new GRU hidden state calculated by this op.
The node id of each weight
The node is not placed on any Execution Provider. 
The node kind, that is, the comparison to make at the node. There is no comparison to make at a leaf node.<br>One of 'BRANCH_LEQ', 'BRANCH_LT', 'BRANCH_GTE', 'BRANCH_GT', 'BRANCH_EQ', 'BRANCH_NEQ', 'LEAF'
The normal input data.
The number of batch dimensions. The gather of indexing starts from dimension of data[batch_dims:]
The number of channels to sum over
The number of support vectors.
The only subscript labels allowed are lower-cased letters (a-z) and upper-cased letters (A-Z)
The only supported values for the environment variable 
The order of the normalization, only 1 or 2 are supported.
The ORT format model version [
The output is a 1-D tensor of string, float, or integer.
The output is a tensor of strings or integers. Its shape will be the same as the input shape.
The output type will be a tensor of strings or integers, and will have the same shape as the input.
The output type will be a tensor of strings or integers, depending on which of the the classlabels_* attributes is used.
The output type will be a tensor of strings or integers, depending on which of the the classlabels_* attributes is used. Its size will match the bactch size of the input.
The output will be a sequence of string or integer maps to float.
The output will be a tensor of strings or integers.
The output will be a tensor of the value type of the input map. It's shape will be [1,C], where C is the length of the input dictionary.
The output.
The override dims are not compatible with given tensor's shape. 
The pads attribute cannot be used simultaneously with auto_pad attribute
The parent of shape nodes are expected to be input_ids.
The parent of two shape nodes are expected to be input_ids.
The pooling method. Two modes are supported: 'avg' and 'max'. Default is 'avg'.
The pooling method. Two modes are supported: 'bilinear' and 'nearest'. Default is 'bilinear'.
The previous GRU hidden state.
The rank of input tensor must be >= axis
The rank of the input must match permutation size for Transpose
The ratio of random dropout
the same ordering as the image pixel format.
The scale along height dimension. It takes value greater than or equal to 1.
The scale along width dimension. It takes value greater than or equal to 1.
The scale array along each dimension. It takes value greater than or equal to 1. The number of elements of 'scales' should be the same as the rank of input 'X'.
The scale to apply.
The scaled hyperbolic tangent values of the input tensor computed element-wise
The second input of CDist kernel has wrong shape: 
The sequence output for the hidden is optional if 0. Default 0.
The shape of the convolution kernel. If not present, should be inferred from input W.
The shape of the convolution kernel. If not present, should be inferred from input 'w'.
The shape of the output can be explicitly set which will cause pads values to be auto generated. If output_shape is specified pads values are ignored. See doc for details for equations to generate pads
The shape of the output tensor.
The size of each input in the input list
The size of the kernel along each axis.
The standard deviation of the normal distribution.
The starting indexes of 1-grams, 2-grams, and so on in pool. It is useful when determining the boundary between two consecutive collections of n-grams. For example, if ngram_counts is [0, 17, 36], the first index (zero-based) of 1-gram/2-gram/3-gram in pool are 0/17/36. This format is essentially identical to CSR (or CSC) sparse matrix format, and we choose to use this due to its popularity.
The storage order of the tensor. 0 is row major, and 1 is column major.
The string used to pad output tensors when the tokens extracted doesn't match the maximum number of tokens found. If start/end markers are needed, padding will appear outside the markers.
The strings of the map. This sequence must be the same length as the 'cats_int64s' sequence
The subgraph in 'body' requires 
the tensor elementwise.
the tensor to be tiled using Tile OP must be atleast 1 dimensional
The third input of Range should be a constant with value 1.
The timestep for this operation.
The total number of regression targets, 1 if not defined.
The total number of targets.
The type of the output tensor is integer.
The underlying implementation is MurmurHash3_x86_32 generating low latency 32bits hash suitable for implementing lookup tables, Bloom filters, count min sketch or feature hashing.
The value for the elements of the output tensor in sparse format.
The value for the elements of the output tensor.
The value for the sole element for the scalar, float32, output tensor.
The value for the sole element for the scalar, int64, output tensor.
The value for the sole element for the scalar, UTF-8 string, output tensor.
The values for the elements for the 1D, float32, output tensor.
The values for the elements for the 1D, int64, output tensor.
The values for the elements for the 1D, UTF-8 string, output tensor.
The weight for each target
The weight for the class in class_id.
The weighting criteria. It can be one of "TF" (term frequency), "IDF" (inverse document frequency), and "TFIDF" (the combination of TF and IDF)
The WordConvEmbedding takes in a batch of sequence words and embed each word to a vector.
The zero-padding added to one side of the output. This is also called adjs/adjustment in some frameworks.
then optionally start the crop offset by the left/top border amounts.
then_branch
then_branch and else_branch produce different number of outputs. 
then_feeds_fetches_manager_ && else_feeds_fetches_manager_
there are multiple cases for the number of outputs, which we list below:
There must be one (and only one) dynamic typed input to the custom op. Its type info at runtime will be used to infer the type info of this dynamic typed output which is required for the success of the model loading step. More than one dynamic typed inputs are currently not supported as differing types at runtime means the output type cannot be inferred without which model loading cannot proceed.
There was a problem acquiring temporary memory allocator in Einsum op
There's no data transfer registered for copying tensors from 
tHH9Y
this API does not support strings
This attribute describes how to transform the coordinate in the resized tensor to the coordinate in the original tensor. <br/>
This is an invalid model. At top level graph without matching NodeArg that subgraph consumes. Name=
This is an invalid model. Error in Node:
This is an invalid model. Error: Duplicate definition of name (
This is an invalid model. Error: the graph is not acyclic.
This is an invalid model. Error: two nodes with same node name (
This is an invalid model. Failed to find NodeArg in all parent graphs. Name=
This is an invalid model. Graph output (
This is an invalid model. Model input (
This is an invalid model. Node (
This is an invalid model. Tensor does not have type information.
This is an invalid model. The sum of input arg count is not equal to size of input defs in node (
This is an invalid model. Type Error: Type '
This may prevent some of the graph optimizations, like const folding. 
This operator applies convolution to word from left to right with window equal to conv_window_size and stride to 1.Take word 'example' for example, with conv_window_size equal to 2, conv is applied to [ex],[xa], [am], [mp]...If not provide, use the first dimension of conv kernal shape.
This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.
This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).
This optimizer does not support external data for unidirectional mask right now
This session already contains a loaded model.
This session has already been initialized.
This session will use the allocator registered with the environment.
This transformer is already registered 
This ZeroCopyOutputStream doesn't support aliasing. Reaching here usually means a ZeroCopyOutputStream implementation bug.
this->base_values_.size() == predictions.size()
thisProto->value_case() == TypeProto::ValueCase::kMapType
thisProto->value_case() == TypeProto::ValueCase::kSequenceType
thisProto->value_case() == TypeProto::ValueCase::kSparseTensorType
thisProto->value_case() == TypeProto::ValueCase::kTensorType
Three interpolation modes: nearest (default), linear and cubic. The "linear" mode includes linear interpolation for 1D tensor and N-linear interpolation for N-D tensor (for example, bilinear interpolation for 2D tensor). The "cubic" mode includes cubic interpolation for 1D tensor and N-cubic interpolation for N-D tensor (for example, bicubic interpolation for 2D tensor).
Three modes: `constant`(default) - pads with a given constant value, `reflect` - pads with the reflection of the vector mirrored on the first and last values of the vector along each axis, `edge` - pads with the edge values of array
Three modes: constant(default), reflect, edge
threshold
Threshold
Threshold value
thresholdedrelu
ThresholdedRelu
Thresholds to do the splitting on for each node.
THV(TrR
THV(TzR
THV(X~THV(X
t'I;B(t
Tibetan
Tifinagh
Tile doesn't have an implementation yet for the type: 
Tile doesn't support string type yet
tiles
time_axis
time_axis < 2
time_axis and batch_axis must have different values but both are 
timed out
Tirhuta
tJba|H
tKba|H
t'L99t
t'L9A
tlvdxBz~vL~*
tMD9axtGH
tmHcS
tmp_cats_int64s.empty() || tmp_cats_strings.empty()
to flatten the input shape to (N x C*D1*D2 ..*Dn) before a BatchNormalization Op.
TO_FLOAT
TO_INT64
TO_STRING
tOA9N(
tokenexp
Tokenized strings
Tokenizer
tOLc@(H
too many files open
too many files open in system
too many links
too many symbolic link levels
Total allocated bytes: 
Total fused Attention node count: 
Total fused reshape node count: 
Total Gelu Approximation (FastGelu) node count: 
totalRunDuration
totalRuns
tQL9P uK
TraceAllocation for ort_value_idx=
TraceFree for ort_value_idx=
trailing \
training_mode
training_mode of Dropout must be a scalar.
TrainingStepTensor
TransA
transA
transB
TransB
transform_targets
Translation
Transpose
Transpose not implemented for empty tensors.
Transpose of element size not supported in this build. Size=
transposed
TransposeMatMul
trD9n(
trD9w(
Tree id for each node.
TreeEnsembleClassifier
TreeEnsembleRegressor
trHcP0I
tried creating tensor with negative value in shape
tried to allocate 0 bytes
Tried to allocate without valid type information, ort_value index=
Trilu
tRLcY
true literal
TryAcquireSRWLockExclusive
Trying to allocate memory for unused optional inputs/outputs
Trying to get a SparseTensor, but got: 
Trying to get a Tensor, but got: 
Trying to get a TensorSeq, but got: 
Trying to register schema with name 
tTH;>v
tTL9P uN
ttx2t|x2z
tUba|H
tvfA;E
tvfD9D$8tnH
twH!]
twH+A
tWH9]
twIcF
tWIcJ ;H uJ
Two interpolation modes: nearest (default), and linear (including bilinear, trilinear, etc)
Two interpolation modes: nearest(default), bilinear
two paths share the same shape
tXHcs
tXIcF
TXVpXXZB\TXL`,b
type 
type != nullptr
type == dtype_
type case mismatch. existing=
type case unsupported. existing=
Type Error: Data in initializer '
Type Error: Shape of initializer 
Type Error: Type (
Type Error: Type parameter (
type field and data field mismatch in attribute 
Type mismatch. Current=
type must be number, but is 
Type of reduction to apply to loss: none, sum, mean (default). 'none': the output is the loss for each sample. 'sum': the output will be summed. 'mean': the sum of the output will be divided by the sum of applied weights.
Type of reduction to apply to loss: none, sum, mean(default). 'none': no reduction will be applied, 'sum': the output will be summed. 'mean': the sum of the output will be divided by the number of elements in the output.
type used for stash mean/inv_std_var
Type:
type_error
type_id_counter == 1
type_proto is not of type map!
type_proto is not of type sequence!
TypeAndShapeInferenceFunction implementation incomplete: this line should never be reached.
tzD9{(
tZH9K
U A+U0A
U H+U
u HcA<H
u HcK0H
U I+U
U IcE
u!bA-@
u!H;U
u!L9gxu
u"L;!
u#D9R
u%A:^
u%A8p
u%HcK(
u&H!B
U(A+U0A
u(bA-@
U(H;U0t:H
U(H+U H
U(H9x 
u(L9m
u)9X u$I
u)A8p
u*D8n
u*H99u
u.H;U
U:0^E
u?K94
U@H9}XH
u@L!}
u@L+u8I
u@M;K
u[McA
u]@83
u^A;p0t
U`H;|$pt
U`I+UXH
u{H9V
u}D9c(
u}I;B(twH
u}I;S(twH
u+I!K
u<!T$@
u<!T$P
u=H;y r7H
u=L9|$hu6L;d$pu/H
u>D!D$ 
u>fD;
u>H!U
u>H;Q
u0D;s
U0H+U(H
u0HcT$pL
u1</w
u1D97u,E
u4I9}(
u7H;P
u8@8p
u8A9}
u-8Cst(H
u8D9R
u8HcK0H
uaD8g
UATAUAVAWH
UATAVH
UATAWH
UAUAVH
UAVAWH
uD9g(
uD9o(
ue8Y$t
uED8U
uEHc/H
uFL;B r@I
ufLc7M
ug9_(
Ugaritic
UhH;Upt
UhH+UXH
uHL;B rBI
uIIc@
uint16
uint32
uint64
uint64_data
uint8
uj9_(
uj9{(
uJHcN$L
ulD9g(
ulD9k(t
umD9c(
umD9o(
umE9n(
Unable to find compiled kernel hash for node '
Unable to find node 
Unable to get an allocator
Unable to serialize model as it contains compiled nodes. Please disable any execution providers which generate compiled nodes.
Unactivated gate outputs from forget, update, and output gates, pre-activation.
unD9o(
UNDEFINED
Undefined tensor type!
unexpected 
Unexpected CAST_TO value of 
Unexpected data type for Clip input of 
Unexpected data type for Clip 'min' input of 
Unexpected element size of 
unexpected error
unexpected failure
Unexpected input data type. Actual: (
Unexpected mode of 
Unexpected mode:
Unexpected NORMALIZE value of 
Unexpected op in Regexp::Equal: 
Unexpected opcode in IsMatch: 
Unexpected opcode in short circuit: 
Unexpected opcode: 
Unexpected special state in RunStateOnByte
Unexpected value for 'add_second_class' of 
uNH9Y
unhandled 
Unhandled 
unhandled opcode: 
UnhandledExceptionFilter
unidir mask is not constant
unidir mask shape not expected
unidirectional
unimplemented activation: 
union 
Unique
UNKNOWN
unknown
Unknown aggregation function in TreeEnsemble.
Unknown AutoPadType String
Unknown Category and zeros = 0.
Unknown encoding 
unknown error
Unknown error during EndProfiling()
Unknown exception
Unknown exception in Load()
Unknown exception was caught by catch-all handler.
unknown kernel type
Unknown model file format version.
unknown round: 
Unknown tensor type of 
unknown token
unknown_dim == -1
Unloading DSO 
unordered_map/set too long
UnpackTensor: the pre-allocate size does not match the size in proto
UnpackTensor: the pre-allocated size does not match the raw data size, expected 
Unrecognized attribute: 
Unrecognized data_type (tensor name: 
Unrecognized type value case (value_info name: 
unsigned 
Unsqueeze
unsqueeze_after_gather axes value not expected
UnsqueezeElimination
UnsqueezeElimination cannot remove node 
UnsqueezeElimination_
Unsupported attribute value type of 
Unsupported AutoPad Type.
Unsupported convolution size.
Unsupported data type of 
Unsupported data type: 
unsupported data type: 
Unsupported 'dtype' value: 
Unsupported execution_mode value in ORT config: 
Unsupported graph_optimization_level value in ORT config: 
Unsupported input data type of 
Unsupported input element type of 
Unsupported input type
Unsupported level
Unsupported level 
Unsupported non-raw-data data type!
Unsupported output datatype with size: 
Unsupported output type of 
Unsupported pooling size : 
Unsupported pooling size.
Unsupported sparse tensor data type of 
Unsupported tensor element type in the input: 
Unsupported tensor type of 
Unsupported type:
Unsupported type: 
Unsupported value attribute datatype with size: 
Unsupported value attribute datatype: 
Unsupported value for enable_profiling option: 
Unsupported value for inter_op_num_threads: 
Unsupported value for intra_op_num_threads: 
Unsupported version '
Unsupported X type: 
Unsupported Y type: 
unused
uoD8g
uOH;S
uoH91}BH
upD9c(
updates
updates shape: 
updates tensor should have shape equal to indices.shape[:-1] + data.shape[indices.shape[-1]:]. 
UpdatesTensor
uPH!;H!{
upHcU
uPK94
upper
UPPER
Upper boundary of the output values.
Upsample
Upsample operator
Upsample: input shape needs to be at least a single dimension.
Upsample: input tensor's dimension does not match the scales.
Upsample: input/output value is nullptr
Upsample: input/output value's dimension mismatch
Upsample: unexpected mode
uqD9o(
uQH!u'H
urD9{(
urD9c(
urD9k(
urD9o(
urD9w(
uS;r0t
us9_(
use_approximation
UseClipThreshold
usefp16
Using an input in multiple nodes on different devices is not supported currently. Input:
Using global/env threadpools since use_per_session_threads_ is false
Using user supplied initializer with name (
uSM9&tN
USVATAUAVAWH
USVWATAUAVAWH
USVWATAVAWH
USVWAVAWH
USVWAVH
USVWH
UTCReplace_AppSessionGuid
utD9c(
utE9f(
uTHcW
utils::HasDataType(t_proto)
utils::HasElemType(thisProto->sequence_type())
utils::HasElemType(thisProto->sparse_tensor_type())
utils::HasElemType(thisProto->tensor_type())
utils::HasKeyType(thisProto->map_type())
utils::HasName(sparse_tensor)
utils::IsPrimitiveDataType<T>(dtype_)
uuD9c(
uuHcF 9G ul
UUUUUUU
uV@83
uv9_(
UVAVH
uVD8n
uVJ9|
UVWATAUAVAW
UVWATAUAVAWH
UVWATAVH
UVWAUAVH
UVWAUAWH
UVWAVAWH
UvZH+
UWATAUAVH
UWATAUAWH
UWATAVAWH
UWATH
UWAUAVAWH
UWAUH
UWAVH
UWAWH
uwM9f 
uX@8p
ux9_(
uXD8{
uxD9k(
uxD9o(
UxH+UpH
UxI+UpH
uXK9<
uXK94
UXL;m`A
uYH!D$PH
uzD8g
uzD9o(
V &". X"" r":$T&X J(T*L b,
V +V0
v >= 0 && static_cast<uint64_t>(v) <= std::numeric_limits<size_t>::max()
V A+V0A
V H+V
V"@$"&
V(+V0
V(A+V0A
V(H;V0tH
V(H;V0tI
V(H+V H
v:fD;
V@H+V8H
V@I+V8H
v_final_and_scan_outputs
v_initial
v_reshape initializer value is not expected
v0I;Q
V2Z.\$^pZ"V\<a
V4`8.:,4,<P>.@,
V8H+V0H
V8pB&F
VALID
valid
ValidateUnidirMask returns false for mask_slice
Validating no unexpected access using an invalid node_index. Got:
Value
value
value at X[t][n] >= seqLengths[n].
Value of alpha
Value of alpha default to 0.2
Value of alpha.
Value of attribute 
Value of beta
Value of beta default to 0.5
Value of beta.
value of k must not be negative
Value tensor should be a 1D tensor of size 1 with the same type as that of the input tensor
value too large
Value type is not supported yet: 
Value used for extrapolation, when applicable. Default is 0.0f. 
Value(s) to change to
Value(s) to change to.
value_float
value_floats
value_info
value_int
value_ints
value_proto != nullptr
value_string
value_strings
value_tensor->DataType() == data_type && value_tensor->Shape().Size() == 1
value_type
value_type != nullptr
ValueDataType
ValueDelta
Values
values
Values greater than this are mapped to 1, others to 0.
values in 'axes' are beyond the bounds of the computed output shape
values is 
values of data_type '
values.size() == static_cast<size_t>(attr->floats_size())
values.size() == static_cast<size_t>(attr->ints_size())
values_floats
values_int64s
values_strings
ValueStart
ValuesTensor
VarFileInfo
variadic_output
Variance
VarianceTensor
VbbeH
vb'vb'v
vd<[u)
vector<bool> too long
vector<T> too long
vectors_per_class
VFXNZ8X \1
vhL;vh
VHX(V
Violation of the requirment that all input tensors must have the same data type.
virtual 
VirtualAlloc
VirtualFree
VitisAIExecutionProvider
VIWEF
void 
volatile
volatile 
VpH;Vxt
VPX,Z2XV\&^6\P^4`TbX\JdTfL\bh
VS_VERSION_INFO
VUUUUUUUE
VUUUUUUUH
Vv2t8
VWATAUAVAWH
VWATAUAVH
VWATAVAWH
VWATH
VWAUAVAWH
VWAVH
VWAWH
VXI+VPH
vzfD9x
w @8q t.H
w A+w0A
W A+W0A
W H+W
W HcG
W(+W0
W(A+W0A
w(D+w0D
W(H;W0t
w(H9|$Pu
W@8o`
W@H;WHt
w_^[]
w_scale
W_scale
W_zero_point
w_zero_point
W=E6*
W8H;W@t
wA\A]A^_^[]
WaitForSingleObject
WaitForSingleObjectEx
WakeAllConditionVariable
WakeConditionVariable
Walk NULL
Warang_Citi
WARNING
Warning: Checker does not support models with experimental ops: 
Warning: Shape inference does not support
Warning: Unsupported operator 
'was added but does not exist. 
WATAUAVAWD
WATAUAVAWH
WATAUAVAWL
WATAVAWH
WATAVH
WAVAWH
wchar_t
wcsnlen
We do not expect duplicate registration of types for: 
We do not support type [
We don't expect custom allocators for non-tensor types, so a shape is mandatory here.
weight
weight and zero_point pair is expected to have same type.
Weight buffer for initializer '
weight must be a scalar or 1D tensor of size 1
Weight point must be constant
Weight rank must be 1.
weight zero point must be a scalar or 1D tensor of size 1.
Weight zero point must be zero
weight_gather
weight_gather_sum
weight_gather_temp
weight_gather_temp_1
weight_scale
weight_zero_point
weights
Weights of the intercepts, if used.
Weights of the model(s).
weights.quant_para_
WeightTensor
wGu(H
WHcF(;E(u
When computing the output of the hidden gate, apply the linear transformation before multiplying by the output of the reset gate.
When coordinate_transformation_mode is "tf_crop_and_resize" and x_original is outside the range [0, length_original - 1], this value is used as the corresponding output value. Default is 0.0f.
When the session is not configured to use per session threadpools, the env must be created with the the CreateEnvWithGlobalThreadPools API.
Where
where const not matched.
Whether A should be transposed
Whether A should be transposed on the last two dimensions before doing multiplication
Whether B should be transposed
Whether B should be transposed on the last two dimensions before doing multiplication
Whether C should be broadcasted
Whether every token can only attend to previous tokens. Default value is 0.
Whether include pad pixels when calculating values for the edges. Default is 0, doesn't count include pad.
Whether the operator should behave like fmod (default=0 meaning it will do integer mods); Set this to 1 to force fmod treatment
Whether to return the elements in sorted order.
Whether to return the top-K largest or smallest elements.
Whether to select the last index or the first index if the {name} appears in multiple indices, default is False (first index).
Whether to use ceil or floor (default) to compute the output shape.
WhI;Wpt
Which axis to concat on
Which axis to concat on.  Default value is 1.
Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs)..
Which axis to concat on. Accepted range in `[-r, r - 1]`, where `r` is the rank of input tensors. When `new_axis` is 1, accepted range is `[-r - 1, r]`. 
Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1]
Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
Which axis to scatter on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1]
Which axis to scatter on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
Which axis to split on
Which axis to split on. 
Which axis to split on. A negative value means counting dimensions from the back. Accepted range is [-rank, rank-1] where r = rank(input).
Which axis to split on. A negative value means counting dimensions from the back. Accepted range is [-rank, rank-1].
which does not equal the specified override of 
while parsing 
wi9_(
WideCharToMultiByte
width_scale
WilError_03
window
Windows.Foundation.Collections.IIterator`1<Windows.Foundation.Collections.IKeyValuePair`2<String, UInt32>>
Windows.Foundation.Collections.IKeyValuePair`2<String, UInt32>
Windows.Foundation.Collections.IMap`2<String, UInt32>
Windows::AI::MachineLearning::Adapter::SessionRegisterCustomRegistry
WindowsCreateString
WindowsDeleteString
WindowsDeleteStringBuffer
WindowsDuplicateString
WindowsGetStringRawBuffer
WindowSize
WindowsPreallocateStringBuffer
WindowsPromoteStringBuffer
WinmlRuleTransformer
with a fixed dimension size 
with activation 
Word embedding shape not expected.
word_embedding
word_embedding and position_embedding shall have same dimension 1
word_embedding and segment_embedding shall have same dimension 1
word_embedding is expected to have 2 dimensions, got 
word_embedding should have 2 dimensions and dimension size is known.
WordConvEmbedding
WpD;W`|zA
wRH9Q
Writing profiler data to file 
wrong protocol type
wstr != wconv_error
WtHv:A
WXH;WXuBL
wxHcGpH
X != nullptr
X and mask should have the same shape
x ATAUAVAWLc
x ATAVAW
x ATAVAWH
x ATAVAWL
x AUAVAWH
x AVAWH
x AVAWL
x AVH
x AVI
x AVL
x AVM
x AWH
x b"t$
x D9c(t
X D9c(t
X dims is empty.
X f"t$^&h(t*P,h.t0P2f468`:l<H>(<
X h"r$X&h(r*X,t.
x H+x
X H+X
X H9{
X I+X
X input is required!
X L"F$B F&P
X L9c
x L9o
x L9w
X num_dims does not match W num_dims.
x UATAUAVAWH
x UATAUH
x UATAVH
x UATAWH
x UAUAVH
x UAVAWH
X UVWATAUAVAWH
X UVWAVAWH
X UVWH
X X"B$@&`*T.l2f6f:f><@fDfHfLfPLTdXh\d`ldlh<j
X Z"0&^(p*H,(*
x"j$>((*B.
x&L;O(} M
x&L;W(} M
x(D$@
x(L$0
x(T$ 
x)T$`H
x*;CP}%L
X*Z@\N^V\8`M
X.d244
x_^[]
X_^[]
X_^][
X_alpha
X_Div
X_Exp
X_Log
X_Max
X_Max_Adjusted
X_Min
X_Min_Adjusted
x_original = (x_resized + 0.5) / scale - 0.5, <br/>
x_original = (x_resized + 0.5) / scale, <br/>
x_original = length_resized > 1 ? (x_resized + 0.5) / scale - 0.5 : 0, <br/>
x_original = length_resized > 1 ? start_x * (length_original - 1) + x_resized * (end_x - start_x) * (length_original - 1) / (length_resized - 1) : 0.5 * (start_x + end_x) * (length_original - 1).
x_original = x_resized * (length_original - 1) / (length_resized - 1), <br/>
x_original = x_resized / scale, <br/>
x_ptr != nullptr
X_Range
X_rank == 4
X_ReduceMax
X_ReduceSum
X_scale
x_scale
X_shape.NumDimensions() == 4
X_squared
X_Sub
X_variance
x_zero_point
X_zero_point
x_zero_point must be null or 1D tensor with size 
x_zero_point must be null or a scalar or 1D tensor or size 1.
X->Shape().GetDims().size() == output_dims.size()
X->Shape().NumDimensions() == 4
x69\$0v
XA^_^[
xA_A^_^[]
XA_A^A]A\_^[]
xA_A^A]A\_^[]
XA_A^A]A\_^][
xA_A^A]A\_^][
XA_A^A]A\_^][
xFzF|6z ~
xH;A u
xH;C8tu
xH|(~txH|(~fxH|(~
xHz(|
xI;N u
xI;N8u
XL$0f
xLcB`L;D$ht
X-lr8
XlZd\N^~ZXb*d
XPLc}
XTVzd\^
Y != nullptr
Y = softmax(scores + bias)) with simple broadcast on bias. Intended to specialize softmax(scores + additive_mask) commonly found in transformer models.
y_scale
Y_scale
Y_zero_point
y_zero_point
y0H+y(H
y8D+y(M
yC:\apilot\agent\_work\6\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMaxUnpool.cpp
YE9gp
yet this opset 
yxI;]
z 9s(u
z H+z
z I;X
z R",$j&Q
z"u 3
Z(6*,,8*
z(H;]
z(u&3
z)u'3
z*u(3
Z^\F^X`>\JbVZ,
z<u:A
z=u;I
z>u<H
z1u/I;
z3u1I;
z5u3H
z5u3I;
z6u4H
Z8L;u
Zanabazar_Square
zAu?H
zero_point == nullptr || std::all_of(zero_point, zero_point + x_zero_point->Shape().Size(), [](int32_t zp) { return zp == 0; })
zero_point_ptr == nullptr || (zero_point_ptr->Shape().NumDimensions() == 1 && zero_point_ptr->Shape()[0] == broadcast_dim)
zero_point_ptr == nullptr || IsScalarOr1ElementVector(zero_point_ptr)
Zeropoint
ZeroPointTensor
zeros
ZH^(`
ZH^(`|Z<b
ZH^(`bZb^H`(^VZb^H`(^
ZH^(`JZ
ZH^(`jZh^H`(^^Z`^H`(^
ZH^(`xZ>b
zH|(z
zHI;zPu
ZipMap
Zipmap does not support empty dim count
Zipmap only supports 1D or 2D input tensors
zlHn(pllHn(p
zoU L
z-u+L
Zv_DmlExecutionProvider_
zXuVH
