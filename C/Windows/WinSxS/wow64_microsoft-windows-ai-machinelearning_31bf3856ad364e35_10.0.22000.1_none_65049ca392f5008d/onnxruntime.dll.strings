      
            
              
                    [
                    [0.0, 0.0, 1.0, 1.2],
                    [0.0, 0.0, 2.3, 3.4],
                    [0.0, 0.0, 4.5, 5.7],
                    [1.0, 1.2],
                    [2.3, 3.4],
                    [4.5, 5.7],
                    ]
                    ],
                input_x = [2, 1, 1, 3, 4, 3]
                output_counts = [1, 2, 2, 1]
                output_idx = [0, 1, 1, 2, 3, 2]
                output_uniques = [2, 1, 3, 4]
              Example:
              Finds all the unique values (deduped list) present in the given input tensor.
              of each value of the input in 'uniques'.
              sorted in the same order that they occur in the input.
              The first output tensor 'uniques' contains all of the unique elements of the input,
              The second output tensor 'idx' is the same size as the input and it contains the index
              The third output tensor 'counts' contains the count of each element of 'uniques' in the input.
              This operator returns 3 outputs.
            data = [
            Example:
            Given `data` tensor, pads, mode, and value.
            Insert 0 pads to the beginning of the second dimension.
            output = [
            pads = [0, 2, 0, 0]
        (possibly with aspect ratio change) to a common output size specified by crop_height and crop_width.
        <requestedExecutionLevel level='asInvoker' uiAccess='false' />
        a fixed size = [crop_height, crop_width]. The result is a 4-D tensor [num_boxes, crop_height, crop_width, depth].
        Extracts crops from the input image tensor and resizes them using bilinear sampling or nearest neighbor sampling
        Returns a tensor with crops from the input image at positions defined at the bounding box locations in boxes.
        The cropped boxes are all resized (with bilinear or nearest neighbor interpolation) to
        The resizing is corner aligned.
      [1, 2, 3, 4],
      [2, 3, 4],
      [5, 6, 7, 8],
      [5, 6, 7],
      </requestedPrivileges>
      <requestedPrivileges>
      All other elements in the matrix are set to zero.
      If k = 0, the triangular part on and above/below the main diagonal is retained.
      If upper is set to false, a positive k retains the lower triangular matrix including k diagonals above
      If upper is set to true, a positive k retains the upper triangular matrix excluding k diagonals above
      of the elements on and above the given diagonal (k). The lower triangular part consists of elements on and below the diagonal.
      Returns the upper or lower triangular part of a 2-D matrix, or batches of 2-D matrices. If the attribute "upper" is set to true,
      the main diagonal. A negative k value excludes as many diagonals below the main diagonal.
      the main diagonal. A negative k value includes as many diagonals below the main diagonal.
      the upper triangular matrix is retained. Lower triangular matrix is retained otherwise. Default value for upper is true.
      Trilu takes one input tensor of shape [*, N, M], where * is zero or more batch dimensions. The upper triangular part consists
    </security>
    <security>
   ' 0 8 ; > A C G Q S S U ^ 
  "separators" is a list of strings which are regular expressions. "tokenexp" is a single regular expression.
  ["Hello World", "I love computer science !"] whose shape is [2],
  </trustInfo>
  <trustInfo xmlns="urn:schemas-microsoft-com:asm.v3">
 ' 0 C E Q S ^ } ~ 
  09AZ__az ~09AZ__az!~
  axes = [0, 1]
  AZazAZ
  data    = [[[0,1],[2,3]],[[4,5],[6,7]]]
  data    = [[0,1],[2,3]]
  data = [
  ends = [-1, 1000]
  ends = [2, 3]
  If input is
  If input is ["Hello", "World"],
  If the maximum number of tokens found per input string is D, the output shape would be [N, C, D] when input shape is [N, C].
  indices = [[[0,1]],[[1,0]]]
  indices = [[0,0],[1,1]]
  indices = [[0,1],[1,0]]
  indices = [[1],[0]]
  Let's assume "separators" is [" "] and consider an example.
  Let's consider another example to illustrate the effect of setting "mark" to true.
  output  = [[[2,3]],[[4,5]]]
  output  = [[2,3],[0,1]]
  output  = [[2,3],[4,5]]
  output  = [0,3]
  result = [
  shape(A) = (2, 3, 4, 5), shape(B) = (,), i.e. B is a scalar tensor
  shape(A) = (2, 3, 4, 5), shape(B) = (1, 1), i.e. B is an 1-element tensor
  shape(A) = (2, 3, 4, 5), shape(B) = (2), with axis=0
  shape(A) = (2, 3, 4, 5), shape(B) = (3, 4), with axis=1
  shape(A) = (2, 3, 4, 5), shape(B) = (4, 5)
  shape(A) = (2, 3, 4, 5), shape(B) = (5,)
  Similarly, if input shape is [C] then the output should be [C, D]. Tokenizer has two different operation modes.
  starts = [0, 1]
  starts = [1, 0]
  The first mode is selected when "tokenexp" is not set and "separators" is set. If "tokenexp" is set and "separators" is not set,
  the second mode will be used. The first mode breaks each input string into tokens by matching and removing separators.
  then the corresponding output would be [0x02, "Hello", "World", 0x03].
  then the output would be
  This implies that if mark is true, [C]/[N, C] - input's output shape becomes [C, D+2]/[N, C, D+2].
  Tokenizer divides each string in X into a vector of strings along the last axis. Allowed input shapes are [C] and [N, C].
 != mat_h:
 ( ) / / _ _ 
 (domain: 
 (falsenode).
 (node 
 (node_version: 
 (truenode).
 (weights).
 ) is different from what is supplied (
 * . ` d f o 
 *!+!2!2!N!N!`!
 *0-0
 / / _ _ 
 : : 
 @33{@33{@33{@33{@
 ["I", "love", "computer", "science", "!"]]
 [["Hello", "World", padvalue, padvalue, padvalue],
 [seqno=
 [truncated]
 _^[]
 |,},o-o-/./.
 0%0,010@0L0
 0@0`0
 0F0^0
 9 9 
 already exist.
 and 
 and function is trying to import opset version 
 and the number of 
 and then launches another search starting from the first remained character after the first matched token.
 and type (
' appeared multiple times.
 appears in graph inputs and will not be treated as constant value/weight. 
 are '0' and '1'. 
 are '0' and '1'. The environment variable contained the value: 
 arena_extend_strategy 
 arg 
 as it still has output edges.
 at line 
 at pos=
 attribtues in LabelEncoder 
' attribute.
 Attribute:
 Axis is 
 axis value 
 Axis=
 BackUp() can only be called after Next().
 Base Class Array'
 Base Class Descriptor at (
 bins of max chunk size 
 but 
 but different TensorProto.
 but expected 
 but has 
 but has rank 
 but input '
 but is of type: 
 but ngram_indexes size: 
 but subgraphs produce 
 but the actually size is: 
 but the node in the model has the following type (
 but usage of initializer in graph expects 
 bytes for 
 bytes were able to be read.
 bytes.
 cannot be safely updated to 
 Can't back up over more bytes than were returned by the last call to Next().
 Char embedding size: 
 char_embedding_size attribute: 
 Class Hierarchy Descriptor'
 Complete Object Locator'
 conv filter size: 
 conv kernal size 1: 
 Conv kernal size 2 : 
 conv_window_size attribute: 
 d f p t ~ 
 delete
 delete[]
 DeviceId:
 did not match batch size of 
 did not return correct number of compiled functions
 did not.
' dimension 
 dimension != 
 Dimension=
 dimensions or more but input had shape of 
 dimensions.
 does not align with rank of input data: 
 does not contain a graph.
 does not match existing output type of 
 does not match the equation indices.
 does not match. 
 does not specify a valid type.
 does not.
' doesn't support memcpy 
 E E } } 
 elements.
 else=
 embedding_size attribute: 
 Encountered following errors: (
 entries which doesn't match the number of fetches the frame was initialized with of 
 exceeded maximum protobuf size of 2GB: 
 Expected 
 Expected DENSE or SPARSE
 Expected std::map<int64_t, float> or std::map<int64_t, std::string>
 expected to be a registered ONNX type
 expected to be of type: 
 expected to have rank 
 expected to have rank >
 expected to have sequence type
 expected to have tensor type
 expected to have type but instead is null
 Expected TO_FLOAT, TO_STRING or TO_INT64
 Expected:
 Expected: 
 ExplicitInputs:
 fail, errcode = 
 fail: unexpected end
 failed
' failed
 failed.
 failed. Error:
 failed. File doesn't exist
 failed. Only 
 failed:
 failed: 
 filter_number: 
 for attribute 
 for domain 
 For each input string, the second mode searches matches of "tokenexp" and each match will be a token in Y.
 for input shape 
 for operator 
 for SizeFromDimension. Tensor has 
 for the following indices
 for the same domain
 found!
 Found:
 Got:
 Got: 
 got: 
 Graph may not conform to the ONNX spec and contain initializers that are not graph inputs.
 group: 
 has already been loaded.
 has already been registered.
 has batch size of 
' has been deprecated since version 
' has been used as graph input names multiple times.
' has been used as output names multiple times.
 has Compile error: 
' has element type 
 has length of 
 has mismatched dimensions of 
 has unknown expected type
 hh]>
 However the types are incompatible.
 hxZ>
 id: 
 If "separators" contains a single empty string, the Tokenizer will enter into character tokenezation mode. This means all strings
 If no match found, this operator will remove the first character from the remained string and do another search.
 Implicit input name 
 ImplicitInputs:
 imports opset version 
 in AddToThreadq
 in ComputeFirstByte
 in 'Constant' node '
' in custom op '
 in initializer but not in graph input
 in KernelRegistryManager
 in node 
 in node (
 in one of the subgraphs.
 in step
 in the same dimension
 in the supported version range
 in tree 
 Index:
 index: 
 inferred output shape:
 inferred=
 initializer name is not unique
 Input dim value: 
 input dimensions instead
 Input shape=
 input with name 
 Input=
 inputs and requires 
 inputs but 
 inputs but Scan was only given 
 inputs but subgraph has 
 inputs. Either provide all subgraph inputs, or just the required inputs.
 inputs. Found:
' instead of '
 into softmax(input + bias)
 is already there.
 is defined.
 is deprecated in domain_version of 
' is expected to have field 'floats'
' is expected to have field 'g'
' is expected to have field 'graphs'
' is expected to have field 'ints'
' is expected to have field 'sparse_tensor'
' is expected to have field 'strings'
' is expected to have field 't'
' is expected to have field 'tensors'
 is expected to have type: 
 is greater than input dim=
 is incompatible in the dimension 
 is invalid for a tensor of rank 
 is invalid.
 is marked single but has an empty string in the graph
 is missing type info.
' is missing.
 is missing. Invalid ORT format model.
 is NaN
 is not a registered function/op
 is not a valid date
 is not a valid day
 is not a valid year
 is not compatible with 
 is not currently registered or supported
 is not expected to be of type sparse tensor.
 is not expected to be of type tensor sequence.
 is not expected to be of type tensor.
 is not found
' is not found
 is not implemented
 is not in (0, 
 is not in valid range [-
 is not output of any previous nodes.
 is not supported
 is not supported currently
 is not supported in this function
 is not supported yet
 is not the same as this node's index:
 is not used by any node.
 is null. Invalid ORT format model.
 is null. Type info is expected.
 is out of bounds
 is outside range.
 is repeated.
 is required but missing.
 is required to be non-empty.
 is smaller than requested bytes of 
 is till opset 
 is under development and support for this is limited. The operator schemas and or other functionality could possibly change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain 
 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain 
 is unrecognized, acceptable values are TF,IDF,TFIDF
 is used by node 
 kernel channels: 
 kernel is not supported in 
 kernel not found in 
 kernel start version: 
 kernel_end_version: 
 kernel_shape: 
 known by the checker.
 Left shape override: 
 line 
 Max:
 max_dead_bytes_per_chunk: 
 memory limit: 
 MemoryType:
 message of type "
 Microsoft Corporation. All rights reserved.
' Model is invalid.
 model may run depending upon legacy support of some older opset version operators.
 model uses the deprecated attribute
 models with experimental operators: 
 must be 1 instead of 
 must be either specified in graph inputs or graph initializers.
 must be of equal size
 must be within the inclusive range [
 must have shape {
 new[]
 No matching hash for 
 node '
 node. Name:'
 node_version: 
 noexcept
' not found
 not found.
 not in allowed input sizes.
 not in allowed output sizes.
 not in range [min=
 not specified
 Note that the input at most can have two axes, so 3-D and higher dimension are not supported.
 Num entries in 'split' (must equal number of outputs) was 
 num_input_channels: 
 NumOutputs=
' of 
' of input parameter (
 of node 
' of node: 
 Operating System
' OpType:
 OrtAllocatorType:
 OrtMemType:
 Output dim value: 
 outputs but Scan expects 
 outputs so the subgraph requires 
 outputs which doesn't match the subgraph's 
 outputs.
 outputs. Expected 
 P!_!
 Parameter to BackUp() can't be negative.
 Please fix either the inputs or the model.
 Provider: [
 Requested shape:
 returned nullptr
 Right shape override: 
 Right shape: 
 rounded_bytes:
 row[
 rows[
 should be of integer type and specify a type.
' should be stored in field '
 should only appear once.
 should specify a shape
 size=
' source:
 source=
 sparse initializer name is not unique across initializers and sparse_initializers
 specified. It should be either avg or max
 specified. It should be either bilinear or nearest
' Status Message: 
 Sum of sizes in 'split' (must equal size of selected axis) was 
 sum of split values=
 t!;;r
 t$;;r
 target:
 Target=
 target=
 Tensor=
 The matching of "tokenexp" is conducted greedily (i.e., a match should be as long as possible).
' the model will use the latest encountered initializer
 The production MUST never overflow. The accumulation may overflow if and only if in 32 bits.
 then=
 This op has been implemented only for the following types (
 This operator searches for the first match starting from the beginning of the considered string,
 This procedure will be repeated until reaching the end of the considered string.
 Type Descriptor'
 unknown
 Value=
 values, but NNZ is 
 Version mismatch.
 version: 
 volatile
 vs. 
 was 
' was 
 was false.
 was not
 was not a tensor.
 was not found. Defaulting to a rank 1 shape of {0}.
 were provided
 where as the model imports opset version 
 whose shape is [2, 5] because you can find at most 5 tokens per input string.
 will be broken part into individual characters.
 Windows
 with domain_version of 
 with following configs: initial_chunk_size_bytes: 
!#!%!%!'!'!)!)!.!.!:!;!@!D!J!M!O!O!
!#!%!%!'!'!)!)!.!.!:!;!J!J!L!M!O!O!
!#%'**,,./:;?@\\
!#%*,/:;?@[]__{{}}
!$!$!&!&!(!(!*!-!/!9!<!?!E!I!N!N!
!$!$!&!&!(!(!*!-!0!3!>!?!E!E!
!%!'!)!,!1!3!M!O!_!
!&$@$J$`$
!(it.GetName().empty())
!/!/!4!4!9!9!<!=!F!I!N!N!
!/:@[`{~09AZaz
!@!D!K!K!
!0,^,a,a,e,f,h,h,j,j,l,l,q,q,s,t,v,{,
!0?0^0
!010q0
!020T0e0n0t0
!0D0N0
!c->in_use() && (c->bin_num != kInvalidBinNum)
!c->in_use() && (c->bin_num == kInvalidBinNum)
!c1->in_use() && !c2->in_use()
!char_tokenezation_ || mincharnum_ < 2
!chunk->in_use()
!coefficients_.empty()
!current_parallel_section
!found
!graph.GetInitializedTensor(new_initializer.name(), existing)
!has_axes || attr_axes_.size() == attr_starts_.size()
!helper.HaveTwoTensorInputs()
!input_tensor.IsDataType<std::string>()
!is_concrete_shape_
!mask || mask->Shape() == X_shape
!normalize_
!pool_strings.empty()
!scale_.empty()
!separators.empty()
!sw.empty()
!This program cannot be run in DOS mode.
!tokenexp.empty()
!using_counters_
!utils::HasExternalData(t_proto)
" #!#|#|#
" : "
" because it is missing required fields: 
"0H0a0z0
"2'2W2\2{3
"args" : {
"dur" :
"name" :"
"ph" : "X",
"pid" :
"tid" :
"ts" :
#"#(#+#{#}#
#&$@$J$
#(#+#&$@$J$
#)#)#h'h'j'j'l'l'n'n'p'p'r'r't't'
#)#*#h'u'
#*#*#i'i'k'k'm'm'o'o'q'q's's'u'u'
#0\0o0|0
#010R0n0
#050]0b0h0n0
#050n0
#0a0v0
#0c0u0
#0U0s0
#hl[>
$$++<>^^``||~~
$_^[]
$0)0/050;0h0
$0.0A0_0
$000P0\0|0
$0f0s0
$hX}=
$RWVjpZjoY
$u;8H
$uf8Q
$VRWjpZ
$VWjoY
$VWQjpZ
-%-'-'-----
%.0Lf
%~3a*
-%-'-'-----0-g-o-o-
%3M3r3
-%-'-'-----A
%b %d %H : %M : %S %Y
%d / %m / %y
%H : %M
%H : %M : %S
%hs!%p: 
%hs(%d) tid(%x) %08X %ws
%hs(%u)\%hs!%p: 
%I : %M : %S %p
%m / %d / %y
%o&o&
%Y-%m-%d_%H-%M-%S
&!&!e
&0\1`1d1h1l1p1t1x1|1
&0+00080I0U0b0n0w0|0
&0Z0g0
&h $?
&hxi@
&n&p&g'
( ( $
(% dA
(%PgA
(%pgA
(([[{{
(){}[]*+?|.^$\
(?HaveMatch:%d)
(?-m:$)
(?-m:^)
(-`gA
(~YV@
(0/0D0Y0s0
(0<0V0h0
(030=0C0O0X0]0
(0D1p1
(0H0R0\0f0m0
(0H0T0t0
(5pdA
(caller: %p) 
(cannot determine missing fields for lite message)
(d$ f
(D$pf
(default) or 
(float, default 0.5) the ratio of random dropout
(fmod == 0) || (fmod == 1)
(inputs_.size() - 1) == i
(int, default 0) if nonzero, run dropout in test mode where the output is simply Y = X.
(items % ngram_size == 0)
(local_source >= source) && (local_source < source + num_blocks * blocksize)
(local_source >= source) && (local_source < source + num_blocks * num_elts_in_block)
(local_source >= source) && (local_source < source + num_blocks)
(local_source >= source) && (local_source < source + sizeof(T) * num_blocks)
(name: 
(null)
(op_type:
(Optional) A scalar or rank 1 tensor containing a single value to be filled if the mode chosen is `constant` (by default it is 0.0).
(Optional) Axis along which one-hot representation in added. Default: axis=-1. axis=-1 means that the additional dimension will be inserted as the innermost/last dimension in the output tensor.
(Optional) Axis along which one-hot representation in added. Default: axis=-1. axis=-1 means that the additional dimension will be inserted as the innermost/last dimension in the output tensor. Negative value means counting dimensions from the back. Accepted range is [-r-1, r] where r = rank(indices).
(Optional) Axis along which to take slices. If not specified, input is flattened before elements being selected.
(Optional) Axis along which to take slices. If not specified, input is flattened before elements being selected. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
(Optional) Index of the diagonal to be populated with ones. Default is 0. If T2 is the output, this op sets T2[i, i+k] = 1. k = 0 populates the main diagonal, k > 0 populates an upper diagonal,  and k < 0 populates a lower diagonal.
(Optional) Seed to the random generator, if not specified we will auto generate one.
(Optional) Specify which axis is batch axis. Must be one of 1 (default), or 0.
(Optional) Specify which axis is time axis. Must be one of 0 (default), or 1.
(Optional) The axis of the dequantizing dimension of the input tensor. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input)
(Optional) The axis of the quantization dimension of the input tensor. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input)
(Optional) The data type for the elements of the output tensor, if not specified, we will use int32.
(Optional) The data type for the elements of the output tensor, if not specified, we will use the data type of the input tensor.
(Optional) The data type for the elements of the output tensor. If not specified,the data type of the input tensor T1 is used. If input tensor T1 is also notspecified, then type defaults to 'float'.
(Optional) The data type of the tensors in the output sequence. The default type is 'float'.
(Optional) The dimension to apply unique. If not specified, the unique elements of the flattened input are returned. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
(Optional) The value of the output elements.Should be a one-element tensor. If not specified, it defaults to a tensor of value 0 and datatype float32
(Optional) Whether map negative infinity to true. Default to 1 so that negative infinity induces true. Set this attribute to 0 if negative infinity should be mapped to false.
(Optional) Whether map positive infinity to true. Default to 1 so that positive infinity induces true. Set this attribute to 0 if positive infinity should be mapped to false.
(Optional) Whether to sort the unique elements in ascending order before returning as output. Must be one of 0, or 1 (default).
(outputs_.size() - 1) == i
(-PgA
(Tensor<T>) where the affine function, y = alpha * x + beta,
(Tensor<T>) where the softplus function, y = alpha * ln(exp(beta * x) + 1), is applied to
) != (
) != new size (
) != split_dim_size (
) + (
) + bottom_border (
) + bottomBorder (
) + right_border (
) + rightBorder (
) + scale[0] (
) + scale[1] (
) + scale_[0] (
) + scale_[1] (
) and node 
) and outputs (
) and the split dimension of the input (
) are not at boundary of span with size:
) attribute (
) because the ORT planned memory location device 
) bound to different types (
) dimensions are not positive.
) does not exist in the graph.
) does not have type information set by parent node.
) does not have type information.
) does not match expected type (
) does not match number of inputdimensions values (
) does not match the data size(
) does not match the number of channels (
) first dimension size does not equal NNZ.
) for attribute 'axis'
) for tensor of length:
) from file 
) has 
) has input size 
) has more inputs (
) has more outputs (
) has no index values.
) has output size 
) has zero input and zero output.
) in node (
) in op definition.
) in proto
) index value at position [
) input arg (
) is 0-element but contains data!
) is invalid.
) is not equal to number of scan inputs (
) is not equal to number of scan outputs (
) is not equal to the existing dim value (
) is not equal to the existing rank value (
) is required but not specified.
) is stored externally and should not have data field.
) is stored externally but doesn't have a location.
) must have a dense-rank > 0
) must have INT64 type.
) must have rank 1 or 2.
) must have rank 1.
) must have the same length. 
) needs to be greater than or equal to the left_border (
) needs to be greater than or equal to the leftBorder (
) needs to be greater than or equal to the top_border (
) needs to be greater than or equal to the topBorder (
) of node (
) of operator (
) of output arg (
) Op (
) or 1
) output arg (
) second dimension size does not match rank of tensor.
) should be stored in 
) should contain one and only one value field.
) should not be stored in raw_data field
) should not contain more than one value field.
) should refer to attribute in parent node.
) specified for sequence of size (
) than declared (
) to UNDEFINED is not allowed
) type inference failed
) vs (
)".".$.$.&.&.(.(.B.B.
)#.#.%.%.'.'.).).
)) , expected: (
))]]}}
), but the current device does not support 16-bit float.
), input tensor data type (
)0/0B0Q0
)0/0B0Z0q0
)0=0]0q0
)8_^[]
)D$`;T$
)D$`v
)D$p;|$
)'s input 
)'s output 
)s+v+
-*0/0
*0+D+G+L+)
-*0-0
*030}0
*0W0\0i0o0
*0z0=1G1\1q1
*L$,SPR
*out_size >= 0
, am_attn_size}, Got:
, aw_attn_size}. Got:
, block in memory pattern size is: 
, but it doesn't exist or is not accessible.
, but it is already registered from file 
, but it its domain is not
, but it its version is higher
, but it its version is not 
, column 
, data shape: 
, error code: 
, external_data.length: 
, fall back to default allocation behavior
, Got 
, got 
, indices shape: 
', location: 
, max=
, node name: 
, requested shape:
, type: 
,.,`,`,b,d,g,g,i,i,k,k,m,p,r,r,u,u,~,
,.,0,^,@
,.,0,^,`,
,<ellipsis>
,020:0\0h0|0
,3L3T3\3h3
,p-p-
,SVWj
. . .P
'. 0 == forward. 1 == reverse.
. batch_size=
. bin_num:
. Can't constant fold 
. Dimension 0 is 
. Do you have duplicated calls to SessionState::AddInitializedTensor function?
. Execution Provider must generate unique names across the entire model.
. Falling back to lenient merge.
. Ignoring allocator from 
. Index:
. Input rank=
. Input tensor rank was 
. Invalid ORT format model.
. It can only be 
'. It is no longer used by any node.
'. It is not used by any node and should be removed from the model.
. Must be 0 or 1
'. Must be one of 'forward', 'reverse', or 'bidirectional'.
. No schema registered for this operator.
. Num args is 
. Output tensor rank was 
. Please, fix your model.
. Shape:
. shape=
. The shape is: 
'. Valid values are 'LEFT' or 'RIGHT'.
. Validate usage of dim_value (values should be > 0) and dim_param (all values with the same string should equate to the same size) in shapes in the model.
.!.!.@k
.*...0.9.<.?.A.A.C.N.
...0.N.
.:.;.@.@.
.?AU?$Abs@_J@functors@onnxruntime@@
.?AU?$Abs@_K@functors@onnxruntime@@
.?AU?$Abs@C@functors@onnxruntime@@
.?AU?$Abs@E@functors@onnxruntime@@
.?AU?$Abs@F@functors@onnxruntime@@
.?AU?$Abs@G@functors@onnxruntime@@
.?AU?$Abs@H@functors@onnxruntime@@
.?AU?$Abs@I@functors@onnxruntime@@
.?AU?$Abs@M@functors@onnxruntime@@
.?AU?$Abs@N@functors@onnxruntime@@
.?AU?$Ceil@M@functors@onnxruntime@@
.?AU?$ChainInterfaces@UIMLOperatorKernelCreationContextPrivate@@UIMLOperatorKernelCreationContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@WRL@Microsoft@@
.?AU?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@WRL@Microsoft@@
.?AU?$ChainInterfaces@UIMLOperatorSupportQueryContextPrivate@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@VNil@Details@WRL@Microsoft@@V4567@V4567@V4567@V4567@V4567@V4567@@WRL@Microsoft@@
.?AU?$default_delete@VBFCArena@onnxruntime@@@std@@
.?AU?$default_delete@VIAllocator@onnxruntime@@@std@@
.?AU?$default_delete@VModel@onnxruntime@@@std@@
.?AU?$ElementWiseRangedTransform@_J@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@_K@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@C@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@E@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@F@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@G@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@H@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@I@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@M@functors@onnxruntime@@
.?AU?$ElementWiseRangedTransform@N@functors@onnxruntime@@
.?AU?$Elu@M@functors@onnxruntime@@
.?AU?$Exp@M@functors@onnxruntime@@
.?AU?$Exp@N@functors@onnxruntime@@
.?AU?$Floor@M@functors@onnxruntime@@
.?AU?$HardSigmoid@M@functors@onnxruntime@@
.?AU?$heap_implements@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@@impl@winrt@@
.?AU?$heap_implements@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@@impl@winrt@@
.?AU?$heap_implements@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@@impl@winrt@@
.?AU?$implements@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@3@U?$IMapView@Uhstring@winrt@@I@5673@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@5673@@winrt@@
.?AU?$implements@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@@winrt@@
.?AU?$implements@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@@winrt@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$$V@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00UIMLOperatorRegistryPrivate@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00UIMLOperatorTensorShapeDescription@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00UIMLOperatorTypeInferenceContext@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00UIWinmlExecutionProvider@Adapter@MachineLearning@AI@Windows@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@U?$ChainInterfaces@UIMLOperatorKernelCreationContextPrivate@@UIMLOperatorKernelCreationContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTensorShapeDescription@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTypeInferenceContext@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@U?$ChainInterfaces@UIMLOperatorSupportQueryContextPrivate@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@VNil@Details@WRL@Microsoft@@V4567@V4567@V4567@V4567@V4567@V4567@@23@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIExecutionProvider@Dml@@UIWinmlExecutionProvider@Adapter@MachineLearning@AI@Windows@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIMLOperatorKernel@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIMLOperatorKernelContext@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIMLOperatorKernelFactory@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIMLOperatorRegistry@@UIMLOperatorRegistryPrivate@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIMLOperatorShapeInferrer@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIMLOperatorSupportQueryPrivate@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIMLOperatorTensor@@@Details@WRL@Microsoft@@
.?AU?$ImplementsHelper@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$0A@UIUnknown@@@Details@WRL@Microsoft@@
.?AU?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@
.?AU?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@
.?AU?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@
.?AU?$LeakyRelu@M@functors@onnxruntime@@
.?AU?$Log@M@functors@onnxruntime@@
.?AU?$Log@N@functors@onnxruntime@@
.?AU?$map_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@Uhstring@3@I@winrt@@
.?AU?$map_view_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@Uhstring@3@IUcollection_version@23@@winrt@@
.?AU?$MaxPool1DTask@C@onnxruntime@@
.?AU?$MaxPool1DTask@E@onnxruntime@@
.?AU?$MaxPool1DTask@M@onnxruntime@@
.?AU?$MaxPool1DTask@N@onnxruntime@@
.?AU?$MaxPool2DTask@C@onnxruntime@@
.?AU?$MaxPool2DTask@E@onnxruntime@@
.?AU?$MaxPool2DTask@M@onnxruntime@@
.?AU?$MaxPool2DTask@N@onnxruntime@@
.?AU?$MaxPool3DTask@C@onnxruntime@@
.?AU?$MaxPool3DTask@E@onnxruntime@@
.?AU?$MaxPool3DTask@M@onnxruntime@@
.?AU?$MaxPool3DTask@N@onnxruntime@@
.?AU?$MaxpoolWithMask1DTask@M@contrib@onnxruntime@@
.?AU?$MaxpoolWithMask2DTask@M@contrib@onnxruntime@@
.?AU?$MaxpoolWithMask3DTask@M@contrib@onnxruntime@@
.?AU?$Neg@_J@functors@onnxruntime@@
.?AU?$Neg@C@functors@onnxruntime@@
.?AU?$Neg@H@functors@onnxruntime@@
.?AU?$Neg@M@functors@onnxruntime@@
.?AU?$Neg@N@functors@onnxruntime@@
.?AU?$ParametricSoftplus@M@functors@onnxruntime@@
.?AU?$Pool1DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AU?$Pool2DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AU?$Pool3DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AU?$Powx@M@functors@onnxruntime@@
.?AU?$produce@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@@impl@winrt@@
.?AU?$produce@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@3@@impl@winrt@@
.?AU?$produce@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMapView@Uhstring@winrt@@I@Collections@Foundation@Windows@3@@impl@winrt@@
.?AU?$produce@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@@impl@winrt@@
.?AU?$produce@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@@impl@winrt@@
.?AU?$produce_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$produce_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$produce_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMapView@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$produce_base@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$produce_base@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMapView@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer_convert@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer_convert@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer_convert@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMapView@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer_convert@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producer_convert@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@X@impl@winrt@@
.?AU?$producers_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@V?$tuple@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@U?$IMapView@Uhstring@winrt@@I@2345@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@2345@@std@@@impl@winrt@@
.?AU?$producers_base@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@V?$tuple@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@std@@@impl@winrt@@
.?AU?$producers_base@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@V?$tuple@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@winrt@@@std@@@impl@winrt@@
.?AU?$Reciprocal@M@functors@onnxruntime@@
.?AU?$Reciprocal@N@functors@onnxruntime@@
.?AU?$Relu@M@functors@onnxruntime@@
.?AU?$Relu@N@functors@onnxruntime@@
.?AU?$root_implements@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@3@U?$IMapView@Uhstring@winrt@@I@5673@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@5673@@impl@winrt@@
.?AU?$root_implements@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@@impl@winrt@@
.?AU?$root_implements@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@3@@impl@winrt@@
.?AU?$root_implements_composable_inner@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@$0A@@impl@winrt@@
.?AU?$root_implements_composable_inner@U?$key_value_pair@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@impl@winrt@@$0A@@impl@winrt@@
.?AU?$root_implements_composable_inner@Uiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@$0A@@impl@winrt@@
.?AU?$root_implements_composing_outer@$0A@@impl@winrt@@
.?AU?$RuntimeClassFlags@$03@WRL@Microsoft@@
.?AU?$ScaledTanh@M@functors@onnxruntime@@
.?AU?$Selu@M@functors@onnxruntime@@
.?AU?$Sigmoid@M@functors@onnxruntime@@
.?AU?$Sigmoid@N@functors@onnxruntime@@
.?AU?$Softplus@M@functors@onnxruntime@@
.?AU?$Softsign@M@functors@onnxruntime@@
.?AU?$Sqrt@M@functors@onnxruntime@@
.?AU?$Sqrt@N@functors@onnxruntime@@
.?AU?$Tanh@M@functors@onnxruntime@@
.?AU?$Tanh@N@functors@onnxruntime@@
.?AU?$ThresholdedRelu@M@functors@onnxruntime@@
.?AU?$weak_ref@$00@impl@winrt@@
.?AU?$weak_source@$00@impl@winrt@@
.?AU?$weak_source_producer@$00@impl@winrt@@
.?AU_Crt_new_delete@std@@
.?AUBFloat16@onnxruntime@@
.?AUcollection_version@impl@winrt@@
.?AUContainer@?$InternalMetadataWithArenaBase@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VInternalMetadataWithArenaLite@internal@protobuf@google@@@internal@protobuf@google@@
.?AUCpuProviderFactory@onnxruntime@@
.?AUctype_base@std@@
.?AUCustomOpKernel@onnxruntime@@
.?AUDMLProviderFactory@onnxruntime@@
.?AUException@Ort@@
.?AUFunctionBodyBuildContext@onnx@@
.?AUFunctionBodyBuildContextImpl@onnx@@
.?AUhresult_access_denied@winrt@@
.?AUhresult_canceled@winrt@@
.?AUhresult_changed_state@winrt@@
.?AUhresult_class_not_available@winrt@@
.?AUhresult_error@winrt@@
.?AUhresult_illegal_delegate_assignment@winrt@@
.?AUhresult_illegal_method_call@winrt@@
.?AUhresult_illegal_state_change@winrt@@
.?AUhresult_invalid_argument@winrt@@
.?AUhresult_no_interface@winrt@@
.?AUhresult_not_implemented@winrt@@
.?AUhresult_out_of_bounds@winrt@@
.?AUhresult_wrong_thread@winrt@@
.?AUIExecutionProvider@Dml@@
.?AUIExecutionProviderFactory@onnxruntime@@
.?AUIMarshal@impl@winrt@@
.?AUIMLOperatorAttributes@@
.?AUIMLOperatorAttributes1@@
.?AUIMLOperatorKernel@@
.?AUIMLOperatorKernelContext@@
.?AUIMLOperatorKernelCreationContext@@
.?AUIMLOperatorKernelCreationContextPrivate@@
.?AUIMLOperatorKernelFactory@@
.?AUIMLOperatorRegistry@@
.?AUIMLOperatorRegistryPrivate@@
.?AUIMLOperatorShapeInferenceContext@@
.?AUIMLOperatorShapeInferenceContextPrivate@@
.?AUIMLOperatorShapeInferrer@@
.?AUIMLOperatorSupportQueryContextPrivate@@
.?AUIMLOperatorSupportQueryPrivate@@
.?AUIMLOperatorTensor@@
.?AUIMLOperatorTensorShapeDescription@@
.?AUIMLOperatorTypeInferenceContext@@
.?AUInferenceContext@onnx@@
.?AUInferenceContextImpl@shape_inference@onnx@@
.?AUinput_adapter_protocol@detail@nlohmann@@
.?AUiterator@?$iterable_base@U?$input_map@Uhstring@winrt@@IV?$map@Uhstring@winrt@@IU?$less@Uhstring@winrt@@@std@@V?$allocator@U?$pair@$$CBUhstring@winrt@@I@std@@@4@@std@@@impl@winrt@@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@3@Ucollection_version@23@@winrt@@
.?AUiterator_type@collection_version@impl@winrt@@
.?AUIUnknown@@
.?AUIWeakReference@impl@winrt@@
.?AUIWeakReferenceSource@impl@winrt@@
.?AUIWinmlExecutionProvider@Adapter@MachineLearning@AI@Windows@@
.?AUmarshaler@?1??make_marshaler@impl@winrt@@YAHPAUtype@?$abi@UIUnknown@Foundation@Windows@winrt@@X@23@PAPAX@Z@
.?AUmessages_base@std@@
.?AUMLFloat16@onnxruntime@@
.?AUmoney_base@std@@
.?AUNode__EdgeIterator@onnxruntime@@
.?AUNode__EdgeIterator_Impl@onnxruntime@@
.?AUNode__NodeIterator@onnxruntime@@
.?AUNode__NodeIterator_Impl@onnxruntime@@
.?AUNodeAttributes_Iterator@onnxruntime@@
.?AUNodeAttributes_Iterator_Impl@onnxruntime@@
.?AUNodeCompare@onnxruntime@@
.?AUnull_type@onnxruntime@@
.?AUOpKernel_Translator@onnxruntime@@
.?AUOrtAllocator@@
.?AUOrtAllocatorImpl@@
.?AUOrtDefaultAllocator@@
.?AUPad@onnxruntime@@
.?AUPriorityNodeCompare@onnxruntime@@
.?AUProvider_TensorShapeProto_Dimension_Iterator@onnxruntime@@
.?AUProvider_TensorShapeProto_Dimension_Iterator_Impl@onnxruntime@@
.?AUProviderHost@onnxruntime@@
.?AUProviderHostImpl@onnxruntime@@
.?AUSequentialExecutionPlan@onnxruntime@@
.?AUSlice1@onnxruntime@@
.?AUSlice10@onnxruntime@@
.?AUTile@onnxruntime@@
.?AUtime_base@std@@
.?AUtype@?$abi@U?$IIterable@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@winrt@@X@impl@winrt@@
.?AUtype@?$abi@U?$IIterator@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@@Collections@Foundation@Windows@winrt@@X@impl@winrt@@
.?AUtype@?$abi@U?$IKeyValuePair@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@X@impl@winrt@@
.?AUtype@?$abi@U?$IMap@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@X@impl@winrt@@
.?AUtype@?$abi@U?$IMapView@Uhstring@winrt@@I@Collections@Foundation@Windows@winrt@@X@impl@winrt@@
.?AUtype@?$abi@UIInspectable@Foundation@Windows@winrt@@X@impl@winrt@@
.?AUtype@?$abi@UIUnknown@Foundation@Windows@winrt@@X@impl@winrt@@
.?AV?$_ExceptionPtr_static@Vbad_alloc@std@@@?A0x327206fc@@
.?AV?$_ExceptionPtr_static@Vbad_exception@std@@@?A0x327206fc@@
.?AV?$_Func_base@_JM_N@std@@
.?AV?$_Func_base@_N$$V@std@@
.?AV?$_Func_base@_NABUFunctionBodyBuildContext@onnx@@ABVOpSchema@2@AAVFunctionProto@2@@std@@
.?AV?$_Func_base@_NABVNode@onnxruntime@@@std@@
.?AV?$_Func_base@_NH@std@@
.?AV?$_Func_base@_NI@std@@
.?AV?$_Func_base@_NPBVNode@onnxruntime@@PBV12@@std@@
.?AV?$_Func_base@HPAUComputeContext@onnxruntime@@PAPAX@std@@
.?AV?$_Func_base@MM@std@@
.?AV?$_Func_base@MMMM@std@@
.?AV?$_Func_base@MMMMMMM@std@@
.?AV?$_Func_base@PAVOpKernel@onnxruntime@@ABVOpKernelInfo@2@@std@@
.?AV?$_Func_base@V?$ComPtr@UIMLOperatorTensor@@@WRL@Microsoft@@I@std@@
.?AV?$_Func_base@V?$OrtValueTensorSlicer@$$CBUOrtValue@@@onnxruntime@@ABUOrtValue@@_J_J@std@@
.?AV?$_Func_base@V?$OrtValueTensorSlicer@UOrtValue@@@onnxruntime@@AAUOrtValue@@_J_J@std@@
.?AV?$_Func_base@V?$shared_ptr@VIAllocator@onnxruntime@@@std@@HW4OrtMemType@@@std@@
.?AV?$_Func_base@V?$unique_ptr@VIAllocator@onnxruntime@@U?$default_delete@VIAllocator@onnxruntime@@@std@@@std@@F@std@@
.?AV?$_Func_base@V?$unique_ptr@VTensor@onnxruntime@@U?$default_delete@VTensor@onnxruntime@@@std@@@std@@ABVTensor@onnxruntime@@_J_JV?$shared_ptr@VIAllocator@onnxruntime@@@2@PAX@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@$$V@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@AAV?$shared_ptr@VModel@onnxruntime@@@std@@@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@AAVGraph@3@@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@ABV?$vector@IV?$allocator@I@std@@@std@@ABVTensor@3@AAV63@@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@ABV?$vector@IV?$allocator@I@std@@@std@@ABVTensor@3@AAV63@PBVTensorShape@3@PAX@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@ABVNode@3@AAVGraph@3@ABV?$vector@PBVTypeProto@onnx@@V?$allocator@PBVTypeProto@onnx@@@std@@@std@@AAV67@ABUResolveOptions@53@@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@ABVNodeArg@3@I@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@ABVTensor@3@AAV43@PAX@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@ABVTensorShape@3@ABUOrtMemoryInfo@@AAUOrtValue@@AA_N@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@HABUOrtValue@@ABUOrtCallback@3@_N@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@PAXAAV?$vector@UOrtValue@@V?$allocator@UOrtValue@@@std@@@std@@PAXI@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@PAXI@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@PAXPBUOrtApi@@PAUOrtKernelContext@@@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@PB_JPB_JPA_JIIIIIIIPAVThreadPool@concurrency@3@PAX@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@PBHPBHPAHIIIIIIIPAVThreadPool@concurrency@3@PAX@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@PBMPBMPAMIIIIIIIPAVThreadPool@concurrency@3@PAX@std@@
.?AV?$_Func_base@VStatus@common@onnxruntime@@PBNPBNPANIIIIIIIPAVThreadPool@concurrency@3@PAX@std@@
.?AV?$_Func_base@VTensor@onnxruntime@@ABV12@ABV?$vector@_JV?$allocator@_J@std@@@std@@_NV?$shared_ptr@VIAllocator@onnxruntime@@@4@PBVTensorShape@2@PAVThreadPool@concurrency@2@PAX@std@@
.?AV?$_Func_base@X$$QAVOpSchema@onnx@@@std@@
.?AV?$_Func_base@X$$V@std@@
.?AV?$_Func_base@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_base@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_base@XABVNode@onnxruntime@@AAV?$function@$$A6A?AV?$ComPtr@UIMLOperatorTensor@@@WRL@Microsoft@@I@Z@std@@PBXPAUDmlGraphNodeCreateInfo@Adapter@MachineLearning@AI@Windows@@@std@@
.?AV?$_Func_base@XABVNodeArg@onnxruntime@@_N@std@@
.?AV?$_Func_base@XH@std@@
.?AV?$_Func_base@XHH@std@@
.?AV?$_Func_base@XI@std@@
.?AV?$_Func_base@XII@std@@
.?AV?$_Func_base@XMPAM@std@@
.?AV?$_Func_base@XPAD@std@@
.?AV?$_Func_base@XPAH@std@@
.?AV?$_Func_base@XPAI@std@@
.?AV?$_Func_base@XPAM@std@@
.?AV?$_Func_base@XPAPAUOrtValue@@@std@@
.?AV?$_Func_base@XPAX@std@@
.?AV?$_Func_base@XPBMPAMI@std@@
.?AV?$_Func_base@XPBVDataTypeImpl@onnxruntime@@@std@@
.?AV?$_Func_base@XPBVNode@onnxruntime@@@std@@
.?AV?$_Func_impl_no_alloc@P6A?AV?$OrtValueTensorSlicer@$$CBUOrtValue@@@onnxruntime@@ABUOrtValue@@_J1@ZV12@ABU3@_J_J@std@@
.?AV?$_Func_impl_no_alloc@P6A?AV?$OrtValueTensorSlicer@UOrtValue@@@onnxruntime@@AAUOrtValue@@_J1@ZV12@AAU3@_J_J@std@@
.?AV?$_Func_impl_no_alloc@P6A?AV?$unique_ptr@VTensor@onnxruntime@@U?$default_delete@VTensor@onnxruntime@@@std@@@std@@ABVTensor@onnxruntime@@_J1V?$shared_ptr@VIAllocator@onnxruntime@@@2@PAX@ZV12@ABV34@_J_JV52@PAX@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@ABV?$vector@IV?$allocator@I@std@@@std@@ABVTensor@3@AAV63@PBVTensorShape@3@PAX@ZV123@ABV45@ABV63@AAV63@PBV73@PAX@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@ABVNode@3@AAVGraph@3@ABV?$vector@PBVTypeProto@onnx@@V?$allocator@PBVTypeProto@onnx@@@std@@@std@@AAV67@ABUResolveOptions@53@@ZV123@ABV43@AAV53@ABV67@AAV67@ABU853@@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@ABVTensor@3@AAV43@PAX@ZV123@ABV43@AAV43@PAX@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@PAXAAV?$vector@UOrtValue@@V?$allocator@UOrtValue@@@std@@@std@@0I@ZV123@PAXAAV45@PAXI@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@PB_J0PA_JIIIIIIIPAVThreadPool@concurrency@3@PAX@ZV123@PB_JPB_JPA_JIIIIIIIPAV453@PAX@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@PBH0PAHIIIIIIIPAVThreadPool@concurrency@3@PAX@ZV123@PBHPBHPAHIIIIIIIPAV453@PAX@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@PBM0PAMIIIIIIIPAVThreadPool@concurrency@3@PAX@ZV123@PBMPBMPAMIIIIIIIPAV453@PAX@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVStatus@common@onnxruntime@@PBN0PANIIIIIIIPAVThreadPool@concurrency@3@PAX@ZV123@PBNPBNPANIIIIIIIPAV453@PAX@std@@
.?AV?$_Func_impl_no_alloc@P6A?AVTensor@onnxruntime@@ABV12@ABV?$vector@_JV?$allocator@_J@std@@@std@@_NV?$shared_ptr@VIAllocator@onnxruntime@@@4@PBVTensorShape@2@PAVThreadPool@concurrency@2@PAX@ZV12@ABV12@ABV34@_NV54@PBV62@PAV782@PAX@std@@
.?AV?$_Func_impl_no_alloc@P6A_NABUFunctionBodyBuildContext@onnx@@ABVOpSchema@2@AAVFunctionProto@2@@Z_NABU12@ABV32@AAV42@@std@@
.?AV?$_Func_impl_no_alloc@P6AMMMM@ZMMMM@std@@
.?AV?$_Func_impl_no_alloc@P6APAVOpKernel@onnxruntime@@ABVOpKernelInfo@2@@ZPAV12@ABV32@@std@@
.?AV?$_Func_impl_no_alloc@P6AX$$QAVOpSchema@onnx@@@ZX$$QAV12@@std@@
.?AV?$_Func_impl_no_alloc@P6AXAAUInferenceContext@onnx@@@ZXAAU12@@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@_J@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@_K@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@C@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@E@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@F@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@G@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@H@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@I@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Abs@N@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Ceil@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Elu@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Exp@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Exp@N@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Floor@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$HardSigmoid@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$LeakyRelu@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Log@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Log@N@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool1DTask@C@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool1DTask@E@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool1DTask@M@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool1DTask@N@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool2DTask@C@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool2DTask@E@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool2DTask@M@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool2DTask@N@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool3DTask@C@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool3DTask@E@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool3DTask@M@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxPool3DTask@N@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxpoolWithMask1DTask@M@contrib@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxpoolWithMask2DTask@M@contrib@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$MaxpoolWithMask3DTask@M@contrib@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Neg@_J@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Neg@C@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Neg@H@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Neg@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Neg@N@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$ParametricSoftplus@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Pool1DTask@MVLpPool@onnxruntime@@@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Pool2DTask@MVLpPool@onnxruntime@@@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Pool3DTask@MVLpPool@onnxruntime@@@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Powx@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Reciprocal@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Reciprocal@N@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Relu@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Relu@N@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$ScaledTanh@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Selu@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Sigmoid@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Sigmoid@N@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Softplus@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Softsign@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Sqrt@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Sqrt@N@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Tanh@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$Tanh@N@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@U?$ThresholdedRelu@M@functors@onnxruntime@@XHH@std@@
.?AV?$_Func_impl_no_alloc@UNodeCompare@onnxruntime@@_NPBVNode@2@PBV32@@std@@
.?AV?$_Func_impl_no_alloc@UPriorityNodeCompare@onnxruntime@@_NPBVNode@2@PBV32@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_006f042491572090b778a6887556c541>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_007ddac2123b71f70c18b64d41e9be57>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_019248982c19876b36bd9d56cbf00fcf>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_01dd80b018ebfebbe11d16795278407c>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_021916736d514528559cd9f1df8e9615>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_02a16f31201b954c010d4c15805851b1>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_02dbf81fbe0e79782741ce39a831bb2f>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_02ebe20f47dd15b8224ab822fe208f4a>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_02f0fede11d4b9d7439baa125e06788b>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_031e8dfd8d6a123502d9ec2194443e60>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_034a872241814873b16b150e98786486>@@XMPAM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0367bcb44eefe2d25755a8cf3194c733>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_05416e20ce8aa4e6f975518b44ddcde1>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_056b0cb2615e46ba7d3a385c7daaf5a0>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_057831c89172b3e745c34953b854d875>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_064044131b4432293f10a20d775cf672>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0753838d59fa1e3c1be3719b0022993e>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_077886625d25c359a62527fbaa92e49b>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_07cc7db52a628b03f17e7b1fd5adf33e>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_090ed96c0956ed48de6a40bf85283e7c>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_09804657b1d4ece14dbea3419a326007>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_09860caeb18fe5c6553c7cece043a6bf>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_09ccd5af3992c7f60b7106631dcd95eb>@@V?$unique_ptr@VIAllocator@onnxruntime@@U?$default_delete@VIAllocator@onnxruntime@@@std@@@std@@F@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_09d30a4170455b2cc3a6766e0ecf4621>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0a1a247f671177308e98d8aa00caba3d>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0a77da8549581038807d4f031ad4715d>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0a786afdc494302a03a8347211af4f5e>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0a9bedc68e3bb7f405a0cb52fae05ca5>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0adf0d24f2c35fa4a66c3d2e349117c6>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0b7b88c72b3c6e2d1499af326149f2ac>@@V?$ComPtr@UIMLOperatorTensor@@@WRL@Microsoft@@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0c69e806d9e2fad559bbd11cd911ec3e>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0cdf3ed2e001d703b14b9857f8148f3d>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0ce5feabbe81f3ff83f9db21f1fe9470>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0d1787685bfcab6f5be503e02e4ed79d>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0d1cea9b21275bcc722a263f1867aab0>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0d37934dc4686e9f8cdc4b98c71d039f>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0d4d08f2f59cb77cf8d5afa00847341b>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0d623cd1935e0ef85c92a37a55dd66c5>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0e8d4dc99952bd2e942002a2267cbc9f>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0ec02d5276c6a16fd616d10b796f4d2c>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0ef38e5b4dd40a551bcbede7fadd1472>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0f9a7ecc0cde6f755170ea655764ab2c>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_0fe8093ffea6514602a81f131e8a213b>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_104e5d4f3c346a5807a7db11389af990>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_11a93f40c7a11b73570d6d0a2bc4c32b>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_12065ee63fda2ae25d17bc16a002c579>@@XPAI@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_125b77507bd9658139842e91e09bb467>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_12dd4c4dcfd6c2d8effe6071c823bd43>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1359900828a9df3bd7d0f766b547ccff>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_13c0d49a798e8cd7f3691f8cf3d13739>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1433cb9cd8f8490322ad74b6fa812120>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_14407bcf8cfc66db9a85fd2745a31d94>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1476157c6a284e4f379191854345eea5>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_14cf9ddcdab5183a36b1158dec720c25>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_159e0921f0d4eb52aba9cdc091e10630>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_15ec4af4e71787ba502de1205d84eea4>@@VStatus@common@onnxruntime@@AAV?$shared_ptr@VModel@onnxruntime@@@std@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_16be1790a9760642272e826be4697ed1>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1712de044b9947fe68befaed2d7ed848>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_175914cea4037e183fc735b60abe96c9>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_18c8bc7d6e1564eeb9ad39be83abfb9f>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1929999175e850d5998c7c370f8aff7a>@@MMMMMMM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1a8b8132ed3c0ab63c63224ad4161527>@@_NABUFunctionBodyBuildContext@onnx@@ABVOpSchema@3@AAVFunctionProto@3@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1bcbff0149a8e25ea59b665acdead2ab>@@XII@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1cdda74d195f4dc7bbdbaed3ee174bf7>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1d958ad8769ba514a23fca7415139480>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1da5929b711c2681e91623a20d50a98f>@@XPBVNode@onnxruntime@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1dab84a38765585bcd86328d60f37519>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1ea0a0832a4cf660ea6a57031389b3fe>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1f154a6fecf26a7ac5bcd91fb4939855>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_1fb55e109ef7e3477c3bb87242d95990>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2036b51976e8571401119fff6c6a9260>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_21332c666c4782640bb01cbce240ea30>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_21a762d3684c6831db7ddce449ffa123>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_221457049dd6e599b1e592be3898266a>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_221cfc4539809ca9dda24743972ab18a>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2242169c388de54aeef0fbc82d712874>@@VStatus@common@onnxruntime@@ABVNodeArg@4@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_22485ec429c0920bef998442ed4b0141>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_226d842a4bcdc8605120f4a2efdc3985>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_227060e20de8b811822001a4d5e06e66>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_236692037ce4d824baf1ec7c2be78fd2>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_236f7c8f49088d4c504fd71f55f4280b>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_240d5c2825474fb53004f276faf2b25d>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_24575e001393f6791a4c588c060b67fa>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_24d5fcc8102cdc77a3db148fe9344e7b>@@XABVNodeArg@onnxruntime@@_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2529e063101eea511edf1238da0f8253>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_25fc2e6bd1e3daa78c370e70eea3061e>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_262fa984091282f85a5ab626cfdef42f>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_26620886b575de26166e451d44bc7aeb>@@XPAM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2667760ea75d0d5cd7152eb3bcb76dfa>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_26bff8e61e9bb411091ea7936af6af7c>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_26ee2a828034f17afc207c8ad30c609a>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2749f0a3e647c4a8e874fd0bb99b759f>@@XPBMPAMI@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_27a631d2450c357a52927c1dfbd2efda>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_27e8a99165a81df90f9c1dae1c5fcca4>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_27eb941e257b57f9cf2ee9f3ef3087b3>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2875b3e2e66b274d36ac70e9f0680993>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_29092bf385326e6f20e0b55a91956311>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_29460ba72c9a9f61937f695b212c2385>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2997ea06f466f3cb7af7580139e0e9b1>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2a6ee45563922231f230ab0594ec8ab5>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2aa6dc840362673ab7e0d38f455745fb>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2b2ce10a039b3e6bb8b2efe3128f96db>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2bb569fe2197271e2b1302ced7b75d4a>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2bc5f319d61b1e5d5273f7fc52f04186>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2bf41ec2c8021b2e4a040cfc2c7e5e74>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2cda3ec4180407fdb66cae08f370124f>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2cfb41272b7bf611216d7e49d5041299>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2e2760691203ec6d385de10317d43eb8>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2e79487384136cbeb73bf5cd55164ae5>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2f4103e1c4a1773101a6c1f055d56df4>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2f6217725d36a8fee3233a5a3a8acbc4>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2f80399ac55308892511bf2d4fa0c615>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2f8e437cb9f95dd76b3982067268238d>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_2fd33bbe7c4a2b258d4d665b8341f671>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_301279dcdc0bb6da9000a75437e1d615>@@XPAD@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3199312d6fe856623dcfbc1d675d418f>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_324b41c3d7492264a93fa781c1a5839e>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_327bf988738179df2cb078834240ff6e>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_32893135699ceef534edcfb3e0168a55>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_32cb8615ab18e281529c5adcc2581f88>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_331d7506767c1ed93515cbb23d39b0d9>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3354f944db7877754471d985be3fe29b>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_338e5fd506fd3a59b6f8e616d7366dcc>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3418ac6d0a6ccb6c692baad9c09fd00f>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_345b51dbe4e505cf996585c6c69fef99>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_34eb695d67d7709c51f33f1b8d0d7456>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_34f93c0ee84ed95033247b350e3e9c13>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3535579646aa11381d1f86e8f1dad915>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_355ced77e5add7bf5de4b6cde27fcd08>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_35b794944dfe770731cae452127f793f>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3674353b915e5a5f4e97c63731ea3b7b>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_36945aef9b69b8bc246158e60ded74ff>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_36c84409ed98093c11b3d0d02c089264>@@XABVNodeArg@onnxruntime@@_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_371d862278e5d64352c8bab4a05583a0>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_37fc46271b5a9d577e78557058b76819>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_388a8e360d5910da0bc0c1d744f82389>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_38e1bdb2070443b647c40452b6c90c15>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3918f71a17942b897d267f29eb5a4cb3>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_39a1fb20d4e6bdee774e59704a059816>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3a8a940e89b14c9a8617dfa32f79a60a>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3aa61f6258ac2da5aa1399bf10de3644>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3bcb9b903ba1cf4e300afd0f291b2ee4>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3bf9d7b5239a137326d2c5fa821fa737>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3c57c4f415fca3790b9eb6cedd38e28f>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3cd6c3d57d7e8805d712028bde485ef8>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3d97bd48e7741aa8b9ee3562eae8be98>@@_JM_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3deb1638445ba1a8c860cf9e9ece0b32>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3e8fe92fc2a983eaaae8e23df9000d98>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3f2f36db15d672a75a701714494919da>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_3f992177007ef3a6a4a7f362ca0d72b8>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_404f128c43627cca26dd30079f566078>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_40d7d549b7296d37bd8a375a6a32735f>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_40f74427ec28f20a5d59b70a3f7ba162>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_414a43a198ea3ff0dfdcf4ed89cfe63a>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_41ba827606a0dc44b626761483b4bac9>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_423c7f4514cb0f580d17c1969d94ab4d>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4276ac6945fda74461cbbc3f55ea202e>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_429334e9c462f3e67003e15324abcc42>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_430c7041764d504802816f1b3fb25d0a>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_43719f9d8c83073f26885f37cf523fe1>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_43d76a454d7e446551a32b163679964a>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_446597e43ad5dfcf42334d9ec384bc3b>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_447f5b0eec09765ec587d8b36006ec6e>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_44f740f4362270fce5c964ebb50cbf77>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_452b080256e46867afd3b39c75897486>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_45e08ad47c9bff0006768a2c526864a3>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4623cd97b09f16cc323b1da4783c1a74>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4682f43d0273e3d01e9224f8a5dcf803>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4692eb6e70108664f447bb895cd16b99>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_46e1dc415ba1567f93a11214638e2255>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_47109259520c4e793b98906acbd4073b>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_476f16e46856ad1afb9e2f3aa987cf47>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4782a3788b4d37949c863bd3b1d2db4e>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4895a69c6e2d4d3665d84240b52946d2>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_491a5ff9baf0c87e80ac72a0beb259ee>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_493755c0718e549fc6eb6376aaf1c076>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4963f16a0eb793cb3ee709a47577548b>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4a6aeb4175d7888941bfb44f87a822c0>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4a825ecad3e2e0b4fc2d33026044f2b0>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4b1533207937d10b38e9bd484b05643c>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4b40e91ae8e8c37fb63980fee637f885>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4b6a543f93b634de8567a05c1dc832e8>@@VStatus@common@onnxruntime@@HABUOrtValue@@ABUOrtCallback@4@_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4baf23d0c206ed351c6ebf16f344f7e0>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4c36c751efc3ade178fd6425d4750a0b>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4cd082e17ba7994385c699d185cdacd5>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4e3f051fdc405a96de9c485e123872ab>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4e6fa5fab57276f6c38287bd072060d0>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4e978ba75a8a0f3f4d2d1e75d74ac4de>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4f86db608486d9d830b4d71843ace9e1>@@HPAUComputeContext@onnxruntime@@PAPAX@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_4fc71a60d0d1e6360508e57a55f6b5c9>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_50815597be56ed0e327183a6f984971c>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_509e8645eefb2a1b0561e4ee228415fa>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5151cdb5e2d0e967b0ff9b516f4dde5e>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5198f0fd761e3f604ccc31929c553d15>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_51e6c2078824708885b36339be574a42>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_51e7ce4347ad68176c6a114dbe1a7fb9>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_51fc723ea39c4518ba8a40d8ce3fda32>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_51ff266d4a509f82cc61194622d6aafc>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5241072fb302daf3d92fe8af4902fcf1>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5260c61ab07867c25056c192e14b912e>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_53958a524045125d538038a834c170f9>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5455b7a3d85ed24852611958e08bf1e7>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_546a5101aad278ccc6f90af11bb20384>@@MMMMMMM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_55a8c7f82471609cdeead8ac3d02d601>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_55c987d2288bf4751b7cab2cd3c58f7e>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_55dd3d0ebe2e48cd699a7e50bd7c1613>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_55e8e059d201c280196491784367b3c1>@@PAVOpKernel@onnxruntime@@ABVOpKernelInfo@3@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_565473cc7704d655ba5df76fa41b94f3>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_56616cb04d479ecbbef60d16cb985ff3>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_566e56833ba647bded2d1d52059227e2>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_56708f1224b2fa079c579dbcc99f5723>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_576babf2632b8f119a1c6bd71dacdba2>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_577697c65f7f30ab5c890fb5e643c3c6>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_57b3023538a7039f2e0e4aacf995d8cd>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5816ab89d71996ccace6db8ba2545f64>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_58385ecae81d2f32df5183427720040f>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5887a420a41b7e61b78788bc0c22095d>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_58c175adcec771ca341f59d8965ef956>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_58e45bcf81f2eb4f6ae6b3f124defb13>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5949c53d8ff4275e91f18552fb52fd7c>@@XPAX@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_597c15d719a814d12541cbd2bbff60a5>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_598cf5f3152cf23096c5a28a030ee0b7>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_59d686db14cf1737be131f4a3fc9791f>@@VStatus@common@onnxruntime@@ABVNodeArg@4@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_59ed2dcfa7c0d744c558df76076fb0aa>@@XPAPAUOrtValue@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5a03cad439a535c73c9479bab198ff8a>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5b19bcf90126dbac847fa4a3a03b8aab>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5ca3b2fb2261ad467c7e40fda304e71e>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5cc43c49af3b89fd43e2aa8a8afc2e1b>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5cd9b18fb4de22826b438d8133646797>@@MMMMMMM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5d86fa40c6f141e2f5308ab25a37373d>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5daecdbe57f90568020b13975f1cd001>@@X$$V@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5dbbe93e2a9b11b44d01ac5bd9e5d0bd>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5dddc316c01bb02791c37b03fa523bb0>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5de94ebf8ac34b72c76fcea7a63f223e>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5df54278b3724a9baf121a1f8852e38e>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5e0adb550519bb305a282a49bc052ff5>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5ead457e3eeae74d2baaa54a34ab876e>@@XI@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5f12e6a0112b95dae1a14ebbefa23368>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5f4057cf46e1dea27ced686274a4ab95>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_5f5c509e4ecbb203bb441c84afd312d2>@@VStatus@common@onnxruntime@@ABVNodeArg@4@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_608411f62512d6d36e0c058dda512de6>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6098da3ff9cb8b08151d3844bed31f70>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_609ecd802714b672c2c81632a57c73f4>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_60af20c304d3399c0b13e2dd97473212>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6137ab0b6712207e9054d1c872b492dd>@@V?$ComPtr@UIMLOperatorTensor@@@WRL@Microsoft@@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_613905a3c12d2353d146a5d251b5caad>@@VStatus@common@onnxruntime@@ABVNodeArg@4@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_616493d339e7f246f89d6e49042ee686>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_61ce4a3a6a2e52170ed094a3091671ab>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6277f32b20d3d158c3ebcc1771d29c1a>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_62f1ce9bc208e37c7e5739c8f36388ed>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6367df6aa050372a33bd7465b9bffebf>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_637d27d6a01beef351cd6df162f9426e>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6423e25c1122e8ea98faabe27ee57b24>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_64826d400df5e2683a863a4dbb954602>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_64c62750e999b82780ab6a59a6ba6851>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_65c7db542fd8e8c34231ca74f6edf804>@@XABVNodeArg@onnxruntime@@_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_66b6761e17b1698f92d4664857f34eef>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_670b3cf7fae0fc01758a26323e302b88>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6730021c9a4195800fc17a38baeda605>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_67cab59cf9e4d6fff4a228b6f50a31f4>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6829271c72aa0e0367fc0ccf7a9e3edb>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_684acbd5f928eb904d7ad61767659782>@@VStatus@common@onnxruntime@@ABV?$vector@IV?$allocator@I@std@@@std@@ABVTensor@4@AAV74@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6857bc45ce6cabf2b8f5f4f7c03ce1fe>@@_JM_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_68ac02ac858e6ede64d0fbc5d9dfc15f>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_691026265f1e73c9b49221bbcaaf7e8d>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_69631e77932e21817a56851e5c6edc44>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_69b772cd14ccb0d3e3ddf4a285e464b6>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_69c32f0b6e172d0bef2d15e0c00baa25>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_69e57fd1b30d7ec9bd7519732750e12c>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_69fb29d342558f2407dd09c009c85823>@@XPBMPAMI@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6a160209166296c80c9867c0c24688ea>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6a32611970906250c6b47d8292ad00e7>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6ae1892ff4a4003e2e47baa65ff24d8d>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6bf7af48e7a4158e20b4f833c15de130>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6c2b6db637aede97107450673ae7cf3a>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6c435e48e6003c8c444134537317003d>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6d1a31765a629c9bc43dcea20434203a>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6d3877051847983c73e11af2112f64fa>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6d6f3d3230139a3d222e52865731387e>@@VStatus@common@onnxruntime@@AAVGraph@4@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6db08e7b535fdfa2ab543d6397b422f2>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6e1ae7feac7554001d4e77c1f621284e>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6e98eced94f0a80de9a71b3e8fa901f1>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6ee0b11a8d2719c95f7e0f11cb29efcc>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_6f80d65fbc6388042df99a3b220305d1>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_70279f22222c392cb493fce9840987c7>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7029d41d4a95ff2575c8227d099f78bc>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_715d031581e8d9abada945eeadbab576>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_730e393831d92a8b1623c0ee3e0e1a73>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7361ba811d5ff92bf50e102c73312480>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_738d98ba114f0b992fde43f3c2b8082f>@@_NABUFunctionBodyBuildContext@onnx@@ABVOpSchema@3@AAVFunctionProto@3@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_740a6591f9964eaf27aee9c62c1bdfd5>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_747d01ad8121cb89860d325b0c86de9f>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_747f5a5054cea5042105b3988bb8e54a>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_74ee45c15b018f2c2709e09284f9b432>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_75106fbcafd9b6387725a0ac61d90f0b>@@V?$shared_ptr@VIAllocator@onnxruntime@@@std@@HW4OrtMemType@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_76eec3e346e3038343d242b83e7ef7cf>@@XPBVNode@onnxruntime@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_76f850960fe6ac327acd4613d41835f7>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_771974d945874fd2518ef4fcc7999ce0>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_772b291d0a2826994f823d005fbf5dd0>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_77304148f0ddead6f8725fd6b1fe3a6f>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7843dc957c42d3b8b9e2111287c74d54>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_795d004014b095ac6c3348f10d228401>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7a6442793a842f584bc31790203846c7>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7a970010ad38e3a1508f7194da0afc42>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7abe23a5198e897647d783cb391d2ac6>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7bf0c45af15d48ff0bf3482d9ab1530d>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7c5eb1f068179d900c888fe3d5890426>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7d18ce867b586dae3ed4309094f85b3d>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7d4a5c6b07fd360e232dc4416f33eeb2>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7d7d52dd8f31ce0fc48036fb1081b283>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7e6056f43d2e17d5b39d88df6efe7e61>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7e64dc56260f5aa848b67008058a8451>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7ec65dfaaf4dc75c3f7f2cbd42331128>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7ef4d7bf9a11acea8de40a758239a896>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_7fd088052d119656db3d6706c3514329>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_80083253b4a44259ccb0f1ab2453639e>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8032534fac3fd5dda5243b4509fad456>@@VStatus@common@onnxruntime@@$$V@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_80600ba38f1578c0943cc3b4cc8d084c>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_814d8d8f4b4ee5499087afcf23c8abef>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_839e4f861e16af089c617eb4d1d9047f>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_83eca28d6349860dc86100acaa252e03>@@VStatus@common@onnxruntime@@ABVNodeArg@4@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_843c10906c86ce430854ae817b2b31e9>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8448e89789bef89a83708200e0dba2ac>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_845fb0d365668e61a65e09bae6280c09>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_850028fba9642b015a2c6ff81bf07107>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8516815e67f3594718fcfcc07116860d>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_853403f8a1cb1bf480beff4c6dac6b21>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_85740bb65c1a7256d972833e138b25dd>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_85cd850b3a70267b82c425a1808d56e3>@@_JM_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_86574d63891f8965457453a282e51ac5>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_86895a13d56222c09289c706b79a6852>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_872c985ed95e21f46464410321ab9d0c>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_87377fe9a7b55ca099852704c7649a37>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_87386769d120dc3c5202374ca430a8d9>@@PAVOpKernel@onnxruntime@@ABVOpKernelInfo@3@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_87cb9d4d5e38fb120a31a53965033d4d>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_888f1f17d6f86eade7f5261fdde46e6e>@@_JM_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_88b1496ecc213c07fd24b30bb5c856e7>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_88e4cbdd9578a1aef93a2f7d3b7ececc>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8ac5cfcccd347e512ac9419fd63346cb>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8bb411dd051c5b2789979ee8619f0375>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8c122ed0b2783e809ca408dbc8378b71>@@VStatus@common@onnxruntime@@ABV?$vector@IV?$allocator@I@std@@@std@@ABVTensor@4@AAV74@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8c32b6976971eb48d48ba6d5dd1a9ae2>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8d2c2e51afd333ece2ae8f2a329fc9ee>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8ea5687524a42143533b684f5c9ce181>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8ec99f105e4604958a8ba328f7e77b1f>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_8ee9a80fb3df426a08cdee43eec5f4a1>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_902ce7e426f16d786ecf9d87a583aaa8>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9222e83c56bbeb08f964662158da2721>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_93063d24d3eaf65455400e96fd395987>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_932674f6b1528de9101a6cf1da2f3619>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_932e39bc42b54acf3286176bb275e4a7>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_93640e529bc0ba0d303e1a662462ace2>@@MMMMMMM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_93b65f05ddd7a75740e1646ed2ad5a8e>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_943cc09e2a717989dfbbaabfcc35437a>@@V?$unique_ptr@VIAllocator@onnxruntime@@U?$default_delete@VIAllocator@onnxruntime@@@std@@@std@@F@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_945932bb7c945ed8e789f6af7f6d509f>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9461f2e660e0c1ae91a37f46d99b3e80>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_94623d72acfd58b8a8d7733e9b2c9c90>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_94f802a6045ff51c50912ff8c7e2bd10>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9531d8dd38f435ce1e9840c5a24c7961>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_95a8f80a3b96610e783f7b34d80ac7bd>@@XABVNodeArg@onnxruntime@@_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9671e9cce0def787c04a0ca1f741034c>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_974421a2166ea3684014171d0d75f69e>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_975b91ffee199f0bfc6f725443e3d8f8>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_976a37e7ee737ef6672be8fc72541275>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_977aa0989600c553e6ad21ef5f183702>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9821a1e0b606be014cc2fca34ba53b1a>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_98b53a1372ec257c871e8ef28d59be07>@@VStatus@common@onnxruntime@@AAVGraph@4@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_991d63fb8a809dc84e4e523068ce486a>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9935e3057ff6619551b128a634c37edb>@@XPBMPAMI@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9974bd71538aef5818f74e4a5e52a52e>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9a2786289a4d18fa33a8facc5fc736fa>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9a56c482bf06c1a34241f10d7df60646>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9a80c5c6458e824bfa0876a779605b0b>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9abb244ecf1fe7549bfa1b3b46427d69>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9b848ee398be1c2a596086e8c2c0124c>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9baa1cdad376a56943ca503609534da1>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9c4f15881ef2441e36fb65a5f1a748dd>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9c9dbe7f2291bf7ad9d3c12ac8113947>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9cb8596ba7d3038d751f936a264c9fff>@@XPBMPAMI@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9ddccf775856736f4137dfc22445dcab>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9e468aee1cda7bcda7bd21224cf0d16f>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9e6211c87401ff34ec951ed3f9d92212>@@VStatus@common@onnxruntime@@AAV?$shared_ptr@VModel@onnxruntime@@@std@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_9eb53e876a6f2ea1f0d78d7a6be208e9>@@_N$$V@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a0842de1be7c8b620f11f3eaf0782629>@@XABVNode@onnxruntime@@AAV?$function@$$A6A?AV?$ComPtr@UIMLOperatorTensor@@@WRL@Microsoft@@I@Z@std@@PBXPAUDmlGraphNodeCreateInfo@Adapter@MachineLearning@AI@Windows@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a09e1ae91603c934837ab1779a5b1cd7>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a1b09d8bbea23dd46d5c1124ff420ef8>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a1dc46dc98606ea7b7b5f34303ee2a2f>@@VStatus@common@onnxruntime@@AAV?$shared_ptr@VModel@onnxruntime@@@std@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a1e22134fbe96254ce6d7a539c26c203>@@XPBMPAMI@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a2dbfc730df855f1da452ed738a41cc7>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a3360ab2153a0d5e361146591e0ed940>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a4eb294818c1d4196ae37e2aec2c57f3>@@XABVNodeArg@onnxruntime@@_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a4fe3bd39dcd0107f90455cb45c21429>@@_NH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a57196164107acf0b8c09a6af15b6c67>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a5bc10369070c00aebb065dae57fa0bc>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a5f2de4a08d99e6253357934807ae733>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a65213a64b99d0456231cd4b3dfd703b>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a690b97e1252e2aa05e689270ad1214e>@@MMMMMMM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a7688683053999985ad4b5ee1c5e7d55>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a777aefbfa0ed449706f4a933e2595f0>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a7e0ef8dbac7898789e7674a96621dea>@@VStatus@common@onnxruntime@@AAVGraph@4@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a7ed49975b7832e11720c7c8bc70dba8>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a81247647b5856024e7c9c86be60bd88>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a856ca1b7d6657f47466353c332cab4d>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a875e26a77cffbb176a8ef549b2a14b9>@@VStatus@common@onnxruntime@@AAV?$shared_ptr@VModel@onnxruntime@@@std@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a8dc603fd8183714a0dc357fc9b787c1>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a8e944132f86054019a0dd06ad885a3f>@@XPBVNode@onnxruntime@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a90d8b4d11b49a93db077960be52512b>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a949a47209eded0ee39e3c26cfeb0e88>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a96924e5b8f730c8b67a0b16014102d9>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_a9804f77c3a977ad53df45464de9292b>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_aa40aac03a68df4d791c9ae960a96e81>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ab509a36b64c6185495bed43165d3ae5>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ab9c2c5f50505ae2c8183bb62b8be9b3>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ac9e5c5dae7b58201a89b0e1484eabd5>@@V?$ComPtr@UIMLOperatorTensor@@@WRL@Microsoft@@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_acbad9bc57c95ab4ffc746caf04c813c>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ad9ee849a4f4af706ba1761253c50820>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ae9b41333b82c3f43259db32dcb29699>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_aebf67a76b65926eb07014f4fd08d631>@@X$$V@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_af2b72ce93dfb4b4ab2e91dda701576d>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_af2c39467b4484bc2a2f611cf533daa7>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_af494c70070cc8b5902d69428e0f310d>@@MM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_af999ce2e03a2039dbf31c1ff13e0675>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_aff3a47ffdeb0107e2635883ea459138>@@V?$ComPtr@UIMLOperatorTensor@@@WRL@Microsoft@@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b0de799044b27f15cb610b7d88655272>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b123eebc6aab753cdc93a7cd4699b893>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b13b0c3bacb7bf27cf9bf4bac72849bb>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b15572961e53281d7d33053637faa8a6>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b1dfe0352ec14503f6bc27c9ab7381cc>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b39fcf5d316099631cb4151a025dde22>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b3cbfd0276cf0ad6f3b58532d60f239f>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b4521294892e50b63d12361cda2a891c>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b4697c7f06fc1e7e59f4a5183c6a98c3>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b51428820b3477616d25ebf36e7dd645>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b52f9681df527cde6dd7dae12a89dffb>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b56eca9e5f17216a19fa025aad3fbc12>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b62023a8b21b8c32235c601a9833387c>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b63c2e5f4896a6001dfbdd641092de94>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b676af55e67de34be8258a85059e3fa8>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b6aaeef95996305e5431d1fd80eaed87>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b6b6722f1e0f042c63804b4645002c54>@@XMPAM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b6cf00bbc72a844e74b65b515f429504>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b7019bfd7941f33388d4d95b9daa50e8>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b71fc6f482e72afaaed63683aaf4c733>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b769cb65454a05d6b4a26fb57be3e5e8>@@VStatus@common@onnxruntime@@ABVNodeArg@4@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b7be74f8e6a45391be9d480e0a0a34ff>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b86aa8ef1de0593a5547f08d792c1565>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b87eb49fd5ec35570cf6bd7d61795bc8>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b8c456fe40447d4b39feffecfde76884>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b8dcef670a65ded42eb20f2062e03c5e>@@MM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b9005de86b74a327b831744e992c382d>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b904ddb979f839dff33d10b1e7d0bdbf>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b92977e632ab65a34aad3c8d85f14f57>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b937f7d4f097577d4fa0e0107ad25600>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_b9ee3e2cf9bbfb73bfd678f2ffa5cccc>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ba0868398d1cd25f6a2d11110301255b>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bab97670c306dda36cb6e184f5555653>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bb3d425e1c6940fb9e0c2d83749b4004>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bb907a92df9fde70b67de149cb976bb1>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bbfb6c4f9961aa9de543511f0a00bf20>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bc73456ccef784a5af3834b4cd8faaf9>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bc9a9f99cbeabaf2d0a936a0cad6b782>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bce28680af6d6e63afbcec5203580c30>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bd14764690ecfef9fa94612b0f5ef502>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bd88b325eaf102bf56f0337b94c4e196>@@VStatus@common@onnxruntime@@ABVNodeArg@4@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_beb5ca5c13f598ae93cc9dc5c5727597>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_bfc475f1e9fa3006d556ca921366385c>@@XII@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c0010c06ff92f81c472abefd6166a78e>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c13687892ba25942bc19e5eb13b503f9>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c20b2bc71558ce98a32f42bb85de6d28>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c2d5469345cd97550e4777826a9777d5>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c2e380cf1102ff4d7d9bd2e215fde5b4>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c311c1ca35d5610174ed400a6355dd88>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c332dd4138e096b0ec5e0eae392e23e1>@@XPBVDataTypeImpl@onnxruntime@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c3857b8060a6c3155dc6bb6df09a4841>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c3afc2c3b924ee88b1311ef23b1644eb>@@VStatus@common@onnxruntime@@PAXI@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c3d54ef5797172fcc9eaba420c8f75e8>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c4a3869be068e7ba91c5160df08ada1c>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c5677617c2b0074bdccd3f6596388b76>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c580670d4132814d3bd00ecba71eaf16>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c5dec7e5184735ef54bbb3fa15124e26>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c6108df36ce1e471ee073cdcb906363b>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c6e8ce8febb47c28ae5d88eebb07bb81>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c720e61805dcd56f6632856caaf966d8>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c7bc13683ecb5a18d043a137e0218d42>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c7c1c280bcb590aff0e32b0d2bffa767>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c7e777d5cfa84aa6006d2063a10ce1f4>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_c8c33a5265022502f864eaf2ce2649c7>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ca1582b0d14abd4d03d9895812c8d107>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ca50ed85a5b6a014480832a0283a2f4d>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ca605469c50319317c9a56255477531a>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ca87e505bf8a7e1f26579083c3683d94>@@_NH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_caa048ef690674a871bcb383934123ad>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_cb60ff922e3e1d629a152c3a1068e6c5>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_cc1c5bc88263b1d3d134206e36d4f483>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_cc270699afce6708083292447c6ec5a7>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ccb51d052b759c6bf0040059226512eb>@@XI@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_cd453f5abbb4020fb3775475830cd8d1>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_cd4c33e9450ec2c40f1249d17dbfa6ba>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_cd4f9515fd1d18f38ab6ba0038b1c96b>@@PAVOpKernel@onnxruntime@@ABVOpKernelInfo@3@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ceef0f6dad2176ea59816e5c9d007c22>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d01cb7d72935562762275ef2725e3ce2>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d028772904ecde58905f5707c0de32a5>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d099cfc55508afaa7cb7103a60412ec4>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d13d78c406485322a9ca312b3346960c>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d1d74ae6bdde7a081748ee6565b14bf6>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d252a66a2c07e9c22634d060e74c7bae>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d2a49f03e450ccce2f16f6b50c34ec41>@@X$$V@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d2d693a490e9887da5a024c703dc8e3e>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d303bd53dadec643ee9070953471f748>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d318dbc50159c2f1256019c88eaff40b>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d375a3eb9b0fdb1da985e8c55163a4cf>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d3bde8af1c4b2f7f5d32bc0631a76a47>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d41a0a7dd6ff244599c8f5fcf6aa6a8e>@@XABVNodeArg@onnxruntime@@_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d4ed0ae11d4b451fa79547d0fd0e7282>@@VStatus@common@onnxruntime@@ABVNodeArg@4@I@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d51c06990161dc274f89c0ff046b1644>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d523df6827c8f76e5f91e59ed710694a>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d5280148c0688c7d0bac28aeda9b34e8>@@VStatus@common@onnxruntime@@PAXPBUOrtApi@@PAUOrtKernelContext@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d5aface184b6dca3aadbb4032d024091>@@MM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d5f3ad82c6f06fa69e5f77344156b4ec>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d61a4c9d12d8fdf51773fa74732acc59>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d67cddb616713624f417e1920c5d4f1a>@@XPAH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d7733c1584aa3badfa0c0498f12b13b3>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d8b06a4dd6f551b1fc6c7009ad0ed428>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d8bd629bd1b1f6ad17cb30e686c3da2f>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d8c2999bab01791ce81720c649701452>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d8d69ea69c6171a0cf8e50796541d588>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d95ddce1a1fa4ff8b9f16e34e85c7615>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d966a160cb34bd8cd4e5e8dbd91fdf85>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_d9adada7822ba67bc618dd6220c7e11e>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dad5fe9e33f684546a8c61d0c010bbc0>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dada44d2f02d0415eebeca9a96054391>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_db34e867122da08651e3e6cf481fea25>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dbb7db98ef2a06cce32a7330369a258b>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dc05c873fcc02dfc08a02f74938f6e72>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dc262a6b1bcaffa0d7b330449a737cd7>@@PAVOpKernel@onnxruntime@@ABVOpKernelInfo@3@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dcbbb46dcb2ccb38f1b7e3e89a0ca172>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dcd60b8d026e641a76ed4cfca4de49da>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dd0fa2bf36aea47d4ec8777b68b77c1f>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dd4a8a293eb1e8d9bc247b4bc07121c6>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dd922fd085bf68a1538b4dfe400a4e5d>@@XPBMPAMI@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dda191f6c4c5c14fe1c2434042cb685f>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_de0ef70b4af308e6dbad1f0877114d15>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_de5c152ca9e3f93d72392e47d49640bb>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_deb528c4a03e167325d02f55697f57f9>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dfc50b5c75707c0e32bd4de87875de8d>@@XMPAM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dfdd1161f2f278e15b5873916c2b0ca7>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_dfdef6da643cd6a11fea2d67407fdc34>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e0b99860c2bc4f26550c2e8d362d7aeb>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e12a5cc64ca47f52fabae1f759666c1b>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e1960fdb548c2ba32cd565fbc4783a60>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e24b21811bde89af90e60da39c77df1b>@@VStatus@common@onnxruntime@@ABVTensorShape@4@ABUOrtMemoryInfo@@AAUOrtValue@@AA_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e2b51361958443a9e5e8688df90bcc31>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e2bb8f55a256e670e2f45d1b39e2618b>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e31b0a3938d8cf9c535bd4f318233ee3>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e32e28fdb3d333c868b22a20bac87494>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e34957b95c2645f541b4bd46255fbbb0>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e36f361e8289b79d05cd119226887515>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e4094104640a99b7059b503bea5cd4c0>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e430b604f7b922944a8b9691ed8b0438>@@MM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e479058e031cb9a0657aa4c881cf2c85>@@MMMMMMM@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e4af5663d262f6d7cf485c6657cb9e95>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e57b1a786ad480298008b53b2aee7436>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e5b736ff629e45c243838260d1c41f20>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e6d5b5718178868e630fc4afa1928cce>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e742661535f51343e97c0bd1f594fd9e>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e7465666a1cce2f1be8b885d5e56a9d2>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e77a22eae973afb20f2d0750afc5e252>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e795049375f61fd2d3a131a7c274e255>@@_NABVNode@onnxruntime@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e79e85aa06c0020ce8ba4eced08c2c92>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e8ba98e57e993cbf3811de764c1ca6fa>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e8e963fdb995e953f45fcef5a7c8333d>@@_JM_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e8f0421704f572aad2649a099fbec081>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e971bf56206beddda75940cb23d81d7c>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_e9c91c962bbc67adb808a120781581d7>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ea73e250586924fb71aef6e9068c74b3>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_eb6625ff603571627d15a6f97beef5e6>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_eba0ea83d45e95a6a590a5b2e5133b79>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ebc32470c7ba4b7c18e53e48be43fa56>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ec1910e0e2f828f423168f93cf5ec05e>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ec38228ee8b8eb97ec41ac102234721a>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ec3a9f1754c313e2d781cd72b20f6949>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ee2abb9e567c98f54823297ece51bc5a>@@VStatus@common@onnxruntime@@PAXI@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ee98d36ba3b6f304603cfa013effa1d0>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_eedfabccf04dae8365cd4a4775f4c12a>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ef9140f011f6ca89ad52592af55290bc>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_efb13713d16ec9cd372290e4d816e310>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f02097f693588c4875083cbaa36b78e4>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f0c2d149234417642d38bb42f3187940>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f0d5219e49c8319e60d44c8130a3f306>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f0e40b769732eedb69b178cbdb1da2d9>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f112d6225128e4a2bde6b3763bdc14bc>@@VStatus@common@onnxruntime@@ABVTensorShape@4@ABUOrtMemoryInfo@@AAUOrtValue@@AA_N@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f123ac6ed0caec8551b10b1bf958a874>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f1381ec0608811bb961bf0bc58bba8a8>@@VStatus@common@onnxruntime@@$$V@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f1cd88210e6db5811994272c07493962>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f1fa811d25327e50fa43acb91e2eb6a4>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f20e48e7127186879f643480a395548e>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f22ff450a42615719b4e6b964a52c793>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f36a9f1162ce53394a7287b72f55835f>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f37d07a0c75273eece221dd5b60b299b>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f3fcc869c6c1998b84de6d82e27f5b75>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f4bb0cc197d9c7469e8edd1f960265c8>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f4ee709d6b6434a145be84c46551b589>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f529c53f0c288eaa5f1eb9f79a62faaf>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f65c61404cbe8a2665c2dfba4b8d6c08>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f66636158d41c23d6c8741d8b8b758e4>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f749f947d6e043250dfdbe549acd74ee>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f78d8769dd40946ef61ad5956cbf10fe>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f8a8d7e3c30093707b9dfda821b82ba0>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f981a1e78256ff9696dd76fdbfa999a8>@@_NH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_f9f1f2325ee0aa9d5b8ebeaa441162b3>@@XAAVOpSchema@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fa20e9f670f8ec378ebbe3834e9db296>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fa21abf52748e92ccc4db121eff52d81>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_faa349663837247e8666c82fc4f50c4f>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fabc2f3ff1a13bd31a439cf6cb082152>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fada670c8c8fb6ae388f5a9e4132a8cd>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fbb10b34ab7c53fa4628c7de9c1d7ff5>@@_NI@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fc074f31a22d2aec298c9e2b6a482f1c>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fc59cfd6180e7962f9dbfcf6d01970a3>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fc7d1f6c2adcff66a9b847972f40b566>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fd266a9166d0b10d35a0d8f14994daa2>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fd4e281633801c162fcd6dc98f33e424>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fda969412d0889bae9901abe379045b6>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fe53dec043dfef08e033f9e450fe62d9>@@XAAUInferenceContext@onnx@@@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_fe69b453113ca38edb4981f1b14f5af4>@@XHH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ff5e6787b4748302677d41e07ba5b8c0>@@XH@std@@
.?AV?$_Func_impl_no_alloc@V<lambda_ffd7c59dd1db3541219e1571e11a3c77>@@XH@std@@
.?AV?$_Iosb@H@std@@
.?AV?$_Mpunct@_W@std@@
.?AV?$_Mpunct@D@std@@
.?AV?$_Mpunct@G@std@@
.?AV?$_Ref_count@VModel@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@UCpuProviderFactory@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@UDMLProviderFactory@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@UInternalRegistrationInfo@Adapter@MachineLearning@AI@Windows@@@std@@
.?AV?$_Ref_count_obj2@V?$unordered_map@PAVKernelDef@onnxruntime@@V?$shared_ptr@UInternalRegistrationInfo@Adapter@MachineLearning@AI@Windows@@@std@@U?$hash@PAVKernelDef@onnxruntime@@@4@U?$equal_to@PAVKernelDef@onnxruntime@@@4@V?$allocator@U?$pair@QAVKernelDef@onnxruntime@@V?$shared_ptr@UInternalRegistrationInfo@Adapter@MachineLearning@AI@Windows@@@std@@@std@@@4@@std@@@std@@
.?AV?$_Ref_count_obj2@V?$unordered_map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@UFuncInfo@FuncManager@onnxruntime@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@UFuncInfo@FuncManager@onnxruntime@@@std@@@2@@std@@@std@@
.?AV?$_Ref_count_obj2@V?$unordered_map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VTensorProto@onnx@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VTensorProto@onnx@@@std@@@2@@std@@@std@@
.?AV?$_Ref_count_obj2@VAllocatorManager@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@VAllocatorWrapper@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@VBucketizedBufferAllocator@Dml@@@std@@
.?AV?$_Ref_count_obj2@VCommandQueue@Dml@@@std@@
.?AV?$_Ref_count_obj2@VCPUAllocator@Dml@@@std@@
.?AV?$_Ref_count_obj2@VCustomRegistry@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@VExecutionContext@Dml@@@std@@
.?AV?$_Ref_count_obj2@Vinput_buffer_adapter@detail@nlohmann@@@std@@
.?AV?$_Ref_count_obj2@VKernelRegistry@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@VModel@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@VOnnxRuntimeOpSchemaRegistry@onnxruntime@@@std@@
.?AV?$_Ref_count_obj2@VSchemaRegistryManager@onnxruntime@@@std@@
.?AV?$_Ref_count_resource@PAVBFCArena@onnxruntime@@U?$default_delete@VBFCArena@onnxruntime@@@std@@@std@@
.?AV?$_Ref_count_resource@PAVIAllocator@onnxruntime@@U?$default_delete@VIAllocator@onnxruntime@@@std@@@std@@
.?AV?$_Ref_count_resource@PAVModel@onnxruntime@@U?$default_delete@VModel@onnxruntime@@@std@@@std@@
.?AV?$_Ref_count_resource@PAXP6AXPAX@Z@std@@
.?AV?$Acos@M@onnxruntime@@
.?AV?$Acosh@M@onnxruntime@@
.?AV?$Add@_J@onnxruntime@@
.?AV?$Add@H@onnxruntime@@
.?AV?$Add@M@onnxruntime@@
.?AV?$Add@N@onnxruntime@@
.?AV?$Affine@M@contrib@onnxruntime@@
.?AV?$ArgMax@H@onnxruntime@@
.?AV?$ArgMax@M@onnxruntime@@
.?AV?$ArgMax@N@onnxruntime@@
.?AV?$ArgMin@H@onnxruntime@@
.?AV?$ArgMin@M@onnxruntime@@
.?AV?$ArgMin@N@onnxruntime@@
.?AV?$ArrayFeatureExtractorOp@_J@ml@onnxruntime@@
.?AV?$ArrayFeatureExtractorOp@H@ml@onnxruntime@@
.?AV?$ArrayFeatureExtractorOp@M@ml@onnxruntime@@
.?AV?$ArrayFeatureExtractorOp@N@ml@onnxruntime@@
.?AV?$ArrayFeatureExtractorOp@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@ml@onnxruntime@@
.?AV?$Asin@M@onnxruntime@@
.?AV?$Asinh@M@onnxruntime@@
.?AV?$Atan@M@onnxruntime@@
.?AV?$Atanh@M@onnxruntime@@
.?AV?$Attention@M@contrib@onnxruntime@@
.?AV?$AttentionWrapper@M@contrib@onnxruntime@@
.?AV?$BahdanauAttention@M@contrib@onnxruntime@@
.?AV?$basic_filebuf@DU?$char_traits@D@std@@@std@@
.?AV?$basic_ifstream@DU?$char_traits@D@std@@@std@@
.?AV?$basic_ios@_WU?$char_traits@_W@std@@@std@@
.?AV?$basic_ios@DU?$char_traits@D@std@@@std@@
.?AV?$basic_iostream@DU?$char_traits@D@std@@@std@@
.?AV?$basic_istream@DU?$char_traits@D@std@@@std@@
.?AV?$basic_ofstream@DU?$char_traits@D@std@@@std@@
.?AV?$basic_ostream@_WU?$char_traits@_W@std@@@std@@
.?AV?$basic_ostream@DU?$char_traits@D@std@@@std@@
.?AV?$basic_ostringstream@_WU?$char_traits@_W@std@@V?$allocator@_W@2@@std@@
.?AV?$basic_ostringstream@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@
.?AV?$basic_streambuf@_WU?$char_traits@_W@std@@@std@@
.?AV?$basic_streambuf@DU?$char_traits@D@std@@@std@@
.?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@
.?AV?$basic_stringbuf@_WU?$char_traits@_W@std@@V?$allocator@_W@2@@std@@
.?AV?$basic_stringbuf@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@
.?AV?$basic_stringstream@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@
.?AV?$BatchNorm@M@onnxruntime@@
.?AV?$BatchNorm@N@onnxruntime@@
.?AV?$BiasGelu@M$00@contrib@onnxruntime@@
.?AV?$BiasGelu@M$0A@@contrib@onnxruntime@@
.?AV?$BinarizerOp@M@ml@onnxruntime@@
.?AV?$BitShift@_K@onnxruntime@@
.?AV?$BitShift@E@onnxruntime@@
.?AV?$BitShift@I@onnxruntime@@
.?AV?$CDist@M@contrib@onnxruntime@@
.?AV?$CDist@N@contrib@onnxruntime@@
.?AV?$Clip_6@M@onnxruntime@@
.?AV?$Clip_6Base@M@clip_internal@onnxruntime@@
.?AV?$codecvt@_WDU_Mbstatet@@@std@@
.?AV?$codecvt@DDU_Mbstatet@@@std@@
.?AV?$codecvt@GDU_Mbstatet@@@std@@
.?AV?$codecvt_utf8@_W$0BAPPPP@$0A@@std@@
.?AV?$collate@_W@std@@
.?AV?$collate@D@std@@
.?AV?$collate@G@std@@
.?AV?$ConstantOfShapeBase@U?$TypeList@UMLFloat16@onnxruntime@@MNCFH_JEGI_K_N@onnxruntime@@@onnxruntime@@
.?AV?$Conv@M@onnxruntime@@
.?AV?$ConvTranspose@M@onnxruntime@@
.?AV?$ConvTransposeWithDynamicPads@M@contrib@onnxruntime@@
.?AV?$Cos@M@onnxruntime@@
.?AV?$Cosh@M@onnxruntime@@
.?AV?$Crop@M@contrib@onnxruntime@@
.?AV?$CropAndResize@M@contrib@onnxruntime@@
.?AV?$ctype@_W@std@@
.?AV?$ctype@D@std@@
.?AV?$ctype@G@std@@
.?AV?$CumSum@_J@onnxruntime@@
.?AV?$CumSum@H@onnxruntime@@
.?AV?$CumSum@M@onnxruntime@@
.?AV?$CumSum@N@onnxruntime@@
.?AV?$DepthToSpace@M@onnxruntime@@
.?AV?$DequantizeLinear@C@onnxruntime@@
.?AV?$DequantizeLinear@E@onnxruntime@@
.?AV?$DequantizeLinear@H@onnxruntime@@
.?AV?$Det@M@onnxruntime@@
.?AV?$DictVectorizerOp@_JM@ml@onnxruntime@@
.?AV?$DictVectorizerOp@_JN@ml@onnxruntime@@
.?AV?$DictVectorizerOp@_JV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@ml@onnxruntime@@
.?AV?$DictVectorizerOp@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_J@ml@onnxruntime@@
.?AV?$DictVectorizerOp@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@ml@onnxruntime@@
.?AV?$DictVectorizerOp@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@N@ml@onnxruntime@@
.?AV?$Div@_J@onnxruntime@@
.?AV?$Div@H@onnxruntime@@
.?AV?$Div@M@onnxruntime@@
.?AV?$Div@N@onnxruntime@@
.?AV?$DmlOperatorActivationTemplate@$0CD@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CE@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CF@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CG@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CH@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CJ@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CK@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CL@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CM@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CN@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CO@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0CP@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0DA@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0DB@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0DC@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0DD@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0DE@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0FK@@Dml@@
.?AV?$DmlOperatorActivationTemplate@$0IA@@Dml@@
.?AV?$DmlOperatorConvolutionTemplate@$00$00$00@Dml@@
.?AV?$DmlOperatorConvolutionTemplate@$00$00$0A@@Dml@@
.?AV?$DmlOperatorConvolutionTemplate@$00$0A@$0A@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_ADD_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_ADD1_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_DIVIDE_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_AND_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_EQUALS_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_GREATER_THAN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_GREATER_THAN_OR_EQUAL_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_LESS_THAN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_LESS_THAN_OR_EQUAL_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_OR_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_XOR_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_MULTIPLY_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_SUBTRACT_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_ADD_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_ADD1_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_MAX_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_MIN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseQLinear@UDML_ELEMENT_WISE_DEQUANTIZE_LINEAR_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseQLinear@UDML_ELEMENT_WISE_QUANTIZE_LINEAR_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ABS_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ACOS_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ACOSH_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ASIN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ASINH_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ATAN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ATANH_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_CEIL_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_COS_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_COSH_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ERF_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_EXP_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_FLOOR_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_IS_NAN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_LOG_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_LOGICAL_NOT_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_RECIP_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SIGN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SIN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SINH_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SQRT_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_TAN_OPERATOR_DESC@@@Dml@@
.?AV?$DmlOperatorPoolingTemplate@$0DI@$00@Dml@@
.?AV?$DmlOperatorPoolingTemplate@$0DI@$0A@@Dml@@
.?AV?$DmlOperatorPoolingTemplate@$0DJ@$00@Dml@@
.?AV?$DmlOperatorPoolingTemplate@$0DJ@$0A@@Dml@@
.?AV?$DmlOperatorPoolingTemplate@$0DK@$00@Dml@@
.?AV?$DmlOperatorPoolingTemplate@$0GO@$0A@@Dml@@
.?AV?$DmlOperatorReduceTemplate@$00@Dml@@
.?AV?$DmlOperatorReduceTemplate@$01@Dml@@
.?AV?$DmlOperatorReduceTemplate@$02@Dml@@
.?AV?$DmlOperatorReduceTemplate@$03@Dml@@
.?AV?$DmlOperatorReduceTemplate@$04@Dml@@
.?AV?$DmlOperatorReduceTemplate@$05@Dml@@
.?AV?$DmlOperatorReduceTemplate@$06@Dml@@
.?AV?$DmlOperatorReduceTemplate@$07@Dml@@
.?AV?$DmlOperatorReduceTemplate@$08@Dml@@
.?AV?$DmlOperatorReduceTemplate@$09@Dml@@
.?AV?$DmlOperatorReduceTemplate@$0A@@Dml@@
.?AV?$DmlOperatorReduceTemplate@$0L@@Dml@@
.?AV?$Dropout@MM@onnxruntime@@
.?AV?$Dropout@MN@onnxruntime@@
.?AV?$Dropout@NM@onnxruntime@@
.?AV?$Dropout@NN@onnxruntime@@
.?AV?$DynamicQuantizeLinear@E@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@_J@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@_K@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@C@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@E@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@F@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@G@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@H@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@I@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Abs@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Ceil@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Elu@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Exp@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Exp@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Floor@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$HardSigmoid@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$LeakyRelu@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Log@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Log@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Neg@_J@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Neg@C@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Neg@H@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Neg@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Neg@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$ParametricSoftplus@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Reciprocal@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Reciprocal@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Relu@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Relu@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$ScaledTanh@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Selu@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Sigmoid@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Sigmoid@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Softplus@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Softsign@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Sqrt@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Sqrt@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Tanh@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$Tanh@N@functors@onnxruntime@@@onnxruntime@@
.?AV?$ElementWiseKernel@U?$ThresholdedRelu@M@functors@onnxruntime@@@onnxruntime@@
.?AV?$EmbedLayerNorm@M@contrib@onnxruntime@@
.?AV?$Equal@_J@onnxruntime@@
.?AV?$Equal@_N@onnxruntime@@
.?AV?$Equal@H@onnxruntime@@
.?AV?$Equal@M@onnxruntime@@
.?AV?$Equal@N@onnxruntime@@
.?AV?$Erf@M@onnxruntime@@
.?AV?$Expand@_J@onnxruntime@@
.?AV?$Expand@_K@onnxruntime@@
.?AV?$Expand@_N@onnxruntime@@
.?AV?$Expand@C@onnxruntime@@
.?AV?$Expand@E@onnxruntime@@
.?AV?$Expand@F@onnxruntime@@
.?AV?$Expand@G@onnxruntime@@
.?AV?$Expand@H@onnxruntime@@
.?AV?$Expand@I@onnxruntime@@
.?AV?$Expand@M@onnxruntime@@
.?AV?$Expand@N@onnxruntime@@
.?AV?$Expand@UMLFloat16@onnxruntime@@@onnxruntime@@
.?AV?$Expand_8@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@onnxruntime@@
.?AV?$FusedGemm@M@contrib@onnxruntime@@
.?AV?$Gelu@M@contrib@onnxruntime@@
.?AV?$Gemm@M@onnxruntime@@
.?AV?$Gemm@N@onnxruntime@@
.?AV?$Greater@_J@onnxruntime@@
.?AV?$Greater@H@onnxruntime@@
.?AV?$Greater@M@onnxruntime@@
.?AV?$Greater@N@onnxruntime@@
.?AV?$Hardmax@M@onnxruntime@@
.?AV?$IAttentionMechanism@M@contrib@onnxruntime@@
.?AV?$IdentityOp@$00@onnxruntime@@
.?AV?$IdentityOp@$0A@@onnxruntime@@
.?AV?$ImageScaler@M@contrib@onnxruntime@@
.?AV?$InstanceNorm@M@onnxruntime@@
.?AV?$IsNaN@M@onnxruntime@@
.?AV?$IsNaN@UMLFloat16@onnxruntime@@@onnxruntime@@
.?AV?$LabelEncoder_2@_J_J@ml@onnxruntime@@
.?AV?$LabelEncoder_2@_JM@ml@onnxruntime@@
.?AV?$LabelEncoder_2@_JV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@ml@onnxruntime@@
.?AV?$LabelEncoder_2@M_J@ml@onnxruntime@@
.?AV?$LabelEncoder_2@MV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@ml@onnxruntime@@
.?AV?$LabelEncoder_2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_J@ml@onnxruntime@@
.?AV?$LabelEncoder_2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@ml@onnxruntime@@
.?AV?$LayerNorm@M$00@contrib@onnxruntime@@
.?AV?$LayerNorm@M$0A@@contrib@onnxruntime@@
.?AV?$LayerNorm@N$00@contrib@onnxruntime@@
.?AV?$LayerNorm@N$0A@@contrib@onnxruntime@@
.?AV?$Less@_J@onnxruntime@@
.?AV?$Less@H@onnxruntime@@
.?AV?$Less@M@onnxruntime@@
.?AV?$Less@N@onnxruntime@@
.?AV?$LpNorm@M@onnxruntime@@
.?AV?$LpNorm@N@onnxruntime@@
.?AV?$LRN@M@onnxruntime@@
.?AV?$map@_JMU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JM@std@@@2@@std@@
.?AV?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@MU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@std@@@2@@std@@
.?AV?$MapType@V?$map@_J_JU?$less@_J@std@@V?$allocator@U?$pair@$$CB_J_J@std@@@2@@std@@@onnxruntime@@
.?AV?$MapType@V?$map@_JMU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JM@std@@@2@@std@@@onnxruntime@@
.?AV?$MapType@V?$map@_JNU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JN@std@@@2@@std@@@onnxruntime@@
.?AV?$MapType@V?$map@_JV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$less@_J@2@V?$allocator@U?$pair@$$CB_JV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@std@@@2@@std@@@onnxruntime@@
.?AV?$MapType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_JU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_J@std@@@2@@std@@@onnxruntime@@
.?AV?$MapType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@MU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@std@@@2@@std@@@onnxruntime@@
.?AV?$MapType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@NU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@N@std@@@2@@std@@@onnxruntime@@
.?AV?$MapType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V12@U?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V12@@std@@@2@@std@@@onnxruntime@@
.?AV?$MatMul@_J@onnxruntime@@
.?AV?$MatMul@H@onnxruntime@@
.?AV?$MatMul@M@onnxruntime@@
.?AV?$MatMul@N@onnxruntime@@
.?AV?$MatMulInteger16@FFH@contrib@onnxruntime@@
.?AV?$Max_6@M@onnxruntime@@
.?AV?$Mean_6@M@onnxruntime@@
.?AV?$Mean_8@M@onnxruntime@@
.?AV?$MeanVarianceNormalization_0@M@onnxruntime@@
.?AV?$MeanVarianceNormalization_1@M@onnxruntime@@
.?AV?$messages@_W@std@@
.?AV?$messages@D@std@@
.?AV?$messages@G@std@@
.?AV?$Min_6@M@onnxruntime@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CD@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CE@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CF@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CG@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CH@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CJ@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CK@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CL@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CM@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CN@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CO@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0CP@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0DA@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0DB@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0DC@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0DD@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0DE@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0FK@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorActivationTemplate@$0IA@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorConvolutionTemplate@$00$00$00@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorConvolutionTemplate@$00$00$0A@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorConvolutionTemplate@$00$0A@$0A@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_ADD_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_ADD1_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_DIVIDE_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_AND_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_EQUALS_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_GREATER_THAN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_GREATER_THAN_OR_EQUAL_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_LESS_THAN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_LESS_THAN_OR_EQUAL_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_OR_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_LOGICAL_XOR_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_MULTIPLY_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinary@UDML_ELEMENT_WISE_SUBTRACT_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_ADD_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_ADD1_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_MAX_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseBinaryLoop@UDML_ELEMENT_WISE_MIN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseQLinear@UDML_ELEMENT_WISE_DEQUANTIZE_LINEAR_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseQLinear@UDML_ELEMENT_WISE_QUANTIZE_LINEAR_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ABS_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ACOS_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ACOSH_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ASIN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ASINH_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ATAN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ATANH_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_CEIL_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_COS_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_COSH_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_ERF_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_EXP_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_FLOOR_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_IS_NAN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_LOG_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_LOGICAL_NOT_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_RECIP_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SIGN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SIN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SINH_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_SQRT_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorElementwiseUnary@UDML_ELEMENT_WISE_TAN_OPERATOR_DESC@@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorPoolingTemplate@$0DI@$00@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorPoolingTemplate@$0DI@$0A@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorPoolingTemplate@$0DJ@$00@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorPoolingTemplate@$0DJ@$0A@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorPoolingTemplate@$0DK@$00@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorPoolingTemplate@$0GO@$0A@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$00@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$01@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$02@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$03@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$04@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$05@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$06@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$07@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$08@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$09@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$0A@@Dml@@@@
.?AV?$MLOperatorKernel@V?$DmlOperatorReduceTemplate@$0L@@Dml@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorEinSum@Dml@@$0M@@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorPadding@Dml@@$06@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorPadding@Dml@@$0L@@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorRegionOfInterestAlign@Dml@@$09@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorResize@Dml@@$06@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorResize@Dml@@$08@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorResize@Dml@@$09@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorResize@Dml@@$0L@@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorSlice@Dml@@$06@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorSlice@Dml@@$09@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorSlice@Dml@@$0L@@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorTopK@Dml@@$06@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorTopK@Dml@@$09@@@@
.?AV?$MLOperatorKernel@V?$VersionedKernel@VDmlOperatorTopK@Dml@@$0L@@@@@
.?AV?$MLOperatorKernel@VDmlOperatorAffine@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorBatchNormalization@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorCast@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorConcat@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorConstantOfShape@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorConvInteger@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorCopy@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorCrop@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorCumSum@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorDepthToSpace@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorDynamicQuantizeLinear@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseBitShift@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseClip11@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseClip7@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseIf@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseIsInf@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseMean@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseMod@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwisePow@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorElementwiseRound@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorExpand@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorEyeLike@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorGatedRecurrentUnit@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorGather@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorGatherElements@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorGatherNd@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorGemm@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorInstanceNormalization@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorLocalResponseNormalization@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorLongShortTermUnit@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorLpNormalization@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorMatMul@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorMatMulInteger@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorMaxUnpool@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorMeanVarNormalization@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorMemcpy@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorNeg@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorOneHot@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorQLinearAdd@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorQLinearConv@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorQLinearMatMul@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorRange@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorRecurrentNeuralNetwork@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorRegionOfInterestPooling@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorReverseSequence@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorScatter@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorScatterNd@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorSpaceToDepth@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorSplit@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorTile@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorTranspose@Dml@@@@
.?AV?$MLOperatorKernel@VDmlOperatorValueScale2d@Dml@@@@
.?AV?$money_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$money_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$money_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$money_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$money_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$money_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$moneypunct@_W$00@std@@
.?AV?$moneypunct@_W$0A@@std@@
.?AV?$moneypunct@D$00@std@@
.?AV?$moneypunct@D$0A@@std@@
.?AV?$moneypunct@G$00@std@@
.?AV?$moneypunct@G$0A@@std@@
.?AV?$Mul@_J@onnxruntime@@
.?AV?$Mul@H@onnxruntime@@
.?AV?$Mul@M@onnxruntime@@
.?AV?$Mul@N@onnxruntime@@
.?AV?$NonTensorType@V?$map@_J_JU?$less@_J@std@@V?$allocator@U?$pair@$$CB_J_J@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$map@_JMU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JM@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$map@_JNU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JN@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$map@_JV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$less@_J@2@V?$allocator@U?$pair@$$CB_JV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_JU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_J@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@MU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@NU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@N@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V12@U?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V12@@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$vector@V?$map@_JMU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JM@std@@@2@@std@@V?$allocator@V?$map@_JMU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JM@std@@@2@@std@@@2@@std@@@onnxruntime@@
.?AV?$NonTensorType@V?$vector@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@MU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@std@@@2@@std@@V?$allocator@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@MU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@std@@@2@@std@@@2@@std@@@onnxruntime@@
.?AV?$NonZero@_J@onnxruntime@@
.?AV?$NonZero@_N@onnxruntime@@
.?AV?$NonZero@E@onnxruntime@@
.?AV?$NonZero@H@onnxruntime@@
.?AV?$NonZero@M@onnxruntime@@
.?AV?$num_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$num_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$num_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$num_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$num_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$num_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$numpunct@_W@std@@
.?AV?$numpunct@D@std@@
.?AV?$numpunct@G@std@@
.?AV?$OneHotEncoderOp@_J@ml@onnxruntime@@
.?AV?$OneHotEncoderOp@M@ml@onnxruntime@@
.?AV?$OneHotEncoderOp@N@ml@onnxruntime@@
.?AV?$OneHotEncoderOp@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@ml@onnxruntime@@
.?AV?$OneHotOp@_J_J_J@onnxruntime@@
.?AV?$OneHotOp@_JHM@onnxruntime@@
.?AV?$OneHotOp@_JM_J@onnxruntime@@
.?AV?$OneHotOp@_JMH@onnxruntime@@
.?AV?$OneHotOp@_JMM@onnxruntime@@
.?AV?$OneHotOp@_JV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_J@onnxruntime@@
.?AV?$OneHotOp@HMH@onnxruntime@@
.?AV?$OneHotOp@HMM@onnxruntime@@
.?AV?$OneHotOp@M_J_J@onnxruntime@@
.?AV?$OneHotOp@MMM@onnxruntime@@
.?AV?$OneHotOp@MV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@_J@onnxruntime@@
.?AV?$OpNodeInfoWrapper@UInferenceContext@onnx@@V?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTypeInferenceContext@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@WRL@Microsoft@@Unull_type@onnxruntime@@@Adapter@MachineLearning@AI@Windows@@
.?AV?$OpNodeInfoWrapper@VProtoHelperNodeContext@onnxruntime@@V?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorKernelCreationContextPrivate@@UIMLOperatorKernelCreationContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTensorShapeDescription@@UIMLOperatorAttributes1@@@WRL@Microsoft@@Unull_type@2@@Adapter@MachineLearning@AI@Windows@@
.?AV?$OpNodeInfoWrapper@VProtoHelperNodeContext@onnxruntime@@V?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@WRL@Microsoft@@Unull_type@2@@Adapter@MachineLearning@AI@Windows@@
.?AV?$OpNodeInfoWrapper@VProtoHelperNodeContext@onnxruntime@@V?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorSupportQueryContextPrivate@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@VNil@Details@WRL@Microsoft@@V4567@V4567@V4567@V4567@V4567@V4567@@23@@WRL@Microsoft@@Unull_type@2@@Adapter@MachineLearning@AI@Windows@@
.?AV?$Pool@MV?$MaxPool@$00@onnxruntime@@@onnxruntime@@
.?AV?$Pool@MVAveragePool@onnxruntime@@@onnxruntime@@
.?AV?$Pool@MVLpPool@onnxruntime@@@onnxruntime@@
.?AV?$PRelu@M@onnxruntime@@
.?AV?$PrimitiveDataType@_J@onnxruntime@@
.?AV?$PrimitiveDataType@_K@onnxruntime@@
.?AV?$PrimitiveDataType@_N@onnxruntime@@
.?AV?$PrimitiveDataType@C@onnxruntime@@
.?AV?$PrimitiveDataType@E@onnxruntime@@
.?AV?$PrimitiveDataType@F@onnxruntime@@
.?AV?$PrimitiveDataType@G@onnxruntime@@
.?AV?$PrimitiveDataType@H@onnxruntime@@
.?AV?$PrimitiveDataType@I@onnxruntime@@
.?AV?$PrimitiveDataType@M@onnxruntime@@
.?AV?$PrimitiveDataType@N@onnxruntime@@
.?AV?$PrimitiveDataType@UBFloat16@onnxruntime@@@onnxruntime@@
.?AV?$PrimitiveDataType@UMLFloat16@onnxruntime@@@onnxruntime@@
.?AV?$PrimitiveDataType@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@onnxruntime@@
.?AV?$QAttention@M@contrib@onnxruntime@@
.?AV?$QLinearAdd@C@contrib@onnxruntime@@
.?AV?$QLinearAdd@E@contrib@onnxruntime@@
.?AV?$QLinearLeakyRelu@C@contrib@onnxruntime@@
.?AV?$QLinearLeakyRelu@E@contrib@onnxruntime@@
.?AV?$QLinearLookupBase@C@contrib@onnxruntime@@
.?AV?$QLinearLookupBase@E@contrib@onnxruntime@@
.?AV?$QLinearMul@C@contrib@onnxruntime@@
.?AV?$QLinearMul@E@contrib@onnxruntime@@
.?AV?$QLinearSigmoid@C@contrib@onnxruntime@@
.?AV?$QLinearSigmoid@E@contrib@onnxruntime@@
.?AV?$QuantizeLinear@C@onnxruntime@@
.?AV?$QuantizeLinear@E@onnxruntime@@
.?AV?$ReduceKernel@$00@onnxruntime@@
.?AV?$ReduceKernel@$0A@@onnxruntime@@
.?AV?$ReduceKernelBase@$00@onnxruntime@@
.?AV?$ReduceKernelBase@$0A@@onnxruntime@@
.?AV?$ReduceL1@H@onnxruntime@@
.?AV?$ReduceL1@M@onnxruntime@@
.?AV?$ReduceL2@H@onnxruntime@@
.?AV?$ReduceL2@M@onnxruntime@@
.?AV?$ReduceLogSum@H@onnxruntime@@
.?AV?$ReduceLogSum@M@onnxruntime@@
.?AV?$ReduceLogSumExp@H@onnxruntime@@
.?AV?$ReduceLogSumExp@M@onnxruntime@@
.?AV?$ReduceLogSumExp@N@onnxruntime@@
.?AV?$ReduceMax@_J@onnxruntime@@
.?AV?$ReduceMax@C@onnxruntime@@
.?AV?$ReduceMax@E@onnxruntime@@
.?AV?$ReduceMax@H@onnxruntime@@
.?AV?$ReduceMax@M@onnxruntime@@
.?AV?$ReduceMax@N@onnxruntime@@
.?AV?$ReduceMean@H@onnxruntime@@
.?AV?$ReduceMean@M@onnxruntime@@
.?AV?$ReduceMean@N@onnxruntime@@
.?AV?$ReduceMin@_J@onnxruntime@@
.?AV?$ReduceMin@C@onnxruntime@@
.?AV?$ReduceMin@E@onnxruntime@@
.?AV?$ReduceMin@H@onnxruntime@@
.?AV?$ReduceMin@M@onnxruntime@@
.?AV?$ReduceMin@N@onnxruntime@@
.?AV?$ReduceProd@_J@onnxruntime@@
.?AV?$ReduceProd@H@onnxruntime@@
.?AV?$ReduceProd@M@onnxruntime@@
.?AV?$ReduceSum@_J@onnxruntime@@
.?AV?$ReduceSum@H@onnxruntime@@
.?AV?$ReduceSum@M@onnxruntime@@
.?AV?$ReduceSum@N@onnxruntime@@
.?AV?$ReduceSumSquare@H@onnxruntime@@
.?AV?$ReduceSumSquare@M@onnxruntime@@
.?AV?$ReduceSumSquare@N@onnxruntime@@
.?AV?$Resize@E@onnxruntime@@
.?AV?$Resize@H@onnxruntime@@
.?AV?$Resize@M@onnxruntime@@
.?AV?$RNN@M@onnxruntime@@
.?AV?$RoiAlign@M@onnxruntime@@
.?AV?$RoiAlign@N@onnxruntime@@
.?AV?$RoiPool@M@onnxruntime@@
.?AV?$Round@M@onnxruntime@@
.?AV?$Round@N@onnxruntime@@
.?AV?$Round@UMLFloat16@onnxruntime@@@onnxruntime@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorKernelCreationContextPrivate@@UIMLOperatorKernelCreationContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTensorShapeDescription@@UIMLOperatorAttributes1@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTypeInferenceContext@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@U?$ChainInterfaces@UIMLOperatorSupportQueryContextPrivate@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@VNil@Details@WRL@Microsoft@@V4567@V4567@V4567@V4567@V4567@V4567@@23@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIExecutionProvider@Dml@@UIWinmlExecutionProvider@Adapter@MachineLearning@AI@Windows@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIMLOperatorKernel@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIMLOperatorKernelContext@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIMLOperatorKernelFactory@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIMLOperatorRegistry@@UIMLOperatorRegistryPrivate@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIMLOperatorShapeInferrer@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIMLOperatorSupportQueryPrivate@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIMLOperatorTensor@@@WRL@Microsoft@@
.?AV?$RuntimeClass@U?$RuntimeClassFlags@$01@WRL@Microsoft@@UIUnknown@@@WRL@Microsoft@@
.?AV?$RuntimeClassBaseT@$01@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00U?$ChainInterfaces@UIMLOperatorKernelCreationContextPrivate@@UIMLOperatorKernelCreationContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTensorShapeDescription@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00U?$ChainInterfaces@UIMLOperatorShapeInferenceContextPrivate@@UIMLOperatorShapeInferenceContext@@VNil@Details@WRL@Microsoft@@V3456@V3456@V3456@V3456@V3456@V3456@V3456@@23@UIMLOperatorTypeInferenceContext@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00U?$ChainInterfaces@UIMLOperatorSupportQueryContextPrivate@@UIMLOperatorAttributes@@UIMLOperatorAttributes1@@VNil@Details@WRL@Microsoft@@V4567@V4567@V4567@V4567@V4567@V4567@@23@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIExecutionProvider@Dml@@UIWinmlExecutionProvider@Adapter@MachineLearning@AI@Windows@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIMLOperatorKernel@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIMLOperatorKernelContext@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIMLOperatorKernelFactory@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIMLOperatorRegistry@@UIMLOperatorRegistryPrivate@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIMLOperatorShapeInferrer@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIMLOperatorSupportQueryPrivate@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIMLOperatorTensor@@@Details@WRL@Microsoft@@
.?AV?$RuntimeClassImpl@U?$RuntimeClassFlags@$01@WRL@Microsoft@@$00$0A@$00UIUnknown@@@Details@WRL@Microsoft@@
.?AV?$SampleOp@M@contrib@onnxruntime@@
.?AV?$Scale@M@contrib@onnxruntime@@
.?AV?$ScalerOp@_J@ml@onnxruntime@@
.?AV?$ScalerOp@H@ml@onnxruntime@@
.?AV?$ScalerOp@M@ml@onnxruntime@@
.?AV?$ScalerOp@N@ml@onnxruntime@@
.?AV?$Scan@$07@onnxruntime@@
.?AV?$Scan@$08@onnxruntime@@
.?AV?$SequenceTensorType@_J@onnxruntime@@
.?AV?$SequenceTensorType@_K@onnxruntime@@
.?AV?$SequenceTensorType@_N@onnxruntime@@
.?AV?$SequenceTensorType@C@onnxruntime@@
.?AV?$SequenceTensorType@E@onnxruntime@@
.?AV?$SequenceTensorType@F@onnxruntime@@
.?AV?$SequenceTensorType@G@onnxruntime@@
.?AV?$SequenceTensorType@H@onnxruntime@@
.?AV?$SequenceTensorType@I@onnxruntime@@
.?AV?$SequenceTensorType@M@onnxruntime@@
.?AV?$SequenceTensorType@N@onnxruntime@@
.?AV?$SequenceTensorType@UBFloat16@onnxruntime@@@onnxruntime@@
.?AV?$SequenceTensorType@UMLFloat16@onnxruntime@@@onnxruntime@@
.?AV?$SequenceTensorType@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@onnxruntime@@
.?AV?$SequenceType@V?$vector@V?$map@_JMU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JM@std@@@2@@std@@V?$allocator@V?$map@_JMU?$less@_J@std@@V?$allocator@U?$pair@$$CB_JM@std@@@2@@std@@@2@@std@@@onnxruntime@@
.?AV?$SequenceType@V?$vector@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@MU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@std@@@2@@std@@V?$allocator@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@MU?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@M@std@@@2@@std@@@2@@std@@@onnxruntime@@
.?AV?$Sin@M@onnxruntime@@
.?AV?$Sin@N@onnxruntime@@
.?AV?$Sinh@M@onnxruntime@@
.?AV?$SkipLayerNorm@M@contrib@onnxruntime@@
.?AV?$SkipLayerNorm@N@contrib@onnxruntime@@
.?AV?$Softmax@M@onnxruntime@@
.?AV?$Softmax@N@onnxruntime@@
.?AV?$SpaceToDepth@M@onnxruntime@@
.?AV?$SparseTensorType@_J@onnxruntime@@
.?AV?$SparseTensorType@_K@onnxruntime@@
.?AV?$SparseTensorType@_N@onnxruntime@@
.?AV?$SparseTensorType@C@onnxruntime@@
.?AV?$SparseTensorType@E@onnxruntime@@
.?AV?$SparseTensorType@F@onnxruntime@@
.?AV?$SparseTensorType@G@onnxruntime@@
.?AV?$SparseTensorType@H@onnxruntime@@
.?AV?$SparseTensorType@I@onnxruntime@@
.?AV?$SparseTensorType@M@onnxruntime@@
.?AV?$SparseTensorType@N@onnxruntime@@
.?AV?$SparseTensorType@UBFloat16@onnxruntime@@@onnxruntime@@
.?AV?$SparseTensorType@UMLFloat16@onnxruntime@@@onnxruntime@@
.?AV?$Sub@_J@onnxruntime@@
.?AV?$Sub@H@onnxruntime@@
.?AV?$Sub@M@onnxruntime@@
.?AV?$Sub@N@onnxruntime@@
.?AV?$Sum_6@M@onnxruntime@@
.?AV?$Sum_6@N@onnxruntime@@
.?AV?$Sum_8@M@onnxruntime@@
.?AV?$Sum_8@N@onnxruntime@@
.?AV?$SVMRegressor@M@ml@onnxruntime@@
.?AV?$Tan@M@onnxruntime@@
.?AV?$TensorType@_J@onnxruntime@@
.?AV?$TensorType@_K@onnxruntime@@
.?AV?$TensorType@_N@onnxruntime@@
.?AV?$TensorType@C@onnxruntime@@
.?AV?$TensorType@E@onnxruntime@@
.?AV?$TensorType@F@onnxruntime@@
.?AV?$TensorType@G@onnxruntime@@
.?AV?$TensorType@H@onnxruntime@@
.?AV?$TensorType@I@onnxruntime@@
.?AV?$TensorType@M@onnxruntime@@
.?AV?$TensorType@N@onnxruntime@@
.?AV?$TensorType@UBFloat16@onnxruntime@@@onnxruntime@@
.?AV?$TensorType@UMLFloat16@onnxruntime@@@onnxruntime@@
.?AV?$TensorType@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@onnxruntime@@
.?AV?$ThreadPoolTempl@VEnv@onnxruntime@@@concurrency@onnxruntime@@
.?AV?$time_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$time_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$time_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$time_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$time_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$time_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$TopK@$08M@onnxruntime@@
.?AV?$TopK@$08N@onnxruntime@@
.?AV?$TopK@$09M@onnxruntime@@
.?AV?$TopK@$09N@onnxruntime@@
.?AV?$TopK@$0L@_J@onnxruntime@@
.?AV?$TopK@$0L@M@onnxruntime@@
.?AV?$TopK@$0L@N@onnxruntime@@
.?AV?$TreeEnsembleClassifier@_J@ml@onnxruntime@@
.?AV?$TreeEnsembleClassifier@H@ml@onnxruntime@@
.?AV?$TreeEnsembleClassifier@M@ml@onnxruntime@@
.?AV?$TreeEnsembleClassifier@N@ml@onnxruntime@@
.?AV?$TreeEnsembleRegressor@M@ml@onnxruntime@@
.?AV?$TreeEnsembleRegressor@N@ml@onnxruntime@@
.?AV?$Unique@M@contrib@onnxruntime@@
.?AV?$Upsample@E@onnxruntime@@
.?AV?$Upsample@H@onnxruntime@@
.?AV?$Upsample@M@onnxruntime@@
.?AV?$VersionedKernel@VDmlOperatorEinSum@Dml@@$0M@@@
.?AV?$VersionedKernel@VDmlOperatorPadding@Dml@@$06@@
.?AV?$VersionedKernel@VDmlOperatorPadding@Dml@@$0L@@@
.?AV?$VersionedKernel@VDmlOperatorRegionOfInterestAlign@Dml@@$09@@
.?AV?$VersionedKernel@VDmlOperatorResize@Dml@@$06@@
.?AV?$VersionedKernel@VDmlOperatorResize@Dml@@$08@@
.?AV?$VersionedKernel@VDmlOperatorResize@Dml@@$09@@
.?AV?$VersionedKernel@VDmlOperatorResize@Dml@@$0L@@@
.?AV?$VersionedKernel@VDmlOperatorSlice@Dml@@$06@@
.?AV?$VersionedKernel@VDmlOperatorSlice@Dml@@$09@@
.?AV?$VersionedKernel@VDmlOperatorSlice@Dml@@$0L@@@
.?AV?$VersionedKernel@VDmlOperatorTopK@Dml@@$06@@
.?AV?$VersionedKernel@VDmlOperatorTopK@Dml@@$09@@
.?AV?$VersionedKernel@VDmlOperatorTopK@Dml@@$0L@@@
.?AV?$Walker@H@Regexp@re2@@
.?AV?$Walker@PAVRegexp@re2@@@Regexp@re2@@
.?AV?$Walker@UFrag@re2@@@Regexp@re2@@
.?AV?$Where@_J@onnxruntime@@
.?AV?$Where@E@onnxruntime@@
.?AV?$Where@H@onnxruntime@@
.?AV?$Where@M@onnxruntime@@
.?AV?$Where@N@onnxruntime@@
.?AV?$Where@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@onnxruntime@@
.?AV?$wstring_convert@V?$codecvt_utf8@_W$0BAPPPP@$0A@@std@@_WV?$allocator@_W@2@V?$allocator@D@2@@std@@
.?AV__non_rtti_object@std@@
.?AV_ExceptionPtr_normal@?A0x327206fc@@
.?AV_Facet_base@std@@
.?AV_Generic_error_category@std@@
.?AV_Iostream_error_category@std@@
.?AV_Locimp@locale@std@@
.?AV_Ref_count_base@std@@
.?AV_System_error@std@@
.?AV_System_error_category@std@@
.?AV<lambda_006f042491572090b778a6887556c541>@@
.?AV<lambda_007ddac2123b71f70c18b64d41e9be57>@@
.?AV<lambda_019248982c19876b36bd9d56cbf00fcf>@@
.?AV<lambda_01dd80b018ebfebbe11d16795278407c>@@
.?AV<lambda_021916736d514528559cd9f1df8e9615>@@
.?AV<lambda_02a16f31201b954c010d4c15805851b1>@@
.?AV<lambda_02dbf81fbe0e79782741ce39a831bb2f>@@
.?AV<lambda_02ebe20f47dd15b8224ab822fe208f4a>@@
.?AV<lambda_02f0fede11d4b9d7439baa125e06788b>@@
.?AV<lambda_031e8dfd8d6a123502d9ec2194443e60>@@
.?AV<lambda_034a872241814873b16b150e98786486>@@
.?AV<lambda_0367bcb44eefe2d25755a8cf3194c733>@@
.?AV<lambda_05416e20ce8aa4e6f975518b44ddcde1>@@
.?AV<lambda_056b0cb2615e46ba7d3a385c7daaf5a0>@@
.?AV<lambda_057831c89172b3e745c34953b854d875>@@
.?AV<lambda_064044131b4432293f10a20d775cf672>@@
.?AV<lambda_0753838d59fa1e3c1be3719b0022993e>@@
.?AV<lambda_077886625d25c359a62527fbaa92e49b>@@
.?AV<lambda_07cc7db52a628b03f17e7b1fd5adf33e>@@
.?AV<lambda_090ed96c0956ed48de6a40bf85283e7c>@@
.?AV<lambda_09804657b1d4ece14dbea3419a326007>@@
.?AV<lambda_09860caeb18fe5c6553c7cece043a6bf>@@
.?AV<lambda_09ccd5af3992c7f60b7106631dcd95eb>@@
.?AV<lambda_09d30a4170455b2cc3a6766e0ecf4621>@@
.?AV<lambda_0a1a247f671177308e98d8aa00caba3d>@@
.?AV<lambda_0a77da8549581038807d4f031ad4715d>@@
.?AV<lambda_0a786afdc494302a03a8347211af4f5e>@@
.?AV<lambda_0a9bedc68e3bb7f405a0cb52fae05ca5>@@
.?AV<lambda_0adf0d24f2c35fa4a66c3d2e349117c6>@@
.?AV<lambda_0b7b88c72b3c6e2d1499af326149f2ac>@@
.?AV<lambda_0c69e806d9e2fad559bbd11cd911ec3e>@@
.?AV<lambda_0cdf3ed2e001d703b14b9857f8148f3d>@@
.?AV<lambda_0ce5feabbe81f3ff83f9db21f1fe9470>@@
.?AV<lambda_0d1787685bfcab6f5be503e02e4ed79d>@@
.?AV<lambda_0d1cea9b21275bcc722a263f1867aab0>@@
.?AV<lambda_0d37934dc4686e9f8cdc4b98c71d039f>@@
.?AV<lambda_0d4d08f2f59cb77cf8d5afa00847341b>@@
.?AV<lambda_0d623cd1935e0ef85c92a37a55dd66c5>@@
.?AV<lambda_0e8d4dc99952bd2e942002a2267cbc9f>@@
.?AV<lambda_0ec02d5276c6a16fd616d10b796f4d2c>@@
.?AV<lambda_0ef38e5b4dd40a551bcbede7fadd1472>@@
.?AV<lambda_0f9a7ecc0cde6f755170ea655764ab2c>@@
.?AV<lambda_0fe8093ffea6514602a81f131e8a213b>@@
.?AV<lambda_104e5d4f3c346a5807a7db11389af990>@@
.?AV<lambda_11a93f40c7a11b73570d6d0a2bc4c32b>@@
.?AV<lambda_12065ee63fda2ae25d17bc16a002c579>@@
.?AV<lambda_125b77507bd9658139842e91e09bb467>@@
.?AV<lambda_12dd4c4dcfd6c2d8effe6071c823bd43>@@
.?AV<lambda_1359900828a9df3bd7d0f766b547ccff>@@
.?AV<lambda_13c0d49a798e8cd7f3691f8cf3d13739>@@
.?AV<lambda_1433cb9cd8f8490322ad74b6fa812120>@@
.?AV<lambda_14407bcf8cfc66db9a85fd2745a31d94>@@
.?AV<lambda_1476157c6a284e4f379191854345eea5>@@
.?AV<lambda_14cf9ddcdab5183a36b1158dec720c25>@@
.?AV<lambda_159e0921f0d4eb52aba9cdc091e10630>@@
.?AV<lambda_15ec4af4e71787ba502de1205d84eea4>@@
.?AV<lambda_16be1790a9760642272e826be4697ed1>@@
.?AV<lambda_1712de044b9947fe68befaed2d7ed848>@@
.?AV<lambda_175914cea4037e183fc735b60abe96c9>@@
.?AV<lambda_18c8bc7d6e1564eeb9ad39be83abfb9f>@@
.?AV<lambda_1929999175e850d5998c7c370f8aff7a>@@
.?AV<lambda_1a8b8132ed3c0ab63c63224ad4161527>@@
.?AV<lambda_1bcbff0149a8e25ea59b665acdead2ab>@@
.?AV<lambda_1cdda74d195f4dc7bbdbaed3ee174bf7>@@
.?AV<lambda_1d958ad8769ba514a23fca7415139480>@@
.?AV<lambda_1da5929b711c2681e91623a20d50a98f>@@
.?AV<lambda_1dab84a38765585bcd86328d60f37519>@@
.?AV<lambda_1ea0a0832a4cf660ea6a57031389b3fe>@@
.?AV<lambda_1f154a6fecf26a7ac5bcd91fb4939855>@@
.?AV<lambda_1fb55e109ef7e3477c3bb87242d95990>@@
.?AV<lambda_2036b51976e8571401119fff6c6a9260>@@
.?AV<lambda_21332c666c4782640bb01cbce240ea30>@@
.?AV<lambda_21a762d3684c6831db7ddce449ffa123>@@
.?AV<lambda_221457049dd6e599b1e592be3898266a>@@
.?AV<lambda_221cfc4539809ca9dda24743972ab18a>@@
.?AV<lambda_2242169c388de54aeef0fbc82d712874>@@
.?AV<lambda_22485ec429c0920bef998442ed4b0141>@@
.?AV<lambda_226d842a4bcdc8605120f4a2efdc3985>@@
.?AV<lambda_227060e20de8b811822001a4d5e06e66>@@
.?AV<lambda_236692037ce4d824baf1ec7c2be78fd2>@@
.?AV<lambda_236f7c8f49088d4c504fd71f55f4280b>@@
.?AV<lambda_240d5c2825474fb53004f276faf2b25d>@@
.?AV<lambda_24575e001393f6791a4c588c060b67fa>@@
.?AV<lambda_24d5fcc8102cdc77a3db148fe9344e7b>@@
.?AV<lambda_2529e063101eea511edf1238da0f8253>@@
.?AV<lambda_25fc2e6bd1e3daa78c370e70eea3061e>@@
.?AV<lambda_262fa984091282f85a5ab626cfdef42f>@@
.?AV<lambda_26620886b575de26166e451d44bc7aeb>@@
.?AV<lambda_2667760ea75d0d5cd7152eb3bcb76dfa>@@
.?AV<lambda_26bff8e61e9bb411091ea7936af6af7c>@@
.?AV<lambda_26ee2a828034f17afc207c8ad30c609a>@@
.?AV<lambda_2749f0a3e647c4a8e874fd0bb99b759f>@@
.?AV<lambda_27a631d2450c357a52927c1dfbd2efda>@@
.?AV<lambda_27e8a99165a81df90f9c1dae1c5fcca4>@@
.?AV<lambda_27eb941e257b57f9cf2ee9f3ef3087b3>@@
.?AV<lambda_2875b3e2e66b274d36ac70e9f0680993>@@
.?AV<lambda_29092bf385326e6f20e0b55a91956311>@@
.?AV<lambda_29460ba72c9a9f61937f695b212c2385>@@
.?AV<lambda_2997ea06f466f3cb7af7580139e0e9b1>@@
.?AV<lambda_2a6ee45563922231f230ab0594ec8ab5>@@
.?AV<lambda_2aa6dc840362673ab7e0d38f455745fb>@@
.?AV<lambda_2b2ce10a039b3e6bb8b2efe3128f96db>@@
.?AV<lambda_2bb569fe2197271e2b1302ced7b75d4a>@@
.?AV<lambda_2bc5f319d61b1e5d5273f7fc52f04186>@@
.?AV<lambda_2bf41ec2c8021b2e4a040cfc2c7e5e74>@@
.?AV<lambda_2cda3ec4180407fdb66cae08f370124f>@@
.?AV<lambda_2cfb41272b7bf611216d7e49d5041299>@@
.?AV<lambda_2e2760691203ec6d385de10317d43eb8>@@
.?AV<lambda_2e79487384136cbeb73bf5cd55164ae5>@@
.?AV<lambda_2f4103e1c4a1773101a6c1f055d56df4>@@
.?AV<lambda_2f6217725d36a8fee3233a5a3a8acbc4>@@
.?AV<lambda_2f80399ac55308892511bf2d4fa0c615>@@
.?AV<lambda_2f8e437cb9f95dd76b3982067268238d>@@
.?AV<lambda_2fd33bbe7c4a2b258d4d665b8341f671>@@
.?AV<lambda_301279dcdc0bb6da9000a75437e1d615>@@
.?AV<lambda_3199312d6fe856623dcfbc1d675d418f>@@
.?AV<lambda_324b41c3d7492264a93fa781c1a5839e>@@
.?AV<lambda_327bf988738179df2cb078834240ff6e>@@
.?AV<lambda_32893135699ceef534edcfb3e0168a55>@@
.?AV<lambda_32cb8615ab18e281529c5adcc2581f88>@@
.?AV<lambda_331d7506767c1ed93515cbb23d39b0d9>@@
.?AV<lambda_3354f944db7877754471d985be3fe29b>@@
.?AV<lambda_338e5fd506fd3a59b6f8e616d7366dcc>@@
.?AV<lambda_3418ac6d0a6ccb6c692baad9c09fd00f>@@
.?AV<lambda_345b51dbe4e505cf996585c6c69fef99>@@
.?AV<lambda_34eb695d67d7709c51f33f1b8d0d7456>@@
.?AV<lambda_34f93c0ee84ed95033247b350e3e9c13>@@
.?AV<lambda_3535579646aa11381d1f86e8f1dad915>@@
.?AV<lambda_355ced77e5add7bf5de4b6cde27fcd08>@@
.?AV<lambda_35b794944dfe770731cae452127f793f>@@
.?AV<lambda_3674353b915e5a5f4e97c63731ea3b7b>@@
.?AV<lambda_36945aef9b69b8bc246158e60ded74ff>@@
.?AV<lambda_36c84409ed98093c11b3d0d02c089264>@@
.?AV<lambda_371d862278e5d64352c8bab4a05583a0>@@
.?AV<lambda_37fc46271b5a9d577e78557058b76819>@@
.?AV<lambda_388a8e360d5910da0bc0c1d744f82389>@@
.?AV<lambda_38e1bdb2070443b647c40452b6c90c15>@@
.?AV<lambda_3918f71a17942b897d267f29eb5a4cb3>@@
.?AV<lambda_39a1fb20d4e6bdee774e59704a059816>@@
.?AV<lambda_3a8a940e89b14c9a8617dfa32f79a60a>@@
.?AV<lambda_3aa61f6258ac2da5aa1399bf10de3644>@@
.?AV<lambda_3bcb9b903ba1cf4e300afd0f291b2ee4>@@
.?AV<lambda_3bf9d7b5239a137326d2c5fa821fa737>@@
.?AV<lambda_3c57c4f415fca3790b9eb6cedd38e28f>@@
.?AV<lambda_3cd6c3d57d7e8805d712028bde485ef8>@@
.?AV<lambda_3d97bd48e7741aa8b9ee3562eae8be98>@@
.?AV<lambda_3deb1638445ba1a8c860cf9e9ece0b32>@@
.?AV<lambda_3e8fe92fc2a983eaaae8e23df9000d98>@@
.?AV<lambda_3f2f36db15d672a75a701714494919da>@@
.?AV<lambda_3f992177007ef3a6a4a7f362ca0d72b8>@@
.?AV<lambda_404f128c43627cca26dd30079f566078>@@
.?AV<lambda_40d7d549b7296d37bd8a375a6a32735f>@@
.?AV<lambda_40f74427ec28f20a5d59b70a3f7ba162>@@
.?AV<lambda_414a43a198ea3ff0dfdcf4ed89cfe63a>@@
.?AV<lambda_41ba827606a0dc44b626761483b4bac9>@@
.?AV<lambda_423c7f4514cb0f580d17c1969d94ab4d>@@
.?AV<lambda_4276ac6945fda74461cbbc3f55ea202e>@@
.?AV<lambda_429334e9c462f3e67003e15324abcc42>@@
.?AV<lambda_430c7041764d504802816f1b3fb25d0a>@@
.?AV<lambda_43719f9d8c83073f26885f37cf523fe1>@@
.?AV<lambda_43d76a454d7e446551a32b163679964a>@@
.?AV<lambda_446597e43ad5dfcf42334d9ec384bc3b>@@
.?AV<lambda_447f5b0eec09765ec587d8b36006ec6e>@@
.?AV<lambda_44f740f4362270fce5c964ebb50cbf77>@@
.?AV<lambda_452b080256e46867afd3b39c75897486>@@
.?AV<lambda_45e08ad47c9bff0006768a2c526864a3>@@
.?AV<lambda_4623cd97b09f16cc323b1da4783c1a74>@@
.?AV<lambda_4682f43d0273e3d01e9224f8a5dcf803>@@
.?AV<lambda_4692eb6e70108664f447bb895cd16b99>@@
.?AV<lambda_46e1dc415ba1567f93a11214638e2255>@@
.?AV<lambda_47109259520c4e793b98906acbd4073b>@@
.?AV<lambda_476f16e46856ad1afb9e2f3aa987cf47>@@
.?AV<lambda_4782a3788b4d37949c863bd3b1d2db4e>@@
.?AV<lambda_4895a69c6e2d4d3665d84240b52946d2>@@
.?AV<lambda_491a5ff9baf0c87e80ac72a0beb259ee>@@
.?AV<lambda_493755c0718e549fc6eb6376aaf1c076>@@
.?AV<lambda_4963f16a0eb793cb3ee709a47577548b>@@
.?AV<lambda_4a6aeb4175d7888941bfb44f87a822c0>@@
.?AV<lambda_4a825ecad3e2e0b4fc2d33026044f2b0>@@
.?AV<lambda_4b1533207937d10b38e9bd484b05643c>@@
.?AV<lambda_4b40e91ae8e8c37fb63980fee637f885>@@
.?AV<lambda_4b6a543f93b634de8567a05c1dc832e8>@@
.?AV<lambda_4baf23d0c206ed351c6ebf16f344f7e0>@@
.?AV<lambda_4c36c751efc3ade178fd6425d4750a0b>@@
.?AV<lambda_4cd082e17ba7994385c699d185cdacd5>@@
.?AV<lambda_4e3f051fdc405a96de9c485e123872ab>@@
.?AV<lambda_4e6fa5fab57276f6c38287bd072060d0>@@
.?AV<lambda_4e978ba75a8a0f3f4d2d1e75d74ac4de>@@
.?AV<lambda_4f86db608486d9d830b4d71843ace9e1>@@
.?AV<lambda_4fc71a60d0d1e6360508e57a55f6b5c9>@@
.?AV<lambda_50815597be56ed0e327183a6f984971c>@@
.?AV<lambda_509e8645eefb2a1b0561e4ee228415fa>@@
.?AV<lambda_5151cdb5e2d0e967b0ff9b516f4dde5e>@@
.?AV<lambda_5198f0fd761e3f604ccc31929c553d15>@@
.?AV<lambda_51e6c2078824708885b36339be574a42>@@
.?AV<lambda_51e7ce4347ad68176c6a114dbe1a7fb9>@@
.?AV<lambda_51fc723ea39c4518ba8a40d8ce3fda32>@@
.?AV<lambda_51ff266d4a509f82cc61194622d6aafc>@@
.?AV<lambda_5241072fb302daf3d92fe8af4902fcf1>@@
.?AV<lambda_5260c61ab07867c25056c192e14b912e>@@
.?AV<lambda_53958a524045125d538038a834c170f9>@@
.?AV<lambda_5455b7a3d85ed24852611958e08bf1e7>@@
.?AV<lambda_546a5101aad278ccc6f90af11bb20384>@@
.?AV<lambda_55a8c7f82471609cdeead8ac3d02d601>@@
.?AV<lambda_55c987d2288bf4751b7cab2cd3c58f7e>@@
.?AV<lambda_55dd3d0ebe2e48cd699a7e50bd7c1613>@@
.?AV<lambda_55e8e059d201c280196491784367b3c1>@@
.?AV<lambda_565473cc7704d655ba5df76fa41b94f3>@@
.?AV<lambda_56616cb04d479ecbbef60d16cb985ff3>@@
.?AV<lambda_566e56833ba647bded2d1d52059227e2>@@
.?AV<lambda_56708f1224b2fa079c579dbcc99f5723>@@
.?AV<lambda_576babf2632b8f119a1c6bd71dacdba2>@@
.?AV<lambda_577697c65f7f30ab5c890fb5e643c3c6>@@
.?AV<lambda_57b3023538a7039f2e0e4aacf995d8cd>@@
.?AV<lambda_5816ab89d71996ccace6db8ba2545f64>@@
.?AV<lambda_58385ecae81d2f32df5183427720040f>@@
.?AV<lambda_5887a420a41b7e61b78788bc0c22095d>@@
.?AV<lambda_58c175adcec771ca341f59d8965ef956>@@
.?AV<lambda_58e45bcf81f2eb4f6ae6b3f124defb13>@@
.?AV<lambda_5949c53d8ff4275e91f18552fb52fd7c>@@
.?AV<lambda_597c15d719a814d12541cbd2bbff60a5>@@
.?AV<lambda_598cf5f3152cf23096c5a28a030ee0b7>@@
.?AV<lambda_59d686db14cf1737be131f4a3fc9791f>@@
.?AV<lambda_59ed2dcfa7c0d744c558df76076fb0aa>@@
.?AV<lambda_5a03cad439a535c73c9479bab198ff8a>@@
.?AV<lambda_5b19bcf90126dbac847fa4a3a03b8aab>@@
.?AV<lambda_5ca3b2fb2261ad467c7e40fda304e71e>@@
.?AV<lambda_5cc43c49af3b89fd43e2aa8a8afc2e1b>@@
.?AV<lambda_5cd9b18fb4de22826b438d8133646797>@@
.?AV<lambda_5d86fa40c6f141e2f5308ab25a37373d>@@
.?AV<lambda_5daecdbe57f90568020b13975f1cd001>@@
.?AV<lambda_5dbbe93e2a9b11b44d01ac5bd9e5d0bd>@@
.?AV<lambda_5dddc316c01bb02791c37b03fa523bb0>@@
.?AV<lambda_5de94ebf8ac34b72c76fcea7a63f223e>@@
.?AV<lambda_5df54278b3724a9baf121a1f8852e38e>@@
.?AV<lambda_5e0adb550519bb305a282a49bc052ff5>@@
.?AV<lambda_5ead457e3eeae74d2baaa54a34ab876e>@@
.?AV<lambda_5f12e6a0112b95dae1a14ebbefa23368>@@
.?AV<lambda_5f4057cf46e1dea27ced686274a4ab95>@@
.?AV<lambda_5f5c509e4ecbb203bb441c84afd312d2>@@
.?AV<lambda_608411f62512d6d36e0c058dda512de6>@@
.?AV<lambda_6098da3ff9cb8b08151d3844bed31f70>@@
.?AV<lambda_609ecd802714b672c2c81632a57c73f4>@@
.?AV<lambda_60af20c304d3399c0b13e2dd97473212>@@
.?AV<lambda_6137ab0b6712207e9054d1c872b492dd>@@
.?AV<lambda_613905a3c12d2353d146a5d251b5caad>@@
.?AV<lambda_616493d339e7f246f89d6e49042ee686>@@
.?AV<lambda_61ce4a3a6a2e52170ed094a3091671ab>@@
.?AV<lambda_6277f32b20d3d158c3ebcc1771d29c1a>@@
.?AV<lambda_62f1ce9bc208e37c7e5739c8f36388ed>@@
.?AV<lambda_6367df6aa050372a33bd7465b9bffebf>@@
.?AV<lambda_637d27d6a01beef351cd6df162f9426e>@@
.?AV<lambda_6423e25c1122e8ea98faabe27ee57b24>@@
.?AV<lambda_64826d400df5e2683a863a4dbb954602>@@
.?AV<lambda_64c62750e999b82780ab6a59a6ba6851>@@
.?AV<lambda_65c7db542fd8e8c34231ca74f6edf804>@@
.?AV<lambda_66b6761e17b1698f92d4664857f34eef>@@
.?AV<lambda_670b3cf7fae0fc01758a26323e302b88>@@
.?AV<lambda_6730021c9a4195800fc17a38baeda605>@@
.?AV<lambda_67cab59cf9e4d6fff4a228b6f50a31f4>@@
.?AV<lambda_6829271c72aa0e0367fc0ccf7a9e3edb>@@
.?AV<lambda_684acbd5f928eb904d7ad61767659782>@@
.?AV<lambda_6857bc45ce6cabf2b8f5f4f7c03ce1fe>@@
.?AV<lambda_68ac02ac858e6ede64d0fbc5d9dfc15f>@@
.?AV<lambda_691026265f1e73c9b49221bbcaaf7e8d>@@
.?AV<lambda_69631e77932e21817a56851e5c6edc44>@@
.?AV<lambda_69b772cd14ccb0d3e3ddf4a285e464b6>@@
.?AV<lambda_69c32f0b6e172d0bef2d15e0c00baa25>@@
.?AV<lambda_69e57fd1b30d7ec9bd7519732750e12c>@@
.?AV<lambda_69fb29d342558f2407dd09c009c85823>@@
.?AV<lambda_6a160209166296c80c9867c0c24688ea>@@
.?AV<lambda_6a32611970906250c6b47d8292ad00e7>@@
.?AV<lambda_6ae1892ff4a4003e2e47baa65ff24d8d>@@
.?AV<lambda_6bf7af48e7a4158e20b4f833c15de130>@@
.?AV<lambda_6c2b6db637aede97107450673ae7cf3a>@@
.?AV<lambda_6c435e48e6003c8c444134537317003d>@@
.?AV<lambda_6d1a31765a629c9bc43dcea20434203a>@@
.?AV<lambda_6d3877051847983c73e11af2112f64fa>@@
.?AV<lambda_6d6f3d3230139a3d222e52865731387e>@@
.?AV<lambda_6db08e7b535fdfa2ab543d6397b422f2>@@
.?AV<lambda_6e1ae7feac7554001d4e77c1f621284e>@@
.?AV<lambda_6e98eced94f0a80de9a71b3e8fa901f1>@@
.?AV<lambda_6ee0b11a8d2719c95f7e0f11cb29efcc>@@
.?AV<lambda_6f80d65fbc6388042df99a3b220305d1>@@
.?AV<lambda_70279f22222c392cb493fce9840987c7>@@
.?AV<lambda_7029d41d4a95ff2575c8227d099f78bc>@@
.?AV<lambda_715d031581e8d9abada945eeadbab576>@@
.?AV<lambda_730e393831d92a8b1623c0ee3e0e1a73>@@
.?AV<lambda_7361ba811d5ff92bf50e102c73312480>@@
.?AV<lambda_738d98ba114f0b992fde43f3c2b8082f>@@
.?AV<lambda_740a6591f9964eaf27aee9c62c1bdfd5>@@
.?AV<lambda_747d01ad8121cb89860d325b0c86de9f>@@
.?AV<lambda_747f5a5054cea5042105b3988bb8e54a>@@
.?AV<lambda_74ee45c15b018f2c2709e09284f9b432>@@
.?AV<lambda_75106fbcafd9b6387725a0ac61d90f0b>@@
.?AV<lambda_76eec3e346e3038343d242b83e7ef7cf>@@
.?AV<lambda_76f850960fe6ac327acd4613d41835f7>@@
.?AV<lambda_771974d945874fd2518ef4fcc7999ce0>@@
.?AV<lambda_772b291d0a2826994f823d005fbf5dd0>@@
.?AV<lambda_77304148f0ddead6f8725fd6b1fe3a6f>@@
.?AV<lambda_7843dc957c42d3b8b9e2111287c74d54>@@
.?AV<lambda_795d004014b095ac6c3348f10d228401>@@
.?AV<lambda_7a6442793a842f584bc31790203846c7>@@
.?AV<lambda_7a970010ad38e3a1508f7194da0afc42>@@
.?AV<lambda_7abe23a5198e897647d783cb391d2ac6>@@
.?AV<lambda_7bf0c45af15d48ff0bf3482d9ab1530d>@@
.?AV<lambda_7c5eb1f068179d900c888fe3d5890426>@@
.?AV<lambda_7d18ce867b586dae3ed4309094f85b3d>@@
.?AV<lambda_7d4a5c6b07fd360e232dc4416f33eeb2>@@
.?AV<lambda_7d7d52dd8f31ce0fc48036fb1081b283>@@
.?AV<lambda_7e6056f43d2e17d5b39d88df6efe7e61>@@
.?AV<lambda_7e64dc56260f5aa848b67008058a8451>@@
.?AV<lambda_7ec65dfaaf4dc75c3f7f2cbd42331128>@@
.?AV<lambda_7ef4d7bf9a11acea8de40a758239a896>@@
.?AV<lambda_7fd088052d119656db3d6706c3514329>@@
.?AV<lambda_80083253b4a44259ccb0f1ab2453639e>@@
.?AV<lambda_8032534fac3fd5dda5243b4509fad456>@@
.?AV<lambda_80600ba38f1578c0943cc3b4cc8d084c>@@
.?AV<lambda_814d8d8f4b4ee5499087afcf23c8abef>@@
.?AV<lambda_839e4f861e16af089c617eb4d1d9047f>@@
.?AV<lambda_83eca28d6349860dc86100acaa252e03>@@
.?AV<lambda_843c10906c86ce430854ae817b2b31e9>@@
.?AV<lambda_8448e89789bef89a83708200e0dba2ac>@@
.?AV<lambda_845fb0d365668e61a65e09bae6280c09>@@
.?AV<lambda_850028fba9642b015a2c6ff81bf07107>@@
.?AV<lambda_8516815e67f3594718fcfcc07116860d>@@
.?AV<lambda_853403f8a1cb1bf480beff4c6dac6b21>@@
.?AV<lambda_85740bb65c1a7256d972833e138b25dd>@@
.?AV<lambda_85cd850b3a70267b82c425a1808d56e3>@@
.?AV<lambda_86574d63891f8965457453a282e51ac5>@@
.?AV<lambda_86895a13d56222c09289c706b79a6852>@@
.?AV<lambda_872c985ed95e21f46464410321ab9d0c>@@
.?AV<lambda_87377fe9a7b55ca099852704c7649a37>@@
.?AV<lambda_87386769d120dc3c5202374ca430a8d9>@@
.?AV<lambda_87cb9d4d5e38fb120a31a53965033d4d>@@
.?AV<lambda_888f1f17d6f86eade7f5261fdde46e6e>@@
.?AV<lambda_88b1496ecc213c07fd24b30bb5c856e7>@@
.?AV<lambda_88e4cbdd9578a1aef93a2f7d3b7ececc>@@
.?AV<lambda_8ac5cfcccd347e512ac9419fd63346cb>@@
.?AV<lambda_8bb411dd051c5b2789979ee8619f0375>@@
.?AV<lambda_8c122ed0b2783e809ca408dbc8378b71>@@
.?AV<lambda_8c32b6976971eb48d48ba6d5dd1a9ae2>@@
.?AV<lambda_8d2c2e51afd333ece2ae8f2a329fc9ee>@@
.?AV<lambda_8ea5687524a42143533b684f5c9ce181>@@
.?AV<lambda_8ec99f105e4604958a8ba328f7e77b1f>@@
.?AV<lambda_8ee9a80fb3df426a08cdee43eec5f4a1>@@
.?AV<lambda_902ce7e426f16d786ecf9d87a583aaa8>@@
.?AV<lambda_9222e83c56bbeb08f964662158da2721>@@
.?AV<lambda_93063d24d3eaf65455400e96fd395987>@@
.?AV<lambda_932674f6b1528de9101a6cf1da2f3619>@@
.?AV<lambda_932e39bc42b54acf3286176bb275e4a7>@@
.?AV<lambda_93640e529bc0ba0d303e1a662462ace2>@@
.?AV<lambda_93b65f05ddd7a75740e1646ed2ad5a8e>@@
.?AV<lambda_943cc09e2a717989dfbbaabfcc35437a>@@
.?AV<lambda_945932bb7c945ed8e789f6af7f6d509f>@@
.?AV<lambda_9461f2e660e0c1ae91a37f46d99b3e80>@@
.?AV<lambda_94623d72acfd58b8a8d7733e9b2c9c90>@@
.?AV<lambda_94f802a6045ff51c50912ff8c7e2bd10>@@
.?AV<lambda_9531d8dd38f435ce1e9840c5a24c7961>@@
.?AV<lambda_95a8f80a3b96610e783f7b34d80ac7bd>@@
.?AV<lambda_9671e9cce0def787c04a0ca1f741034c>@@
.?AV<lambda_974421a2166ea3684014171d0d75f69e>@@
.?AV<lambda_975b91ffee199f0bfc6f725443e3d8f8>@@
.?AV<lambda_976a37e7ee737ef6672be8fc72541275>@@
.?AV<lambda_977aa0989600c553e6ad21ef5f183702>@@
.?AV<lambda_9821a1e0b606be014cc2fca34ba53b1a>@@
.?AV<lambda_98b53a1372ec257c871e8ef28d59be07>@@
.?AV<lambda_991d63fb8a809dc84e4e523068ce486a>@@
.?AV<lambda_9935e3057ff6619551b128a634c37edb>@@
.?AV<lambda_9974bd71538aef5818f74e4a5e52a52e>@@
.?AV<lambda_9a2786289a4d18fa33a8facc5fc736fa>@@
.?AV<lambda_9a56c482bf06c1a34241f10d7df60646>@@
.?AV<lambda_9a80c5c6458e824bfa0876a779605b0b>@@
.?AV<lambda_9abb244ecf1fe7549bfa1b3b46427d69>@@
.?AV<lambda_9b848ee398be1c2a596086e8c2c0124c>@@
.?AV<lambda_9baa1cdad376a56943ca503609534da1>@@
.?AV<lambda_9c4f15881ef2441e36fb65a5f1a748dd>@@
.?AV<lambda_9c9dbe7f2291bf7ad9d3c12ac8113947>@@
.?AV<lambda_9cb8596ba7d3038d751f936a264c9fff>@@
.?AV<lambda_9ddccf775856736f4137dfc22445dcab>@@
.?AV<lambda_9e468aee1cda7bcda7bd21224cf0d16f>@@
.?AV<lambda_9e6211c87401ff34ec951ed3f9d92212>@@
.?AV<lambda_9eb53e876a6f2ea1f0d78d7a6be208e9>@@
.?AV<lambda_a0842de1be7c8b620f11f3eaf0782629>@@
.?AV<lambda_a09e1ae91603c934837ab1779a5b1cd7>@@
.?AV<lambda_a1b09d8bbea23dd46d5c1124ff420ef8>@@
.?AV<lambda_a1dc46dc98606ea7b7b5f34303ee2a2f>@@
.?AV<lambda_a1e22134fbe96254ce6d7a539c26c203>@@
.?AV<lambda_a2dbfc730df855f1da452ed738a41cc7>@@
.?AV<lambda_a3360ab2153a0d5e361146591e0ed940>@@
.?AV<lambda_a4eb294818c1d4196ae37e2aec2c57f3>@@
.?AV<lambda_a4fe3bd39dcd0107f90455cb45c21429>@@
.?AV<lambda_a57196164107acf0b8c09a6af15b6c67>@@
.?AV<lambda_a5bc10369070c00aebb065dae57fa0bc>@@
.?AV<lambda_a5f2de4a08d99e6253357934807ae733>@@
.?AV<lambda_a65213a64b99d0456231cd4b3dfd703b>@@
.?AV<lambda_a690b97e1252e2aa05e689270ad1214e>@@
.?AV<lambda_a7688683053999985ad4b5ee1c5e7d55>@@
.?AV<lambda_a777aefbfa0ed449706f4a933e2595f0>@@
.?AV<lambda_a7e0ef8dbac7898789e7674a96621dea>@@
.?AV<lambda_a7ed49975b7832e11720c7c8bc70dba8>@@
.?AV<lambda_a81247647b5856024e7c9c86be60bd88>@@
.?AV<lambda_a856ca1b7d6657f47466353c332cab4d>@@
.?AV<lambda_a875e26a77cffbb176a8ef549b2a14b9>@@
.?AV<lambda_a8dc603fd8183714a0dc357fc9b787c1>@@
.?AV<lambda_a8e944132f86054019a0dd06ad885a3f>@@
.?AV<lambda_a90d8b4d11b49a93db077960be52512b>@@
.?AV<lambda_a949a47209eded0ee39e3c26cfeb0e88>@@
.?AV<lambda_a96924e5b8f730c8b67a0b16014102d9>@@
.?AV<lambda_a9804f77c3a977ad53df45464de9292b>@@
.?AV<lambda_aa40aac03a68df4d791c9ae960a96e81>@@
.?AV<lambda_ab509a36b64c6185495bed43165d3ae5>@@
.?AV<lambda_ab9c2c5f50505ae2c8183bb62b8be9b3>@@
.?AV<lambda_ac9e5c5dae7b58201a89b0e1484eabd5>@@
.?AV<lambda_acbad9bc57c95ab4ffc746caf04c813c>@@
.?AV<lambda_ad9ee849a4f4af706ba1761253c50820>@@
.?AV<lambda_ae9b41333b82c3f43259db32dcb29699>@@
.?AV<lambda_aebf67a76b65926eb07014f4fd08d631>@@
.?AV<lambda_af2b72ce93dfb4b4ab2e91dda701576d>@@
.?AV<lambda_af2c39467b4484bc2a2f611cf533daa7>@@
.?AV<lambda_af494c70070cc8b5902d69428e0f310d>@@
.?AV<lambda_af999ce2e03a2039dbf31c1ff13e0675>@@
.?AV<lambda_aff3a47ffdeb0107e2635883ea459138>@@
.?AV<lambda_b0de799044b27f15cb610b7d88655272>@@
.?AV<lambda_b123eebc6aab753cdc93a7cd4699b893>@@
.?AV<lambda_b13b0c3bacb7bf27cf9bf4bac72849bb>@@
.?AV<lambda_b15572961e53281d7d33053637faa8a6>@@
.?AV<lambda_b1dfe0352ec14503f6bc27c9ab7381cc>@@
.?AV<lambda_b39fcf5d316099631cb4151a025dde22>@@
.?AV<lambda_b3cbfd0276cf0ad6f3b58532d60f239f>@@
.?AV<lambda_b4521294892e50b63d12361cda2a891c>@@
.?AV<lambda_b4697c7f06fc1e7e59f4a5183c6a98c3>@@
.?AV<lambda_b51428820b3477616d25ebf36e7dd645>@@
.?AV<lambda_b52f9681df527cde6dd7dae12a89dffb>@@
.?AV<lambda_b56eca9e5f17216a19fa025aad3fbc12>@@
.?AV<lambda_b62023a8b21b8c32235c601a9833387c>@@
.?AV<lambda_b63c2e5f4896a6001dfbdd641092de94>@@
.?AV<lambda_b676af55e67de34be8258a85059e3fa8>@@
.?AV<lambda_b6aaeef95996305e5431d1fd80eaed87>@@
.?AV<lambda_b6b6722f1e0f042c63804b4645002c54>@@
.?AV<lambda_b6cf00bbc72a844e74b65b515f429504>@@
.?AV<lambda_b7019bfd7941f33388d4d95b9daa50e8>@@
.?AV<lambda_b71fc6f482e72afaaed63683aaf4c733>@@
.?AV<lambda_b769cb65454a05d6b4a26fb57be3e5e8>@@
.?AV<lambda_b7be74f8e6a45391be9d480e0a0a34ff>@@
.?AV<lambda_b86aa8ef1de0593a5547f08d792c1565>@@
.?AV<lambda_b87eb49fd5ec35570cf6bd7d61795bc8>@@
.?AV<lambda_b8c456fe40447d4b39feffecfde76884>@@
.?AV<lambda_b8dcef670a65ded42eb20f2062e03c5e>@@
.?AV<lambda_b9005de86b74a327b831744e992c382d>@@
.?AV<lambda_b904ddb979f839dff33d10b1e7d0bdbf>@@
.?AV<lambda_b92977e632ab65a34aad3c8d85f14f57>@@
.?AV<lambda_b937f7d4f097577d4fa0e0107ad25600>@@
.?AV<lambda_b9ee3e2cf9bbfb73bfd678f2ffa5cccc>@@
.?AV<lambda_ba0868398d1cd25f6a2d11110301255b>@@
.?AV<lambda_bab97670c306dda36cb6e184f5555653>@@
.?AV<lambda_bb3d425e1c6940fb9e0c2d83749b4004>@@
.?AV<lambda_bb907a92df9fde70b67de149cb976bb1>@@
.?AV<lambda_bbfb6c4f9961aa9de543511f0a00bf20>@@
.?AV<lambda_bc73456ccef784a5af3834b4cd8faaf9>@@
.?AV<lambda_bc9a9f99cbeabaf2d0a936a0cad6b782>@@
.?AV<lambda_bce28680af6d6e63afbcec5203580c30>@@
.?AV<lambda_bd14764690ecfef9fa94612b0f5ef502>@@
.?AV<lambda_bd88b325eaf102bf56f0337b94c4e196>@@
.?AV<lambda_beb5ca5c13f598ae93cc9dc5c5727597>@@
.?AV<lambda_bfc475f1e9fa3006d556ca921366385c>@@
.?AV<lambda_c0010c06ff92f81c472abefd6166a78e>@@
.?AV<lambda_c13687892ba25942bc19e5eb13b503f9>@@
.?AV<lambda_c20b2bc71558ce98a32f42bb85de6d28>@@
.?AV<lambda_c2d5469345cd97550e4777826a9777d5>@@
.?AV<lambda_c2e380cf1102ff4d7d9bd2e215fde5b4>@@
.?AV<lambda_c311c1ca35d5610174ed400a6355dd88>@@
.?AV<lambda_c332dd4138e096b0ec5e0eae392e23e1>@@
.?AV<lambda_c3857b8060a6c3155dc6bb6df09a4841>@@
.?AV<lambda_c3afc2c3b924ee88b1311ef23b1644eb>@@
.?AV<lambda_c3d54ef5797172fcc9eaba420c8f75e8>@@
.?AV<lambda_c4a3869be068e7ba91c5160df08ada1c>@@
.?AV<lambda_c5677617c2b0074bdccd3f6596388b76>@@
.?AV<lambda_c580670d4132814d3bd00ecba71eaf16>@@
.?AV<lambda_c5dec7e5184735ef54bbb3fa15124e26>@@
.?AV<lambda_c6108df36ce1e471ee073cdcb906363b>@@
.?AV<lambda_c6e8ce8febb47c28ae5d88eebb07bb81>@@
.?AV<lambda_c720e61805dcd56f6632856caaf966d8>@@
.?AV<lambda_c7bc13683ecb5a18d043a137e0218d42>@@
.?AV<lambda_c7c1c280bcb590aff0e32b0d2bffa767>@@
.?AV<lambda_c7e777d5cfa84aa6006d2063a10ce1f4>@@
.?AV<lambda_c8c33a5265022502f864eaf2ce2649c7>@@
.?AV<lambda_ca1582b0d14abd4d03d9895812c8d107>@@
.?AV<lambda_ca50ed85a5b6a014480832a0283a2f4d>@@
.?AV<lambda_ca605469c50319317c9a56255477531a>@@
.?AV<lambda_ca87e505bf8a7e1f26579083c3683d94>@@
.?AV<lambda_caa048ef690674a871bcb383934123ad>@@
.?AV<lambda_cb60ff922e3e1d629a152c3a1068e6c5>@@
.?AV<lambda_cc1c5bc88263b1d3d134206e36d4f483>@@
.?AV<lambda_cc270699afce6708083292447c6ec5a7>@@
.?AV<lambda_ccb51d052b759c6bf0040059226512eb>@@
.?AV<lambda_cd453f5abbb4020fb3775475830cd8d1>@@
.?AV<lambda_cd4c33e9450ec2c40f1249d17dbfa6ba>@@
.?AV<lambda_cd4f9515fd1d18f38ab6ba0038b1c96b>@@
.?AV<lambda_ceef0f6dad2176ea59816e5c9d007c22>@@
.?AV<lambda_d01cb7d72935562762275ef2725e3ce2>@@
.?AV<lambda_d028772904ecde58905f5707c0de32a5>@@
.?AV<lambda_d099cfc55508afaa7cb7103a60412ec4>@@
.?AV<lambda_d13d78c406485322a9ca312b3346960c>@@
.?AV<lambda_d1d74ae6bdde7a081748ee6565b14bf6>@@
.?AV<lambda_d252a66a2c07e9c22634d060e74c7bae>@@
.?AV<lambda_d2a49f03e450ccce2f16f6b50c34ec41>@@
.?AV<lambda_d2d693a490e9887da5a024c703dc8e3e>@@
.?AV<lambda_d303bd53dadec643ee9070953471f748>@@
.?AV<lambda_d318dbc50159c2f1256019c88eaff40b>@@
.?AV<lambda_d375a3eb9b0fdb1da985e8c55163a4cf>@@
.?AV<lambda_d3bde8af1c4b2f7f5d32bc0631a76a47>@@
.?AV<lambda_d41a0a7dd6ff244599c8f5fcf6aa6a8e>@@
.?AV<lambda_d4ed0ae11d4b451fa79547d0fd0e7282>@@
.?AV<lambda_d51c06990161dc274f89c0ff046b1644>@@
.?AV<lambda_d523df6827c8f76e5f91e59ed710694a>@@
.?AV<lambda_d5280148c0688c7d0bac28aeda9b34e8>@@
.?AV<lambda_d5aface184b6dca3aadbb4032d024091>@@
.?AV<lambda_d5f3ad82c6f06fa69e5f77344156b4ec>@@
.?AV<lambda_d61a4c9d12d8fdf51773fa74732acc59>@@
.?AV<lambda_d67cddb616713624f417e1920c5d4f1a>@@
.?AV<lambda_d7733c1584aa3badfa0c0498f12b13b3>@@
.?AV<lambda_d8b06a4dd6f551b1fc6c7009ad0ed428>@@
.?AV<lambda_d8bd629bd1b1f6ad17cb30e686c3da2f>@@
.?AV<lambda_d8c2999bab01791ce81720c649701452>@@
.?AV<lambda_d8d69ea69c6171a0cf8e50796541d588>@@
.?AV<lambda_d95ddce1a1fa4ff8b9f16e34e85c7615>@@
.?AV<lambda_d966a160cb34bd8cd4e5e8dbd91fdf85>@@
.?AV<lambda_d9adada7822ba67bc618dd6220c7e11e>@@
.?AV<lambda_dad5fe9e33f684546a8c61d0c010bbc0>@@
.?AV<lambda_dada44d2f02d0415eebeca9a96054391>@@
.?AV<lambda_db34e867122da08651e3e6cf481fea25>@@
.?AV<lambda_dbb7db98ef2a06cce32a7330369a258b>@@
.?AV<lambda_dc05c873fcc02dfc08a02f74938f6e72>@@
.?AV<lambda_dc262a6b1bcaffa0d7b330449a737cd7>@@
.?AV<lambda_dcbbb46dcb2ccb38f1b7e3e89a0ca172>@@
.?AV<lambda_dcd60b8d026e641a76ed4cfca4de49da>@@
.?AV<lambda_dd0fa2bf36aea47d4ec8777b68b77c1f>@@
.?AV<lambda_dd4a8a293eb1e8d9bc247b4bc07121c6>@@
.?AV<lambda_dd922fd085bf68a1538b4dfe400a4e5d>@@
.?AV<lambda_dda191f6c4c5c14fe1c2434042cb685f>@@
.?AV<lambda_de0ef70b4af308e6dbad1f0877114d15>@@
.?AV<lambda_de5c152ca9e3f93d72392e47d49640bb>@@
.?AV<lambda_deb528c4a03e167325d02f55697f57f9>@@
.?AV<lambda_dfc50b5c75707c0e32bd4de87875de8d>@@
.?AV<lambda_dfdd1161f2f278e15b5873916c2b0ca7>@@
.?AV<lambda_dfdef6da643cd6a11fea2d67407fdc34>@@
.?AV<lambda_e0b99860c2bc4f26550c2e8d362d7aeb>@@
.?AV<lambda_e12a5cc64ca47f52fabae1f759666c1b>@@
.?AV<lambda_e1960fdb548c2ba32cd565fbc4783a60>@@
.?AV<lambda_e24b21811bde89af90e60da39c77df1b>@@
.?AV<lambda_e2b51361958443a9e5e8688df90bcc31>@@
.?AV<lambda_e2bb8f55a256e670e2f45d1b39e2618b>@@
.?AV<lambda_e31b0a3938d8cf9c535bd4f318233ee3>@@
.?AV<lambda_e32e28fdb3d333c868b22a20bac87494>@@
.?AV<lambda_e34957b95c2645f541b4bd46255fbbb0>@@
.?AV<lambda_e36f361e8289b79d05cd119226887515>@@
.?AV<lambda_e4094104640a99b7059b503bea5cd4c0>@@
.?AV<lambda_e430b604f7b922944a8b9691ed8b0438>@@
.?AV<lambda_e479058e031cb9a0657aa4c881cf2c85>@@
.?AV<lambda_e4af5663d262f6d7cf485c6657cb9e95>@@
.?AV<lambda_e57b1a786ad480298008b53b2aee7436>@@
.?AV<lambda_e5b736ff629e45c243838260d1c41f20>@@
.?AV<lambda_e6d5b5718178868e630fc4afa1928cce>@@
.?AV<lambda_e742661535f51343e97c0bd1f594fd9e>@@
.?AV<lambda_e7465666a1cce2f1be8b885d5e56a9d2>@@
.?AV<lambda_e77a22eae973afb20f2d0750afc5e252>@@
.?AV<lambda_e795049375f61fd2d3a131a7c274e255>@@
.?AV<lambda_e79e85aa06c0020ce8ba4eced08c2c92>@@
.?AV<lambda_e8ba98e57e993cbf3811de764c1ca6fa>@@
.?AV<lambda_e8e963fdb995e953f45fcef5a7c8333d>@@
.?AV<lambda_e8f0421704f572aad2649a099fbec081>@@
.?AV<lambda_e971bf56206beddda75940cb23d81d7c>@@
.?AV<lambda_e9c91c962bbc67adb808a120781581d7>@@
.?AV<lambda_ea73e250586924fb71aef6e9068c74b3>@@
.?AV<lambda_eb6625ff603571627d15a6f97beef5e6>@@
.?AV<lambda_eba0ea83d45e95a6a590a5b2e5133b79>@@
.?AV<lambda_ebc32470c7ba4b7c18e53e48be43fa56>@@
.?AV<lambda_ec1910e0e2f828f423168f93cf5ec05e>@@
.?AV<lambda_ec38228ee8b8eb97ec41ac102234721a>@@
.?AV<lambda_ec3a9f1754c313e2d781cd72b20f6949>@@
.?AV<lambda_ee2abb9e567c98f54823297ece51bc5a>@@
.?AV<lambda_ee98d36ba3b6f304603cfa013effa1d0>@@
.?AV<lambda_eedfabccf04dae8365cd4a4775f4c12a>@@
.?AV<lambda_ef9140f011f6ca89ad52592af55290bc>@@
.?AV<lambda_efb13713d16ec9cd372290e4d816e310>@@
.?AV<lambda_f02097f693588c4875083cbaa36b78e4>@@
.?AV<lambda_f0c2d149234417642d38bb42f3187940>@@
.?AV<lambda_f0d5219e49c8319e60d44c8130a3f306>@@
.?AV<lambda_f0e40b769732eedb69b178cbdb1da2d9>@@
.?AV<lambda_f112d6225128e4a2bde6b3763bdc14bc>@@
.?AV<lambda_f123ac6ed0caec8551b10b1bf958a874>@@
.?AV<lambda_f1381ec0608811bb961bf0bc58bba8a8>@@
.?AV<lambda_f1cd88210e6db5811994272c07493962>@@
.?AV<lambda_f1fa811d25327e50fa43acb91e2eb6a4>@@
.?AV<lambda_f20e48e7127186879f643480a395548e>@@
.?AV<lambda_f22ff450a42615719b4e6b964a52c793>@@
.?AV<lambda_f36a9f1162ce53394a7287b72f55835f>@@
.?AV<lambda_f37d07a0c75273eece221dd5b60b299b>@@
.?AV<lambda_f3fcc869c6c1998b84de6d82e27f5b75>@@
.?AV<lambda_f4bb0cc197d9c7469e8edd1f960265c8>@@
.?AV<lambda_f4ee709d6b6434a145be84c46551b589>@@
.?AV<lambda_f529c53f0c288eaa5f1eb9f79a62faaf>@@
.?AV<lambda_f65c61404cbe8a2665c2dfba4b8d6c08>@@
.?AV<lambda_f66636158d41c23d6c8741d8b8b758e4>@@
.?AV<lambda_f749f947d6e043250dfdbe549acd74ee>@@
.?AV<lambda_f78d8769dd40946ef61ad5956cbf10fe>@@
.?AV<lambda_f8a8d7e3c30093707b9dfda821b82ba0>@@
.?AV<lambda_f981a1e78256ff9696dd76fdbfa999a8>@@
.?AV<lambda_f9f1f2325ee0aa9d5b8ebeaa441162b3>@@
.?AV<lambda_fa20e9f670f8ec378ebbe3834e9db296>@@
.?AV<lambda_fa21abf52748e92ccc4db121eff52d81>@@
.?AV<lambda_faa349663837247e8666c82fc4f50c4f>@@
.?AV<lambda_fabc2f3ff1a13bd31a439cf6cb082152>@@
.?AV<lambda_fada670c8c8fb6ae388f5a9e4132a8cd>@@
.?AV<lambda_fbb10b34ab7c53fa4628c7de9c1d7ff5>@@
.?AV<lambda_fc074f31a22d2aec298c9e2b6a482f1c>@@
.?AV<lambda_fc59cfd6180e7962f9dbfcf6d01970a3>@@
.?AV<lambda_fc7d1f6c2adcff66a9b847972f40b566>@@
.?AV<lambda_fd266a9166d0b10d35a0d8f14994daa2>@@
.?AV<lambda_fd4e281633801c162fcd6dc98f33e424>@@
.?AV<lambda_fda969412d0889bae9901abe379045b6>@@
.?AV<lambda_fe53dec043dfef08e033f9e450fe62d9>@@
.?AV<lambda_fe69b453113ca38edb4981f1b14f5af4>@@
.?AV<lambda_ff5e6787b4748302677d41e07ba5b8c0>@@
.?AV<lambda_ffd7c59dd1db3541219e1571e11a3c77>@@
.?AVAbiCustomRegistry@Adapter@MachineLearning@AI@Windows@@
.?AVAbiCustomRegistryImpl@Adapter@MachineLearning@AI@Windows@@
.?AVAbiOpKernel@Adapter@MachineLearning@AI@Windows@@
.?AVAllocationInfo@Dml@@
.?AVAllocator@flatbuffers@@
.?AVAllocatorWrapper@onnxruntime@@
.?AVAnd@onnxruntime@@
.?AVAttentionBase@contrib@onnxruntime@@
.?AVAttentionCPUBase@contrib@onnxruntime@@
.?AVAttentionFusion@onnxruntime@@
.?AVAttributeProto@onnx@@
.?AVbad_alloc@std@@
.?AVbad_array_new_length@std@@
.?AVbad_cast@std@@
.?AVbad_exception@std@@
.?AVbad_function_call@std@@
.?AVbad_optional_access@optional_lite@nonstd@@
.?AVbad_optional_access@std@@
.?AVbad_typeid@std@@
.?AVbad_variant_access@std@@
.?AVBatchNormalizationAddFusion@onnxruntime@@
.?AVBatchNormalizationMulFusion@onnxruntime@@
.?AVBFCArena@onnxruntime@@
.?AVBiasGeluFusion@onnxruntime@@
.?AVBiasSoftmaxFusion@onnxruntime@@
.?AVBucketizedBufferAllocator@Dml@@
.?AVCast@?A0x4c3f03b8@onnxruntime@@
.?AVCastElimination@onnxruntime@@
.?AVCastMap@ml@onnxruntime@@
.?AVCategoryMapper@ml@onnxruntime@@
.?AVcharNode@@
.?AVClip@onnxruntime@@
.?AVCLogSink@logging@onnxruntime@@
.?AVClosable@Adapter@MachineLearning@AI@Windows@@
.?AVCoalesceWalker@re2@@
.?AVcodecvt_base@std@@
.?AVCommonSubexpressionElimination@onnxruntime@@
.?AVCompiler@re2@@
.?AVCompress@onnxruntime@@
.?AVConcat@onnxruntime@@
.?AVConcatBase@onnxruntime@@
.?AVConcatFromSequence@onnxruntime@@
.?AVConcatHelper@OperatorHelper@@
.?AVConstantFolding@onnxruntime@@
.?AVConstantOfShape@?A0xf15982ee@onnxruntime@@
.?AVConstantOfShapeHelper@OperatorHelper@@
.?AVConvActivationFusion@onnxruntime@@
.?AVConvAddFusion@onnxruntime@@
.?AVConvBNFusion@onnxruntime@@
.?AVConvInteger@onnxruntime@@
.?AVConvMulFusion@onnxruntime@@
.?AVConvolutionHelperBase@OperatorHelper@@
.?AVCopyingFileInputStream@FileInputStream@io@protobuf@google@@
.?AVCopyingFileOutputStream@FileOutputStream@io@protobuf@google@@
.?AVCopyingInputStream@io@protobuf@google@@
.?AVCopyingInputStreamAdaptor@io@protobuf@google@@
.?AVCopyingOstreamOutputStream@OstreamOutputStream@io@protobuf@google@@
.?AVCopyingOutputStream@io@protobuf@google@@
.?AVCopyingOutputStreamAdaptor@io@protobuf@google@@
.?AVCPUAllocator@Dml@@
.?AVCPUAllocator@onnxruntime@@
.?AVCPUDataTransfer@onnxruntime@@
.?AVCPUExecutionProvider@onnxruntime@@
.?AVCropBase@contrib@onnxruntime@@
.?AVCropHelper@OperatorHelper@@
.?AVDataTransfer@Dml@@
.?AVDataTypeImpl@onnxruntime@@
.?AVDeepCpuAttnLstmOp@contrib@onnxruntime@@
.?AVDeepCpuGruOp@onnxruntime@@
.?AVDeepCpuLstmOp@onnxruntime@@
.?AVDefaultAllocator@flatbuffers@@
.?AVDepthToSpaceHelper@OperatorHelper@@
.?AVDmlCommandRecorder@Dml@@
.?AVDmlGraphOpKernelInfoWrapper@Adapter@MachineLearning@AI@Windows@@
.?AVDmlOperator@Dml@@
.?AVDmlOperatorActivation@Dml@@
.?AVDmlOperatorAffine@Dml@@
.?AVDmlOperatorBatchNormalization@Dml@@
.?AVDmlOperatorCast@Dml@@
.?AVDmlOperatorConcat@Dml@@
.?AVDmlOperatorConstantOfShape@Dml@@
.?AVDmlOperatorConvInteger@Dml@@
.?AVDmlOperatorConvolution@Dml@@
.?AVDmlOperatorCopy@Dml@@
.?AVDmlOperatorCrop@Dml@@
.?AVDmlOperatorCumSum@Dml@@
.?AVDmlOperatorDepthToSpace@Dml@@
.?AVDmlOperatorDynamicQuantizeLinear@Dml@@
.?AVDmlOperatorEinSum@Dml@@
.?AVDmlOperatorElementwiseBitShift@Dml@@
.?AVDmlOperatorElementwiseClip11@Dml@@
.?AVDmlOperatorElementwiseClip7@Dml@@
.?AVDmlOperatorElementwiseIf@Dml@@
.?AVDmlOperatorElementwiseIsInf@Dml@@
.?AVDmlOperatorElementwiseMean@Dml@@
.?AVDmlOperatorElementwiseMod@Dml@@
.?AVDmlOperatorElementwisePow@Dml@@
.?AVDmlOperatorElementwiseRound@Dml@@
.?AVDmlOperatorExpand@Dml@@
.?AVDmlOperatorEyeLike@Dml@@
.?AVDmlOperatorGatedRecurrentUnit@Dml@@
.?AVDmlOperatorGather@Dml@@
.?AVDmlOperatorGatherElements@Dml@@
.?AVDmlOperatorGatherNd@Dml@@
.?AVDmlOperatorGemm@Dml@@
.?AVDmlOperatorInstanceNormalization@Dml@@
.?AVDmlOperatorLocalResponseNormalization@Dml@@
.?AVDmlOperatorLongShortTermUnit@Dml@@
.?AVDmlOperatorLpNormalization@Dml@@
.?AVDmlOperatorMatMul@Dml@@
.?AVDmlOperatorMatMulInteger@Dml@@
.?AVDmlOperatorMaxUnpool@Dml@@
.?AVDmlOperatorMeanVarNormalization@Dml@@
.?AVDmlOperatorMemcpy@Dml@@
.?AVDmlOperatorNeg@Dml@@
.?AVDmlOperatorOneHot@Dml@@
.?AVDmlOperatorPadding@Dml@@
.?AVDmlOperatorPooling@Dml@@
.?AVDmlOperatorQLinearAdd@Dml@@
.?AVDmlOperatorQLinearConv@Dml@@
.?AVDmlOperatorQLinearMatMul@Dml@@
.?AVDmlOperatorRange@Dml@@
.?AVDmlOperatorRecurrentBase@Dml@@
.?AVDmlOperatorRecurrentNeuralNetwork@Dml@@
.?AVDmlOperatorReduce@Dml@@
.?AVDmlOperatorRegionOfInterestAlign@Dml@@
.?AVDmlOperatorRegionOfInterestPooling@Dml@@
.?AVDmlOperatorResize@Dml@@
.?AVDmlOperatorReverseSequence@Dml@@
.?AVDmlOperatorScatter@Dml@@
.?AVDmlOperatorScatterNd@Dml@@
.?AVDmlOperatorSlice@Dml@@
.?AVDmlOperatorSpaceToDepth@Dml@@
.?AVDmlOperatorSplit@Dml@@
.?AVDmlOperatorTile@Dml@@
.?AVDmlOperatorTopK@Dml@@
.?AVDmlOperatorTranspose@Dml@@
.?AVDmlOperatorValueScale2d@Dml@@
.?AVDNameNode@@
.?AVDNameStatusNode@@
.?AVDontUseNewUseMake@Details@WRL@Microsoft@@
.?AVDynamicQuantizeLSTM@contrib@onnxruntime@@
.?AVDynamicQuantizeMatMul@contrib@onnxruntime@@
.?AVDynamicQuantizeMatMulFusion@onnxruntime@@
.?AVEinsum@onnxruntime@@
.?AVEinSumHelper@OperatorHelper@@
.?AVEliminateDropout@onnxruntime@@
.?AVEliminateIdentity@onnxruntime@@
.?AVEliminateSlice@onnxruntime@@
.?AVEmbedLayerNormFusion@onnxruntime@@
.?AVEnv@onnxruntime@@
.?AVEnvThread@onnxruntime@@
.?AVEnvTime@onnxruntime@@
.?AVerror_category@std@@
.?AVexception@detail@nlohmann@@
.?AVexception@std@@
.?AVExecutionFrame@onnxruntime@@
.?AVExecutionPlanBase@onnxruntime@@
.?AVExecutionProvider@Dml@@
.?AVExecutionProviderImpl@Dml@@
.?AVExLibLoader@onnxruntime@@
.?AVExpandDims@contrib@onnxruntime@@
.?AVExpandElimination@onnxruntime@@
.?AVExpandHelper@OperatorHelper@@
.?AVExtendedThreadPoolInterface@concurrency@onnxruntime@@
.?AVEyeLike@onnxruntime@@
.?AVfacet@locale@std@@
.?AVfailure@ios_base@std@@
.?AVFastGeluFusion@onnxruntime@@
.?AVFatalException@protobuf@google@@
.?AVFeatureVectorizer@ml@onnxruntime@@
.?AVFileInputStream@io@protobuf@google@@
.?AVFileOutputStream@io@protobuf@google@@
.?AVFlatten@onnxruntime@@
.?AVFreeDimensionOverrideTransformer@onnxruntime@@
.?AVFunction@onnxruntime@@
.?AVFunctionImpl@onnxruntime@@
.?AVFunctionKernel@onnxruntime@@
.?AVFunctionProto@onnx@@
.?AVFusedConvFloat@contrib@onnxruntime@@
.?AVFusedGraphKernel@Dml@@
.?AVFuseReluClip@onnxruntime@@
.?AVGather@onnxruntime@@
.?AVGatherBase@onnxruntime@@
.?AVGatherElements@onnxruntime@@
.?AVGatherHelper@OperatorHelper@@
.?AVGatherND@onnxruntime@@
.?AVGatherNDBase@onnxruntime@@
.?AVGatherNdHelper@OperatorHelper@@
.?AVGeluApproximation@onnxruntime@@
.?AVGeluFusion@onnxruntime@@
.?AVGemmActivationFusion@onnxruntime@@
.?AVGemmHelper@OperatorHelper@@
.?AVGraph@onnxruntime@@
.?AVGraphInferencer@onnx@@
.?AVGraphInferencerImpl@onnxruntime@@
.?AVGraphInferencerImpl@shape_inference@onnx@@
.?AVGraphProto@onnx@@
.?AVGraphTransformer@Dml@@
.?AVGraphTransformer@onnxruntime@@
.?AVIAllocator@onnxruntime@@
.?AVIArenaAllocator@onnxruntime@@
.?AVICommandRecorder@Dml@@
.?AVIControlFlowKernel@controlflow@onnxruntime@@
.?AVIDataTransfer@onnxruntime@@
.?AVIExecutionFrame@onnxruntime@@
.?AVIExecutionProvider@onnxruntime@@
.?AVIExecutor@onnxruntime@@
.?AVIf@onnxruntime@@
.?AVImputerOp@ml@onnxruntime@@
.?AVInferenceContextImpl@onnxruntime@@
.?AVInferenceError@onnx@@
.?AVInferenceSession@onnxruntime@@
.?AVinput_buffer_adapter@detail@nlohmann@@
.?AVInsertCastTransformer@onnxruntime@@
.?AVinvalid_argument@std@@
.?AVinvalid_iterator@detail@nlohmann@@
.?AVInverse@contrib@onnxruntime@@
.?AVIOnnxRuntimeOpSchemaCollection@onnxruntime@@
.?AVios_base@std@@
.?AVISchemaRegistry@onnx@@
.?AVISequentialPlannerContext@onnxruntime@@
.?AVIsInf@onnxruntime@@
.?AVISink@logging@onnxruntime@@
.?AVITensorAllocator@onnxruntime@@
.?AVIterator@?$OrtValueTensorSlicer@$$CBUOrtValue@@@onnxruntime@@
.?AVIterator@?$OrtValueTensorSlicer@UOrtValue@@@onnxruntime@@
.?AVLabelEncoder@ml@onnxruntime@@
.?AVLayerNormFusion@onnxruntime@@
.?AVlength_error@std@@
.?AVLinearClassifier@ml@onnxruntime@@
.?AVLinearRegressor@ml@onnxruntime@@
.?AVLoggingWrapper@@
.?AVlogic_error@std@@
.?AVLoop@onnxruntime@@
.?AVLSTMBase@onnxruntime@@
.?AVMatMulAddFusion@onnxruntime@@
.?AVMatMulInteger@onnxruntime@@
.?AVMatMulIntegerBase@onnxruntime@@
.?AVMatMulIntegerToFloat@contrib@onnxruntime@@
.?AVMatMulIntegerToFloatBase@contrib@onnxruntime@@
.?AVMatMulIntegerToFloatFusion@onnxruntime@@
.?AVMatMulScaleFusion@onnxruntime@@
.?AVMax_8@onnxruntime@@
.?AVMaxPoolV8@onnxruntime@@
.?AVMaxpoolWithMask@contrib@onnxruntime@@
.?AVMaxUnpool@onnxruntime@@
.?AVMemcpyTransformer@onnxruntime@@
.?AVMessageLite@protobuf@google@@
.?AVMin_8@onnxruntime@@
.?AVMLAS_QGEMM_OUTPUT_PROCESSOR@@
.?AVMLAS_QGEMM_SCALE_BIAS_OUTPUT_PROCESSOR@@
.?AVMLKernelInferenceContext@Adapter@MachineLearning@AI@Windows@@
.?AVMLOperatorKernelFactory@@
.?AVMLOperatorShapeInferrer@@
.?AVMLOperatorSupportQuery@@
.?AVMLSchemaInferenceContext@Adapter@MachineLearning@AI@Windows@@
.?AVMLSupportQueryContext@Adapter@MachineLearning@AI@Windows@@
.?AVMod@onnxruntime@@
.?AVModelProto@onnx@@
.?AVMultinomial@onnxruntime@@
.?AVMurmurHash3@contrib@onnxruntime@@
.?AVNhwcInferenceContext@contrib@onnxruntime@@
.?AVNhwcMaxPool@contrib@onnxruntime@@
.?AVNhwcTransformer@onnxruntime@@
.?AVNodeProto@onnx@@
.?AVNonMaxSuppression@onnxruntime@@
.?AVNonMaxSuppressionBase@onnxruntime@@
.?AVNonTensorTypeBase@onnxruntime@@
.?AVNormalizer@ml@onnxruntime@@
.?AVNot@onnxruntime@@
.?AVNotImplementedException@onnxruntime@@
.?AVNumCapturesWalker@re2@@
.?AVOneHotHelper@OperatorHelper@@
.?AVOnnxRuntimeException@onnxruntime@@
.?AVOnnxRuntimeOpSchemaRegistry@onnxruntime@@
.?AVOnnxTensorWrapper@Adapter@MachineLearning@AI@Windows@@
.?AVOperatorSetIdProto@onnx@@
.?AVOpKernel@onnxruntime@@
.?AVOpKernelContext@onnxruntime@@
.?AVOpKernelContextInternal@onnxruntime@@
.?AVOpKernelContextWrapper@Adapter@MachineLearning@AI@Windows@@
.?AVOpKernelInfoWrapper@Adapter@MachineLearning@AI@Windows@@
.?AVOpSchemaRegistry@onnx@@
.?AVOptimizerExecutionFrame@onnxruntime@@
.?AVOr@onnxruntime@@
.?AVOstreamOutputStream@io@protobuf@google@@
.?AVOStreamSink@logging@onnxruntime@@
.?AVother_error@detail@nlohmann@@
.?AVout_of_range@detail@nlohmann@@
.?AVout_of_range@std@@
.?AVPadBase@onnxruntime@@
.?AVPaddingHelper@OperatorHelper@@
.?AVpairNode@@
.?AVParallelExecutor@onnxruntime@@
.?AVparse_error@detail@nlohmann@@
.?AVpcharNode@@
.?AVpDNameNode@@
.?AVPoolBase@onnxruntime@@
.?AVPoolingHelperBase@OperatorHelper@@
.?AVPow@onnxruntime@@
.?AVPrimitiveDataTypeBase@onnxruntime@@
.?AVQLinearConv@onnxruntime@@
.?AVQLinearGlobalAveragePool@contrib@onnxruntime@@
.?AVQLinearMatMul@onnxruntime@@
.?AVRandomNormal@onnxruntime@@
.?AVRandomNormalLike@onnxruntime@@
.?AVRandomUniform@onnxruntime@@
.?AVRandomUniformLike@onnxruntime@@
.?AVRange@onnxruntime@@
.?AVrange_error@std@@
.?AVRangeHelper@OperatorHelper@@
.?AVRecurrentHelper@OperatorHelper@@
.?AVReduceHelperBase@OperatorHelper@@
.?AVRemoveDuplicateCastTransformer@onnxruntime@@
.?AVRepetitionWalker@re2@@
.?AVReshape@onnxruntime@@
.?AVReshape_1@onnxruntime@@
.?AVReshapeFusion@onnxruntime@@
.?AVResizeHelper@OperatorHelper@@
.?AVResultException@wil@@
.?AVReverseSequenceOp@onnxruntime@@
.?AVRewriteRule@onnxruntime@@
.?AVRoiAlignBase@onnxruntime@@
.?AVRoiAlignHelper@OperatorHelper@@
.?AVRoiPoolingHelper@OperatorHelper@@
.?AVRoiPoolingHelperBase@OperatorHelper@@
.?AVRuleBasedGraphTransformer@onnxruntime@@
.?AVruntime_error@std@@
.?AVRuntimeClassBase@Details@WRL@Microsoft@@
.?AVScatter@onnxruntime@@
.?AVScatterND@onnxruntime@@
.?AVScatterNDBase@onnxruntime@@
.?AVSchemaError@onnx@@
.?AVSchemaRegistryManager@onnxruntime@@
.?AVSequenceAt@onnxruntime@@
.?AVSequenceConstruct@onnxruntime@@
.?AVSequenceEmpty@onnxruntime@@
.?AVSequenceErase@onnxruntime@@
.?AVSequenceInsert@onnxruntime@@
.?AVSequenceLength@onnxruntime@@
.?AVSequenceTensorTypeBase@onnxruntime@@
.?AVSequentialExecutor@onnxruntime@@
.?AVSequentialPlannerContext@onnxruntime@@
.?AVShape@onnxruntime@@
.?AVShapeToInitializer@onnxruntime@@
.?AVShrink@onnxruntime@@
.?AVSign@onnxruntime@@
.?AVSimpleTensorAllocator@onnxruntime@@
.?AVSimplifiedLayerNormFusion@onnxruntime@@
.?AVSimplifyWalker@re2@@
.?AVSize@onnxruntime@@
.?AVSkipLayerNormFusion@onnxruntime@@
.?AVSliceBase@onnxruntime@@
.?AVSliceHelper@OperatorHelper@@
.?AVSpaceDepthBase@onnxruntime@@
.?AVSpaceToDepthHelper@OperatorHelper@@
.?AVSparseTensorProto@onnx@@
.?AVSparseTensorTypeBase@onnxruntime@@
.?AVSplit@onnxruntime@@
.?AVSplitBase@onnxruntime@@
.?AVSplitHelper@OperatorHelper@@
.?AVSplitToSequence@onnxruntime@@
.?AVSqueeze@onnxruntime@@
.?AVSqueezeBase@onnxruntime@@
.?AVstl_critical_section_interface@details@Concurrency@@
.?AVstl_critical_section_win7@details@Concurrency@@
.?AVStringNormalizer@onnxruntime@@
.?AVStringStringEntryProto@onnx@@
.?AVSVMClassifier@ml@onnxruntime@@
.?AVSVMCommon@ml@onnxruntime@@
.?AVsystem_error@std@@
.?AVTelemetry@onnxruntime@@
.?AVTensorAllocatorWithMemPattern@onnxruntime@@
.?AVTensorAnnotation@onnx@@
.?AVTensorProto@onnx@@
.?AVTensorProto_Segment@onnx@@
.?AVTensorShapeProto@onnx@@
.?AVTensorShapeProto_Dimension@onnx@@
.?AVTensorTypeBase@onnxruntime@@
.?AVTensorWrapper@Adapter@MachineLearning@AI@Windows@@
.?AVTfIdfVectorizer@onnxruntime@@
.?AVThreadPoolInterface@Eigen@@
.?AVTileHelper@OperatorHelper@@
.?AVTokenizer@contrib@onnxruntime@@
.?AVTopKHelper@OperatorHelper@@
.?AVToStringWalker@re2@@
.?AVTrainingInfoProto@onnx@@
.?AVTranspose@onnxruntime@@
.?AVTransposeBase@onnxruntime@@
.?AVTransposeHelper@OperatorHelper@@
.?AVTrilu@contrib@onnxruntime@@
.?AVtype_error@detail@nlohmann@@
.?AVtype_info@@
.?AVTypeProto@onnx@@
.?AVTypeProto_Map@onnx@@
.?AVTypeProto_Opaque@onnx@@
.?AVTypeProto_Sequence@onnx@@
.?AVTypeProto_SparseTensor@onnx@@
.?AVTypeProto_Tensor@onnx@@
.?AVUnique@onnxruntime@@
.?AVUnpoolingHelper@OperatorHelper@@
.?AVUnsqueeze@onnxruntime@@
.?AVUnsqueezeBase@onnxruntime@@
.?AVUnsqueezeElimination@onnxruntime@@
.?AVUpsampleBase@onnxruntime@@
.?AVValidationError@checker@onnx@@
.?AVValueInfoProto@onnx@@
.?AVViewerFunctionImpl@onnxruntime@@
.?AVWindowsEnv@?A0x972e638b@onnxruntime@@
.?AVWindowsEnvTime@?A0x77b46f1a@onnxruntime@@
.?AVWindowsTelemetry@onnxruntime@@
.?AVWindowsThread@?A0x972e638b@onnxruntime@@
.?AVWinmlAdapterLoggingWrapper@@
.?AVWordConvEmbedding@contrib@onnxruntime@@
.?AVXor@onnxruntime@@
.?AVZeroCopyInputStream@io@protobuf@google@@
.?AVZeroCopyOutputStream@io@protobuf@google@@
.?AVZipMapOp@ml@onnxruntime@@
.0/0#
.0/011
.00cfg
.0g0l0
.0K0z0
.0Y0f0
.CRT$XCA
.CRT$XCC
.CRT$XCL
.CRT$XCU
.CRT$XCZ
.CRT$XIA
.CRT$XIC
.CRT$XIZ
.CRT$XLA
.CRT$XLZ
.CRT$XPA
.CRT$XPZ
.CRT$XTA
.CRT$XTZ
.data
.data$r
.didat
.didat$2
.didat$3
.didat$4
.didat$5
.didat$6
.didat$7
.edata
.gfids
.giats
.idata$2
.idata$3
.idata$4
.idata$5
.idata$6
.json
.P6A?AV?$OrtValueTensorSlicer@$$CBUOrtValue@@@onnxruntime@@ABUOrtValue@@_J1@Z
.P6A?AV?$OrtValueTensorSlicer@UOrtValue@@@onnxruntime@@AAUOrtValue@@_J1@Z
.P6A?AV?$unique_ptr@VTensor@onnxruntime@@U?$default_delete@VTensor@onnxruntime@@@std@@@std@@ABVTensor@onnxruntime@@_J1V?$shared_ptr@VIAllocator@onnxruntime@@@1@PAX@Z
.P6A?AVStatus@common@onnxruntime@@ABV?$vector@IV?$allocator@I@std@@@std@@ABVTensor@2@AAV52@PBVTensorShape@2@PAX@Z
.P6A?AVStatus@common@onnxruntime@@ABVNode@2@AAVGraph@2@ABV?$vector@PBVTypeProto@onnx@@V?$allocator@PBVTypeProto@onnx@@@std@@@std@@AAV56@ABUResolveOptions@42@@Z
.P6A?AVStatus@common@onnxruntime@@ABVTensor@2@AAV32@PAX@Z
.P6A?AVStatus@common@onnxruntime@@PAXAAV?$vector@UOrtValue@@V?$allocator@UOrtValue@@@std@@@std@@0I@Z
.P6A?AVStatus@common@onnxruntime@@PB_J0PA_JIIIIIIIPAVThreadPool@concurrency@2@PAX@Z
.P6A?AVStatus@common@onnxruntime@@PBH0PAHIIIIIIIPAVThreadPool@concurrency@2@PAX@Z
.P6A?AVStatus@common@onnxruntime@@PBM0PAMIIIIIIIPAVThreadPool@concurrency@2@PAX@Z
.P6A?AVStatus@common@onnxruntime@@PBN0PANIIIIIIIPAVThreadPool@concurrency@2@PAX@Z
.P6A?AVTensor@onnxruntime@@ABV01@ABV?$vector@_JV?$allocator@_J@std@@@std@@_NV?$shared_ptr@VIAllocator@onnxruntime@@@3@PBVTensorShape@1@PAVThreadPool@concurrency@1@PAX@Z
.P6A_NABUFunctionBodyBuildContext@onnx@@ABVOpSchema@1@AAVFunctionProto@1@@Z
.P6AMMMM@Z
.P6APAVOpKernel@onnxruntime@@ABVOpKernelInfo@1@@Z
.P6AX$$QAVOpSchema@onnx@@@Z
.P6AXAAUInferenceContext@onnx@@@Z
.P6AXPAX@Z
.rdata
.rdata$r
.rdata$sxdata
.rdata$T
.rdata$zETW0
.rdata$zETW1
.rdata$zETW2
.rdata$zETW9
.rdata$zzzdbg
.RQVQ
.rsrc
.rsrc$01
.rsrc$02
.rtc$IAA
.rtc$IZZ
.rtc$TAA
.rtc$TZZ
.stls
.text
.text$di
.text$mn
.text$x
.text$yd
.tls$
.tls$ZZZ
.xdata$x
-/./.
/0B0e0v0
/0M0g0
/161O1V1p1
/D$ YYw
: :$:(:,:0:4:8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:x:|:
: :$:(:,:0:4:8:<:@:D:X:\:`:x:
: :$:(:,:0:4:8:<:P:T:X:\:`:d:h:l:p:t:x:|:
: :$:(:,:0:D:H:`:d:h:p:
: :$:(:@:P:`:p:t:x:
: :$:(:0:8:P:`:d:l:
: :$:,:0:4:8:`:h:l:p:x:|:
: :$:<:L:P:T:h:x:|:
: :$:<:L:P:T:X:\:`:d:h:l:p:t:x:|:
: :$:8:<:T:d:t:
: :$:8:H:L:P:T:\:`:h:
: :&:,:2:8:>:D:J:P:V:\:b:h:n:t:z:
: :(:,:@:D:H:L:`:d:h:|:
: :(:,:4:8:<:D:H:P:T:X:`:d:h:l:p:t:x:|:
: :(:=:I:b:h:l:v:
: :(:0:<:\:h:
: :(:4:<:p:x:
: :,:4:h:x:
: :,:L:T:\:d:l:t:
: :,:L:T:\:d:l:x:
: :@:H:T:\:t:
: :@:P:t:|:
: :>:T:q:
: :0:4:8:P:T:h:l:
: :0:8:@:H:X:`:h:p:x:
: :0:8:H:d:t:
: :3:E:f:k:q:
: :4:
: :6:B:
: :6:k:u:
: :7:>:[:
: :8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:|:
: :8:<:T:d:t:x:|:
: :8:H:L:P:T:X:l:|:
: :8:H:X:h:l:p:t:x:
: :8:P:h:
: :9:P:j:z:
: :9:S:Z:s:
: :D:L:T:\:d:l:t:|:
: :N:z:
: :s:
: ;';<;Q;e;
: ;@;L;l;x;
: ;];
: ;<;F;Z;o;
: ;0;<;D;x;
: ;4;
: ;h;
: ;i;
: ;M;
: ;X;
: Conflicting with a registered kernel with op versions.
: failed validating the check: 
:!:-:
:!:@:
:!:H:Q:x:
:!:I:
:!:J;_;q;y;
:!:o:
:!:w:
:!;^;v;
:!;_;v;{;
:!;k;
:":(:.:]:
:":(:.:T:
:":,:E:
:":':,:7:<:B:H:U:h:
:":;:A:E:O:e:u:{:
:":+:
:":0:7:I:c:
:":2:
:":3:~:g;
:":3:D:U:f:w:
:":n:{:
:":v:
:";,;>;X;
:";7;J;V;q;{;
:"<'<
:#:):1:F:R:f:m;3<
:#:/:;:G:q:
:#:-:F:
:#:4:
:#:5:]:b:h:n:
:#:5:V:[:a:
:#:K:w:
:#:M:
:#:u:
:#;,;v;
:#;3;C;
:#;5;V;[;a;
:#;6;o;
:#;c;u;
:#=V=b=
:$:(:,:4:8:<:@:h:p:t:x:
:$:(:@:P:T:X:\:d:h:|:
:$:(:0:H:L:d:t:x:|:
:$:(:8:<:@:X:\:t:
:$:,:4:<:\:d:
:$:,:4:<:D:L:d:l:t:|:
:$:,:4:<:D:L:T:\:d:l:t:
:$:,:4:<:D:L:T:\:d:l:t:|:
:$:,:4:<:D:L:T:\:d:p:
:$:,:4:<:D:L:T:\:h:
:$:,:4:<:D:L:T:`:
:$:,:4:<:D:L:T:h:p:x:
:$:,:4:<:D:P:p:
:$:,:4:<:D:P:p:x:
:$:,:4:<:D:P:t:|:
:$:,:4:<:D:P:X:x:
:$:,:4:<:H:h:p:x:
:$:,:4:<:L:X:x:
:$:,:8:\:d:l:t:|:
:$:,:8:X:`:h:p:x:
:$:,:L:h:x:
:$:,:X:|:
:$:::J:U:Z:c:h:s:x:
:$:<:@:D:H:L:T:l:p:
:$:<:@:X:h:l:p:
:$:<:@:X:h:l:p:t:x:
:$:0:@:P:`:p:
:$:0:7:a:f:t:
:$:0:P:X:`:h:p:x:
:$:0:P:X:`:h:t:
:$:0:P:X:`:l:
:$:0:P:X:`:p:
:$:0:P:X:d:
:$:0:T:\:d:l:t:|:
:$:2:7:>:C:Q:]:j:v:
:$:4:8:<:T:X:p:t:x:
:$:4:8:H:L:P:X:p:
:$:8:H:X:h:p:
:$:9:T:
:$:D:P:p:x:
:$:K:Z:
:$;);/;5;;;q;
:$;.;@;Z;
:$;.;q;
:$;?;o;
:$;4;
:$;A;U;
:$;H;l;
:$;P;
:$;V;
:$;X;
:$<a<y<
:%:::k:r:
:%:+:v:
:%:4:;:P:`:j:u:{:
:%:5:j:z:
:%:F:K:Q:p:
:%:P:V:n:}:
:%:T:Y:_:e:k:
:%;*;3;8;?;D;R;Y;g;v;
:%;*;n;
:%;/;c;
:%;M;h;
:%;R;
:&:-:::G:S:Y:h:
:&:;:{:
:&:?:
:&:?:P:g: ;8;B;[;
:&:+:1:P:
:&:+:1:Y:
:&:>:E:J:O:c:o:t:~:
:&:0:::D:I:[:g:p:u:~:
:&:1:A:H:X:_:o:v:
:&:H:
:&:h:
:&;,;W;
:&;+;
:&;8;O;d;h;l;p;t;x;|;
:&;k;p;t;x;|;
:&;v;(<^<
:(:,:<:@:D:L:P:d:h:l:p:
:(:,:0:8:<:D:H:P:T:\:`:h:
:(:,:0:D:T:X:h:l:p:t:
:(:,:4:L:P:T:h:x:|:
:(:,:D:H:L:`:d:h:l:t:x:|:
:(:;:[:
:(:~:
:(:0:@:d:l:t:|:
:(:0:d:l:t:|:
:(:3:>:J:m:r:
:(:4:<:\:x:
:(:4:T:\:d:l:t:|:
:(:4:T:`:
:(:8:<:L:P:h:x:|:
:(:8:D:L:l:
:(:A:m:
:(:b:i:
:(:C:^:
:(:H:P:X:d:
:(:H:T:t:
:(:L:T:\:|:
:(;8;M;z;
:(;h;
:(;P;
:):3:U:_:i:s:}:
:):5:P:U:j:
:):B:R:P;
:):W:g:
:);0;H;[;{;
:*:1:V:v:#;<;k;
:*:E:
:*:t:
:*;U;
:,:<:@:D:\:l:p:t:
:,:<:@:P:T:X:\:p:t:x:|:
:,:0:@:D:H:\:l:p:
:,:0:@:D:H:P:h:l:p:x:
:,:0:4:8:@:X:\:d:|:
:,:0:4:H:L:P:T:h:l:p:x:|:
:,:0:4:H:X:\:`:x:|:
:,:0:H:L:d:h:l:p:t:|:
:,:0:H:L:d:t:x:|:
:,:0:H:L:P:X:p:
:,:4:@:`:h:t:
:,:4:@:`:h:t:|:
:,:4:8:<:D:H:L:P:x:
:,:8:@:t:
:,:8:X:h:
:,:C:
:,:I:W:
:,:L:`:l:
:,:X:
:,:z:
:,;A;J;^;g;
:,;d;
:,;U;~;
:,;y;
:,u0B
:.:?:S:
:.:=:
:.:B:h:|:
:.:e:{:
:.;E;U;l;|;
:.;Z;
:/:@:W:
:/:@:W:h<o<
:/:D:[:
:/:e;
:/:G:T:j:p:
:/;@;
:/;4;C;S;
:/;6;K;`;w;
:/;C;`;
:/;i;
:':.:4:^:
::;D;];~;
:-:=:
:':6:V:
:-:N:
:;:[:
:;;i;
:;;k;
:;;u;
:-;|;
:-;7;L;a;x;W<a<v<
:-;A;
:';P;z;
:';U;x;
:?:c:m:
:?:I:^:s:
:?:W:u:
:?;D;
:@:F:^:m:
:@:h:o:x:
:@:L:p:
:@:Z:
:@;l;
:@;p;
:@;q;
:@;x;
:[:b:w:
:[:h:
:\:m:
:\:m:u:
:\:v:
:\;==G=
:\;f;x;
:\;s;x;};
:];l;p;t;x;|;
:];x;
:^:g:
:`;g;z;
:+:0>
:+:6:::S:]:k:v:z:
:+:B:
:+:G:k:
:+:i:
:+:R:y:
:+:U:
:+>:?
:<:D:L:T:\:d:l:t:|:
:<:D:L:T:\:h:p:
:<:G:u:
:<:h:
:<:u:
:<=A=
:-<i=B>
:=:D:J:q:
:=;k;
:=;L;_;
:>:a:
:>:k:p:
:>;K;r;y;
:0:::S:d:
:0:<:\:h:
:0:=:r:
:0:4:8:@:D:X:\:l:p:
:0:4:8:P:T:l:|:
:0:4:L:\:`:p:t:
:0:4:L:\:l:p:t:
:0:7:P:
:0:8:@:L:l:x:
:0:J:
:0:V:`:
:0:X:
:0;];
:0;D;h=
:0;s;
:0;X;
:0<:<
:0<g=
:0D0S0p0
:0Q0g0|0
:1:F:f:x:
:1:H:_:v:
:1:H:M:t:
:1:H:Q:h:s:
:1:K:
:1:m:
:1:M:p:
:1:N:v:
:1;6;<;B;l;
:1;d;
:1;E;N;@=
:1;H;_;};
:1;I;_;v;{;
:2:E:e:
:2:H:_:d:{:
:2;C;
:3:::Q:
:3:@:L:]:b:o:t:~:
:3:8:J:O:y:
:3:E:]:b:q:
:3:J:
:3;e;
:3;E;];b;q;
:3;E;f;k;q;
:3;o;
:4:<:D:L:T:\:d:l:t:
:4:<:D:L:T:\:d:l:t:|:
:4:<:D:L:X:x:
:4:<:D:P:p:x:
:4:D:P:t:|:
:4:E:Z:
:4:f:
:4:O:Y:
:4;|;
:4;~;
:4;>;S;h;
:4;N;
:4<Q<u<
:5;x;
:6:;:A:i:
:6:E:
:6:G:
:6:H:
:6:P:
:6:R:w:
:6:v:
:6:X:
:6;d;
:6;I;N;V;c;h;y;
:6;q;
:6;y;
:6;Y;
:7:?:O:V:c:k:q:u:{:
:7:x:
:7;;;?;C;G;K;O;S;a;i;
:7;e;l;
:7;k;
:7;u;
:8:?:D:H:L:P:
:8:@:L:T:t:
:8:l:
:8:p:
:8:y:
:8;|=
:8;5<r<
:8;x;
:8;z;
:9:C:R:r:
:A;\;
:A<Y<`<u<
:AM:am:PM:pm
:B:I:`:u:
:B:M:
:B:s:
:B:x:
:B;_;d;i;n;
:B;i;
:B;L;^;x;0<
:C:]:
:C:H:P:Y:
:C:M:b:w:}=l>
:C:S:o:
:c:u:
:C:x:
:C;{;
:C;J;c;t;
:C;P;];k;
:C;u;
:D:\:r:
:D:g:
:D:X:
:d;6<R<n<
:D;p;
:D;r;
:e;u;
:F:M:T:
:f:q:w:
:G:|:
:G;/<7=Y>
:G;Q;c;};
:G<U<
:H:c:o;_<S=
:H:f:w:
:H:Q:Z:j:x:
:H;V;p;
:I:!;=;
:i<n<
:i<s=
:J:a:f:k:
:J:u:
:J;b;l;
:J;k;
:Jan:January:Feb:February:Mar:March:Apr:April:May:May:Jun:June:Jul:July:Aug:August:Sep:September:Oct:October:Nov:November:Dec:December
:K:}:
:k:r:~:
:K;P;
:k;x;
:K<j<t<
:L:i:
:L:x:
:L;k;
:L;S;
:l=u=
:m:z:
:M;_;v;
:M;a;
:m;z;
:n:{:
:N:U:n:
:n<|<
:o::;o;
:O:|:
:O:}:
:O;u;
:p;*<
:P;A=
:P;i;|;
:p=L?
:Please, install necessary language-pack-XX and configure locales
:Q:m;
:Q;[;p;
:q;1<E<b<r<@=g=|=
:R:~:
:r;{;
:r<|<
:S:\:
:S:e:
:S:g:p:
:S;x;
:Sun:Sunday:Mon:Monday:Tue:Tuesday:Wed:Wednesday:Thu:Thursday:Fri:Friday:Sat:Saturday
:T:]:
:T:^:h:x:
:T:^:p:
:T:s:
:T:t:
:t={=
:U;h;
:U;m;w;
:V:?;I;^;s;
:V:`:u:
:V:q:
:V;c;
:V;c;{;
:w:B;
:W;f;3<
:W<<=
:X;+>
:X;i;
:Y:%;
:Y:`:x:
:Y;^;
:y;~<
:Y>t>
:Z;|;
:Z;s;
; ;$;(;,;0;4;8;<;@;D;H;\;`;d;|;
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;l;p;t;x;|;
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;p;
; ;$;(;,;0;4;8;<;@;D;H;L;T;l;p;
; ;$;(;,;T;\;`;d;l;p;t;x;
; ;$;<;L;P;`;d;|;
; ;$;4;8;<;D;\;l;p;t;|;
; ;$;L;T;X;\;d;h;l;p;
; ;$;P;h;l;t;x;|;
; ;(;,;0;4;\;d;h;l;t;x;|;
; ;(;0;<;\;h;
; ;(;0;8;@;\;d;l;t;|;
; ;(;0;8;@;L;l;t;|;
; ;(;0;8;D;L;
; ;(;4;T;\;d;l;t;
; ;,;
; ;,;4;h;x;
; ;@;H;P;\;|;
; ;@;H;T;t;|;
; ;\;
; ;=;q;
; ;0;@;P;`;p;
; ;0;4;D;H;X;d;t;
; ;0;8;@;P;`;h;x;
; ;0;8;H;L;\;`;p;|;
; ;3;E;f;k;q;
; ;4;<;D;L;`;h;p;x;
; ;4;8;<;T;d;h;x;
; ;4;8;<;T;X;\;p;
; ;4;D;T;X;\;p;t;x;|;
; ;8;<;@;H;`;d;h;p;
; ;8;<;T;X;p;t;x;|;
; ;8;H;L;d;h;l;
; ;9;J;a;
; ;s;
; <(<D<d<l<t<|<
; <@<
; <x<
; <X<
; <X<|<
; =;=E=Z=o=
; expected 
; last read: '
;!;&;5;H;
;!;:;Q;
;!;-;9;
;!;@;
;!;];q;
;!;};
;!;+;0;8;E;
;!;1;8;E;M;p;
;!;1;A;Q;a;q;
;!;5;M;W;\;a;u;
;!;6;K;c;u;
;!;8;A;h;q;
;!;d;
;!;I;
;!;U;
;!<!=@>
;!<[<
;!<o<x<
;!<P<d<y<
;!<T<
;";(;.;];
;";(;.;4;:;@;F;L;R;X;^;d;j;p;v;|;
;";(;2;8;B;H;R;X;b;h;r;x;
;";,;A;V;v;
;";,;P;
;";];j;y;
;";9;
;";L;
;"<3<O<f<
;"<9<N<
;"<M<{<
;"=,=2=q=
;#;(;R;
;#;*;C;T;k;x;
;#;=;H;W;
;#;2;
;#;5;];b;h;n;
;#<*<C<x<
;#<:<[<
;#<;<W<
;#<5<]<b<h<n<
;#<5<V<[<a<
;#<F<s<
;#<M<v<
;#<P<s<
;#<W<
;$;(;,;@;P;`;d;|;
;$;(;,;@;P;`;d;h;
;$;(;,;0;8;P;`;d;t;
;$;(;,;4;L;\;l;p;
;$;(;,;D;T;d;t;x;|;
;$;(;@;D;\;`;x;
;$;(;@;P;T;d;h;l;
;$;(;@;P;T;d;h;p;t;x;
;$;(;@;P;T;l;|;
;$;(;@;P;T;X;p;
;$;(;8;<;@;X;h;l;|;
;$;*;T;
;$;,;@;`;l;
;$;,;4;@;`;l;
;$;,;4;@;d;l;t;|;
;$;,;4;<;D;L;d;|;
;$;,;4;<;D;L;T;\;d;l;t;
;$;,;4;<;D;L;T;\;d;l;t;|;
;$;,;4;<;D;L;T;\;d;p;
;$;,;4;<;D;L;T;\;h;
;$;,;4;<;D;L;T;`;
;$;,;4;<;D;P;p;x;
;$;,;4;<;D;P;t;|;
;$;,;4;<;D;P;X;|;
;$;,;4;<;D;P;X;x;
;$;,;4;<;H;l;t;|;
;$;,;4;D;L;X;x;
;$;,;8;X;`;h;t;
;$;,;8;X;`;l;
;$;,;8;X;h;
;$;<;L;\;`;p;t;x;
;$;0;8;l;t;|;
;$;0;P;X;`;h;p;
;$;0;T;\;d;l;t;|;
;$;4;:;B;W;c;|;
;$;4;8;<;@;D;H;L;P;T;X;\;`;d;h;l;
;$;4;8;H;L;P;T;h;l;
;$;4;8;P;`;d;t;x;|;
;$;6;H;i;
;$;8;S;l;q;
;$;c;
;$;D;L;X;x;
;$;D;P;p;|;
;$;y;
;$<?<h<
;$<a<f<n<w<!=d=
;$<L<
;%;/;:;F;O;
;%;:;O;f;
;%;-;7;C;K;n;z;
;%;@;
;%;@;P;v;
;%;1;K;R;Y;g;
;%;4;@;H;X;`;p;x;
;%;F;K;Q;p;
;%;X;
;%;Y;
;%</<T<k<
;%<=<q<
;%<M<g<q<
;%<N<^<z<
;&;-;5;;;A;H;O;U;d;s;};
;&;?;b;r;
;&;?;E;I;S;i;y;
;&;?;y;
;&;+;1;P;
;&;+;1;Y;
;&;=;Q;
;&;8;
;&;8;c;l;
;&;C;k;
;&;I;Z;v;
;&<0<@<f<w<
;&<3<T<v<
;&<n<~<
;&<S<_=">
;&<w< =3=a=
;(;,;<;@;X;\;`;h;l;
;(;,;<;L;P;T;l;|;
;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;t;x;
;(;,;0;8;<;D;H;\;`;p;
;(;,;0;H;L;P;T;X;\;d;h;l;p;x;
;(;,;D;H;`;d;h;|;
;(;;;G;`;j;};
;(;-;2;G;Q;];x;};
;(;0;<;\;d;l;x;
;(;0;8;@;H;T;t;
;(;1;X;a;
;(;2;K;
;(;8;<;@;X;h;l;|;
;(;8;D;L;l;
;(;b;i;
;(;B;v;
;(;D;T;`;
;(;L;T;\;d;l;t;|;
;(;P;
;(<(=(>(?
;(</<I<`<
;(<;<D<R<m<}<
;(<_<
;(<`<
;(<}<
;(<h<
;(<z<
;(=F=Y=
;(=R=
;);?;N;Z;b;r;z;
;);1;7;;;A;`;h;x;
;);3;=;E;M;U;^;g;p;~;
;);3;V;
;);5;D;I;W;\;
;);G;o;
;)<l<
;)<P<
;*;?;W;{;
;*;K;i;
;*<4<F<`<k<
;*<A<F<K<`<j<t<
;*<M<\<s<x<
;*=/=l=q=
;,;<;@;D;\;l;p;t;
;,;<;@;X;h;l;p;t;|;
;,;<;H;P;p;
;,;<;L;P;`;d;|;
;,;0;@;P;T;X;p;t;
;,;0;4;<;T;d;h;x;|;
;,;0;4;8;L;P;T;X;`;t;x;
;,;0;4;H;X;\;t;
;,;1;6;M;j;
;,;3;H;];s;
;,;4;@;d;
;,;4;<;D;L;T;\;h;
;,;8;?;O;[;g;t;
;,;8;X;d;l;
;,;H;X;d;l;
;,<D<L<T<\<d<
;,<u<
;.;8;?;F;a;z;
;.;E;\;s;
;.;L;h;
;.<{<
;.<5<J<_<
;.<e<|<
;.<r<
;.<v<
;/;8;R;
;/;9;R;l;v;
;/;A;L;R;g;
;/;D;n;
;/;O;x;
;/<9<K<
;/<J<
;:;[;
;:;N;
;:;v;
;:<A<V<k<
;:<D<Y<n<{<
;:<G<v<
;-;:;R;X;_;
;;;r;
;;<]<o<
;;<_<
;;<~<
;;<Q<
;-;>;G;M;
;';1;s;
;';2=*>4>M>w>
;';3;L;e;q;};
;';b;
;-;B;Y;
;-;D;\;d;
;-;O;
;-;R;z;
;';V;t;~;
;?;g;
;?;I;n;
;?;l;
;?;P;x;
;?<D<
;?<F<[<p<
;?<l<
;?<O<
;?<q=
;?<v<
;@;h;
;@;p;
;@;t;
;@;x;
;@<h<
;@<w<
;@<x<
;[;b;{;
;\$,|
;\$0u
;] tm
;] tyk
;]@}Y
;]<B=\=
;]lu";Epu
;^;h;
;`;{;
;`<j<
;{@})
;{<}~
;{<}+
;|$ u
;|$0r
;~ ~X
;+;/<9<N<c<z<_=~=
;+;?;
;+;9;>;
;+;B;G;n;
;+;c;s;y;
;+;U;_;x;
;+;w;
;+<|<
;+<A<
;+<Q<[<{<
;+<Z=
;'<,<8<D<
;<;a;~;
;<;D;`;h;p;x;
;<;D;L;T;\;d;l;t;|;
;<;D;L;T;\;h;
;<;D;P;p;x;
;<;g;
;<;L;];q;
;<;L;X;`;
;<;T;`;l;v;{;
;<<e<
;<<F<[<p<
;<=Q=t=
;'<4<_<e<}<
;-<A<
;'<K<
;-<m<
;=;B;P;Z;g;t;
;=;C;W;
;=;Q;
;=<k<
;>;H;W;w;
;><W<m<
;0;@;d;l;t;|;
;0;<;\;d;p;
;0;4;D;H;L;T;X;`;x;
;0;4;L;P;h;x;|;
;0;4;L;P;T;h;l;p;t;
;0;8;@;H;P;X;`;l;
;0;D;#<|<
;0;O;
;0;p;;?P?V?n?}?
;0;u;
;0;U;p;
;0;x;
;0<]<
;0<h<
;0<u<
;0<X<
;0v:f
;1;H;
;1;I;d;~;
;1=I=L?d?
;1V1q1
;2;7;B;O;[;e;o;y;
;2;9;e;t;
;2;A;F;
;2;I;
;2;I;N;e;j;};
;2<`<
;2<x<
;3;:;`;
;3;=;V;g;r=j>t>
;3;B;
;3;E;m;r;x;~;
;3;M;X;g;
;3<E<f<k<q<
;4;<;d;l;
;4;<;D;L;T;\;d;l;t;|;
;4;<;D;L;T;`;
;4;8;P;`;d;h;
;4;I;`;
;4;l;
;4;P;`;p;
;4<K<P<U<l<
;4<L<
;4<t<
;5;^;n;
;5<:<f<p<E=
;5<l<v<
;6;;;A;i;
;6;E;
;6;P;g;
;6<a<
;6<G<
;6<G<i<z<
;6<Y=
;7;];t;
;7;b;
;7;R;m;
;7;V;`;
;8;@;\;|;
;8;A;d;x;
;8;A;h;q;
;8;D;d;l;t;
;8;p;
;8;T;d;p;x;
;8<p<
;8<P<Z<s<
;8<x<
;8<X<d<
;9;{;
;9;C;c;
;9;f;
;9;i;
;9?>?p?
;9<z<
;A;];
;A;};
;A;v;
;A;Y;
;a<n<
;A=J=
;AHr*
;AHr/
;AHrA
;B;G;M;p;
;b;o;
;B<L<_<z<
;B<L<a<v<
;B<o<
;B<R<
;B<Y=
;B>G>
;C sV
;C(v-
;C;M;`;{;
;C;o;
;c;u;
;C;U;v;{;
;C;Y;^;
;C<u<
;C<U<v<{<
;C<W<c<o<
;c=I>S>h>}>
;D$ |
;D$ u
;D$$u
;D$@r
;D$duJ
;D;L;P;T;\;`;d;h;
;D;L;T;l;
;d<p<
;D<p< =p=
;D<Y<
;E v@
;E vC
;E,va
;e;l;
;e;y;
;E@}P
;E<O<h<w<
;F t:jp_
;F t+
;F;M;f;
;F;v;
;f;v;
;f<~<
;f<k<
;F<t4
;F=D?I?
;F8t%
;G uQ
;G;g;
;G;j;t;~;
;G;t;y<
;G;x;
;H;\;
;H;{;
;h;u;
;I;a;
;I;d;n;
;I<h<
;J;p;z;
;J;w;
;J;x;
;J<a<
;J<h<
;j<o<u=
;K<S<
;k<x<
;K<y<
;L$`|
;L$4r
;L$8u
;L$X|
;L;f;r;~;
;L;S;h;};
;L<d<
;L<d<n<
;m<w<
;M<Z<
;MD}C
;N@sL
;n<x<
;O;b;
;O;V;n;
;O<Q?
;P;m;
;q;{;
;Q;h;
;Q=h>o>
;R;f;
;s,r4w
;s,t%
;S;+<
;S;s;
;S<]<r<
;S<e<
;t$ |
;T$ t%
;t$$|
;t$$r
;t$(r
;t$,|
;t$,v-
;T$<u
;T$0u
;T$8u
;t$dr
;T<`<
;T<`<s<
;T<d<t<
;t<y<
;u t6
;u$s$
;u,s$
;U;_;n;
;U<i<
;U<p<
;uD}P
;v;};
;V<c=
;v>R?c?|?
;W;s;
;W<d<
;W<n<s<
;x<[=
;X<a<
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
? ?$?(?,?0?4?8?<?@?D?X?\?t?
? ?$?(?,?0?4?8?<?D?H?L?P?T?X?\?`?d?h?l?p?t?x?
? ?$?(?,?0?8?P?T?X?\?`?d?h?l?p?t?x?|?
? ?$?(?@?D?\?`?d?h?|?
? ?$?(?0?4?8?<?d?l?p?t?|?
? ?$?(?0?D?H?`?p?t?
? ?$?,?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
? ?$?<?@?X?\?`?h?
? ?$?<?L?P?T?l?p?t?|?
? ?$?4?8?<?T?d?h?
? ?&?,?2?8?>?D?J?P?V?\?a?e?i?m?q?u?y?}?
? ?(?@?P?`?d?l?p?
? ?(?0?8?@?H?X?h?p?x?
? ?(?0?8?D?d?l?x?
? ?(?4?T?\?d?p?
? ?(?4?T?`?
? ?,?4?h?x?
? ?,?L?T?\?d?l?t?|?
? ?,?L?X?x?
? ?'?,?3?[?`?l?y?
? ?'?.?K?
? ?@?H?P?\?|?
? ?@?H?P?X?`?h?p?|?
? ?@?H?T?t?|?
? ?0?@?H?X?d?t?
? ?0?@?P?`?p?
? ?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?t?
? ?8?<?@?D?X?\?`?x?
? ?8?H?L?P?h?l?p?x?|?
? ?9?D?S?v?
? ?D?L?T?\?d?l?t?|?
? ?D?T?\?d?l?t?|?
? ?T?d?p?x?
? @ T T 3
?!?(?-?:???L?Q?
?!?-?
?!?1?a?k?u?
?!?2?
?!?2?Z?o?w?
?!?C?p?
?!?f?
?!?i?o?
?!?J?
?"?(?.?T?
?"?)?0?9?M?]?b?n?z?
?"?.?4?F?d?
?"?.?I?N?c?
?"?7?L?c?}?
?"?9?a?j?
?"?9?P?g?~?
?"?A?Y?s?
?"_^t
?#?(?k?
?#?*?C?T?s?
?#?;?`?
?#?'?+?/?3?7?;???C?G?K?O?S?W?
?#?^?
?#?4?K?
?#?K?
?#?N?
?#?Z?
?$?(?,?@?D?H?L?P?T?X?\?`?d?l?p?t?x?
?$?(?,?D?H?L?`?d?|?
?$?(?@?D?\?`?x?|?
?$?(?<?@?P?`?d?|?
?$?(?0?4?8?@?D?H?L?t?|?
?$?(?8?<?T?d?h?x?
?$?,?<?D?L?T?\?d?l?t?|?
?$?,?4?@?`?l?t?
?$?,?4?<?D?`?
?$?,?4?<?D?L?T?
?$?,?4?<?D?L?T?\?d?l?t?
?$?,?4?<?D?L?T?\?d?l?t?|?
?$?,?4?<?D?L?T?\?d?l?x?
?$?,?4?<?D?L?T?\?d?p?
?$?,?4?<?D?L?T?\?h?
?$?,?4?<?D?L?X?x?
?$?,?4?<?D?P?p?x?
?$?,?4?<?H?P?p?
?$?,?8?X?`?p?
?$?,?8?X?h?
?$?-?
?$?0?8?l?|?
?$?0?8?X?t?
?$?0?P?X?d?
?$?0?T?\?d?l?t?|?
?$?1???M?[?d?i?
?$?4?8?<?D?H?\?`?d?x?
?$?4?8?H?L?P?h?l?
?$?4?8?H?X?\?l?p?t?
?$?4?8?P?`?d?h?
?$?4?D?H?L?T?l?|?
?$?6?B?M?R?W?b?g?m?s?
?$?8?I?
?$?D?L?T?\?h?
?$?D?L?T?`?
?%?;?
?%?2?:?B?
?%?B?
?%?F?K?Q?p?
?%?i?n?t?z?
?%?M?R?X?^?
?&?;?P?g?
?&???F?b?l?
?&?0?G?]?u?
?&?3???
?&?5?
?&?a?
?&?A?a?k?~?
?(?,?<?@?D?\?`?x?|?
?(?,?<?@?D?H?P?h?x?|?
?(?,?0?4?8?@?X?\?t?
?(?,?0?H?X?\?l?p?t?
?(?,?D?T?X?\?d?|?
?(?@?
?(?0?8?@?H?P?`?
?(?0?8?@?H?P?d?l?t?|?
?(?0?P?l?|?
?(?1?H?Q?h?q?
?(?1?X?a?s?
?(?1?X?a?x?
?(?4?<?\?d?l?t?
?(?8?<?@?D?X?h?l?|?
?(?8?<?L?P?`?d?t?
?(?8?H?X?`?p?t?
?(?8?H?X?h?x?
?(?D?[?
?(?D?L?T?\?d?p?x?
?(?g?}?
?(?H?T?t?|?
?)?D?J?
?)?v?
?*?5?;?G?V?[?
?*?6?O?U?Y?c?y?
?*?E?O?[?v?
?*?I?
?,?;?B?I?P?
?,?@?T?\?d?l?t?|?
?,?<?@?D?\?`?x?
?,?<?@?D?\?l?p?
?,?<?@?X?\?`?t?
?,?<?L?P?T?\?`?h?l?
?,?0?4?8?<?@?H?`?d?|?
?,?4?@?`?l?
?,?4?<?D?L?T?\?d?p?
?,?A?X?s?
?,?H?X?d?l?
?,?L?T?\?d?l?x?
?,?N?
?,?T?{?
?.?]?d?}?
?.?8?]?
?.?8?Q?b?
?.?B?V?h?
?.?T?y?
?/?|?
?/?7???z?
?/?D?[?
?/?o?
?/?W?
?:?I?
?;?@?N?S?g?l?~?
?;?n?
?;F(r
?'?,?1?@?S?
???D?
?-?7?J?
?'?B?j?
?'?J?z?
?-?P?
?-?U?
?-?U?^?r?
?-?V?d?
?@?x?
?@s-f
?\?|?
?]?d?o?y?
?^?e?
?^?k?
?^?z?
?`?k?q?}?
?+?i?n?
?+?o?v?
?<?D?L?T?\?d?l?t?|?
?<?D?P?p?x?
?<?F?Y?i?
?<?H?l?t?|?
?<?W?
?<?x?
?=?m?
?>?O?k?
?>?q?
?0?:?L?o?
?0?:?S?
?0?@?D?H?L?`?p?t?x?
?0?`?
?0?4?8?<?@?D?H?L?P?T?X?\?`?h?
?0?4?8?<?@?D?L?d?h?l?p?
?0?4?L?P?h?l?
?0?4?L?P?T?h?l?p?
?0?5?B?a?
?0?8?@?H?P?\?|?
?0?8?<?@?H?L?P?T?|?
?0I0n0
?1?;?Q?[?q?{?
?1?;?V?
?1?8?A?H?^?c?p?|?
?1?H?
?1?m?
?1?T?w?
?1?W?a?
?1?X?a?
?2?>?W?]?a?k?
?2?e?
?3?=?b?
?3?E?f?k?q?
?3?F?
?3?W?
?333333
?4?@?`?l?
?4?<?D?L?T?\?d?l?t?|?
?4?<?D?P?p?x?
?4?8?<?@?D?H?\?`?p?t?x?
?4?8?<?@?H?`?p?t?x?|?
?4?8?<?D?\?l?p?
?4?8?P?T?X?`?d?l?p?
?4?9???E?K?x?
?4?d?
?4?h?
?4?I?
?4?O?q?z?
?4?X?x?
?5?^?n?
?5?b?
?6?;?A?`?
?6?;?A?L?{?
?6?;?F?S?_?i?s?
?6?R?\?q?
?6?s?
?7?`?|?
?8?@?H?P?d?l?t?
?8?@?H?P?X?d?l?
?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
?8?=?E?J?Y?^?l?q?y?~?
?8?p?
?8?X?`?l?
?9?@?a?p?
?9?M?V?l?
?A?c?
?A?g?
?A?R?
?B?g?
?B?n?
?c?h?n?}?
?C?J?d?{?
?c?u?
?D?f?p?
?E?c?u?
?E?L?
?E?L?v?{?
?E?O?
?F?b?
?f?z?
?F?Z?t?
?G?N?g?
?H?M?Y?e?
?H?u?
?H?x?
?J?_?w?
?J?d?
?J?P?h?w?
?J?u?
?K?d?w?
?k?x?
?L?S?k?~?
?N?X?q?
?P?|?
?PShH5@
?QPSjpZjoY
?R?c?
?R?m?
?RQRVQ
?RQVjoZRY
?RWSjpZjoY
?S?e?
?T?\?d?l?t?|?
?U?_?q?
?V?`?y?
?V?a?
?V?b?m?s?
?w/WV
?W?m?
?W?q?
?w6VS
?w9VW
?wBVS
?X?e?
?X?f?
?Y?c?i?u?
@ 90u
@$QPR
@$QPW
@.data
@.reloc
@@QLinearGlobalAveragePool ImageSize too large!
@_^[]
@|+Cx
@0J0d0w0
@0PWQ
@PPPj
@PPPPWQh
@PPSQh
@PPSQh0K?
@PPSWQh
@QSPPSQ
@QSPPSQQh
@QSPPSRQh
@QSPPSRQhl
@t";7r
@t"VS
@t$;;r
@Vj(j
@xQSQ
[%hs(%hs)]
[%hs]
'[', '{', or a literal
[:^alnum:]
[:^alpha:]
[:^ascii:]
[:^blank:]
[:^cntrl:]
[:^digit:]
[:^graph:]
[:^lower:]
[:^print:]
[:^punct:]
[:^space:]
[:^upper:]
[:^word:]
[:^xdigit:]
[:alnum:]
[:alpha:]
[:ascii:]
[:blank:]
[:cntrl:]
[:digit:]
[:graph:]
[:lower:]
[:print:]
[:punct:]
[:space:]
[:upper:]
[:word:]
[:xdigit:]
[]^-\
[^\x00-\x{10ffff}]
[0e0~0
[1b1w1
[8GduT
[j*h@
[j.hX`@
[j+hP
[j2h4&@
[j5h0M?
[json.exception.
[libprotobuf %s %s:%d] %s
[Memory] ExecutionFrame dynamically allocates 
[Memory] ExecutionFrame statically allocates 
[Memory] SessionStateInitializer statically allocates 
[ONNXRuntimeError]
[Sh(!@
[ShapeInferenceError] 
[SPj"
[SWWWRQhx
[thunk]:
[tX9]
[TypeInferenceError] 
\$ ;}
\$$VS
\$0;],
\$0;T$
\$0ta
\$4WV
\$H;L$X
\$l;r
\$T;\$X
\directml.dll
\x%02x
\x{%x}
] != number of classlabels[
] (usually, this means you 
] already exists with value [
] because it's the graph's output.
] for now
] is not supported this build 
] is not supported!
] is not supportted!
] not in lexicographic sorted order.
] not in sorted order.
] op_type [
] out of range [0, 
] out of range.
] should not be greater than specified axis dim value [
], could not find NodeArg 
], Value=
], while 
]. Actual value is 
]. It will be overwritten
]. Its actual value is: 
]<+u8
]4;]@}<
^(+N\
^|*W?
^0u0z0
^f;0^s
^j&h\
^j*h@
^j.hX`@
^j@X;
^j0hp
^j4hl
^j5h0M?
^jQh(#?
^Vh<6<
^Vhxs<
^VSQhT
_,9^,
_,9^,|f
_\9^,
_^[Y]
__based(
__cdecl
__clrcall
__eabi
__fastcall
__int128
__int16
__int32
__int64
__int8
__pascal
__ptr64
__restrict
__stdcall
__strncnt
__swift_1
__swift_2
__thiscall
__unaligned
__vectorcall
__w64 
_Cast
_DmlExecutionProvider
_DmlExecutionProvider_
_fence_after
_fence_before
_FusedMatMulAndScale
_initterm
_initterm_e
_Int32
_j]hx
_kernel_time
_L9^,
_lock_locales
_min_zero_constant
_nhwc
_o____lc_codepage_func
_o____lc_collate_cp_func
_o____lc_locale_name_func
_o____mb_cur_max_func
_o___acrt_iob_func
_o___pctype_func
_o___std_exception_copy
_o___std_exception_destroy
_o___stdio_common_vfprintf
_o___stdio_common_vsnprintf_s
_o___stdio_common_vsprintf
_o___stdio_common_vsprintf_s
_o___stdio_common_vswprintf
_o__aligned_free
_o__aligned_malloc
_o__beginthreadex
_o__callnewh
_o__calloc_base
_o__cexit
_o__CIcosh
_o__CIfmod
_o__CIpow
_o__CIsinh
_o__CItanh
_o__close
_o__configure_narrow_argv
_o__create_locale
_o__crt_atexit
_o__dclass
_o__difftime64
_o__errno
_o__execute_onexit_table
_o__fdclass
_o__fdsign
_o__free_base
_o__free_locale
_o__fseeki64
_o__fstat64i32
_o__get_errno
_o__get_stream_buffer_pointers
_o__Getdays
_o__Getmonths
_o__Gettnames
_o__gmtime64_s
_o__initialize_narrow_environment
_o__initialize_onexit_table
_o__invalid_parameter_noinfo
_o__invalid_parameter_noinfo_noreturn
_o__libm_sse2_acos_precise
_o__libm_sse2_asin_precise
_o__libm_sse2_atan_precise
_o__libm_sse2_cos_precise
_o__libm_sse2_exp_precise
_o__libm_sse2_log_precise
_o__libm_sse2_pow_precise
_o__libm_sse2_sin_precise
_o__libm_sse2_sqrt_precise
_o__libm_sse2_tan_precise
_o__localtime64_s
_o__lock_file
_o__malloc_base
_o__mktime64
_o__purecall
_o__read
_o__realloc_base
_o__register_onexit_function
_o__seh_filter_dll
_o__set_errno
_o__sopen_s
_o__stat64i32
_o__Strftime
_o__strnicmp
_o__strtoi64
_o__towlower_l
_o__towupper_l
_o__unlock_file
_o__W_Getdays
_o__W_Getmonths
_o__W_Gettnames
_o__wcsdup
_o__Wcsftime
_o__wfsopen
_o__write
_o__wsopen_s
_o_abort
_o_acoshf
_o_asinhf
_o_atanhf
_o_atol
_o_bsearch
_o_calloc
_o_ceil
_o_fclose
_o_fflush
_o_fgetc
_o_fgetpos
_o_floor
_o_fputc
_o_fread
_o_free
_o_frexp
_o_fseek
_o_fsetpos
_o_fwrite
_o_isalpha
_o_isdigit
_o_islower
_o_isspace
_o_isupper
_o_ldexp
_o_localeconv
_o_log2
_o_log2f
_o_malloc
_o_remainderf
_o_rint
_o_rintf
_o_roundf
_o_setlocale
_o_setvbuf
_o_strcpy_s
_o_strerror
_o_strncpy_s
_o_strtod
_o_strtof
_o_strtol
_o_strtoll
_o_strtoull
_o_terminate
_o_tolower
_o_ungetc
_o_wcsftime
_RuleBasedTransformer
_token_
_unlock_locales
_Unused
_WhXs<
_WRRj
`.rdata
`>d>h>l>p>t>x>|>
`adjustor{
`anonymous namespace'
`anonymous-namespace'::GetExternalDataInfo
`anonymous-namespace'::ReadExternalDataForTensor
`copy constructor closure'
`default constructor closure'
`dynamic atexit destructor for '
`dynamic initializer for '
`eh vector constructor iterator'
`eh vector copy constructor iterator'
`eh vector destructor iterator'
`eh vector vbase constructor iterator'
`eh vector vbase copy constructor iterator'
`generic-class-parameter-
`generic-method-parameter-
`generic-type-
`local static destructor helper'
`local static guard'
`local static thread guard'
`local vftable'
`local vftable constructor closure'
`managed vector constructor iterator'
`managed vector copy constructor iterator'
`managed vector destructor iterator'
`non-type-template-parameter
`omni callsig'
`placement delete closure'
`placement delete[] closure'
`RTTI
`scalar deleting destructor'
`string'
`template static data member constructor helper'
`template static data member destructor helper'
`template-parameter
`template-parameter-
`template-type-parameter-
`tf_half_pixel_for_nn` is deprecated since opset 13, 
`typeof'
`udt returning'
`unknown ecsu'
`vbase destructor'
`vbtable'
`vcall'
`vector constructor iterator'
`vector copy constructor iterator'
`vector deleting destructor'
`vector destructor iterator'
`vector vbase constructor iterator'
`vector vbase copy constructor iterator'
`vftable'
`virtual displacement map'
`vtordisp{
`vtordispex{
{"cat" : "
{%d,%d}
{%d,}
{additionalDocumentation}
{flat}
{for 
{name}
|$ RQ
|$$;}
|$(+D$
|$(+L$
|$,;\$$
|$\ua
|$H;}
|$H9}
|$T;}
|$T9}
|<3=w=
|0 9\0$|
|oj8O
} for per-channel quantization. Actual:
} for per-tensor/layer quantization or shape {
}, actural: 
}, Got: 
}, input shape = {
}. Actual:
}. Got: 
}.VPW
}:+D$
}L+L$
}N+L$
}T+L$
}Tj X
~,9~$t
~,j4j
~_;;r
~_SV+
~0;;r
~3a*~3a*~3a*~3a*
~4;;r
~4;7r
~59>u
~6;7r
~8;7r
~B;~,r
~d;=D
~h_^]
~ljtX
~p;~p
~X_[^
+/+E+F+M+s+v+
+|$ Q
+|$$Q
++<>||~~
++index < c.size()
++Q5@.
++Q5@.Q5@.Q5@.Q5@.
+101s1}1
+BL9rL
+GduF
+L$4x\
+N +N
+O(+W
+onnxruntime::DynamicQuantizeMatMulFusion::ApplyImpl
+reorder
+T$ ;
+v$x+v$xv$+xv+$xv$+x+$vx+$vx$v+x+$vx$+vx+v $+v $v $+v +$v $++$ v+$ v$ v++$ v$+ v+xv$+ v$v$ +v+ $v$ ++x$v+ $v$v ++ $v$ +v
< <$<(<,<0<4<8<?<
< <$<(<,<0<4<8<<<@<D<H<L<P<d<h<l<p<t<x<|<
< <$<(<,<0<4<8<<<@<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
< <$<(<,<0<4<8<<<@<T<X<h<l<|<
< <$<(<,<0<8<P<`<d<h<
< <$<(<@<P<T<\<p<t<
< <$<(<<<@<D<H<P<T<X<\<p<t<x<|<
< <$<<<@<X<\<t<x<
< <$<4<8<P<`<p<t<x<|<
< <$<8<<<@<X<h<l<p<t<|<
< <(<,<@<D<T<X<h<l<|<
< <(<@<D<\<`<x<|<
< <(<<<D<L<T<h<p<x<
< <(<0<@<H<P<X<`<h<p<x<
< <(<0<8<@<P<t<|<
< <(<0<H<X<\<t<
< <,<L<T<\<d<p<
< <=<N<
< <0<@<P<`<p<
< <0<4<8<<<P<T<d<h<x<|<
< <0<4<8<P<T<X<`<x<
< <0<4<L<\<`<x<
< <0<4<L<P<T<\<t<
< <4<8<H<X<h<l<p<
< <4<8<P<T<h<l<|<
< <4<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
< <c<j<
< =%=V=4>W>
< ===z=
< =D=L=T=\=d=l=t=|=
< =H=p=
< =p=
< =P=
< =X=
<!<,<R<
<!<@<
<!<1<A<Q<a<q<
<!<4<T<
<!<d<
<!<h<q<
<!<H<Q<h<
<!=\=
<!=]=
<!=+=D=
<!=V=
<"<(<2<8<B<H<R<X<b<h<r<x<
<"<*<1<S<[<p<~<
<"<'<><[<
<"<'<N<
<"<1<Y<
<"<7<
<"<7<E=[=i=
<"<A<
<"<c<m<
<"='=
<"='=L=V=
<"=6=F=X=}=
<"=8=d=
<"=B=L=V=`=g=~=
<"=j=
<#<;<B<[<l<
<#<-<Y<
<#<=<K<u<
<#<5<]<b<h<n<
<#<5<V<[<a<
<#=B=L=e=
<#=H=^=
<#=P=}=
<$<(<@<D<H<\<l<p<t<|<
<$<(<@<D<L<d<h<
<$<(<8<<<@<X<h<x<
<$<,<<<D<L<\<d<p<
<$<,<4<@<`<h<t<
<$<,<4<@<`<p<
<$<,<4<@<d<l<t<|<
<$<,<4<<<D<L<T<\<d<l<t<|<
<$<,<4<<<D<L<T<\<d<l<x<
<$<,<4<<<D<L<T<\<h<
<$<,<4<<<D<L<T<`<
<$<,<4<<<D<L<X<x<
<$<,<4<L<d<|<
<$<,<6<B<J<m<y<
<$<,<8<@<X<p<x<
<$<,<8<X<`<h<p<x<
<$<,<L<h<x<
<$<,<L<T<\<d<
<$<<<D<L<T<\<d<l<t<|<
<$<<<L<P<`<d<h<
<$<-<6<?<H<o<
<$<0<<<H<
<$<0<<<I<U<c<h<
<$<0<8<X<t<
<$<0<P<p<x<
<$<0<P<X<`<h<x<
<$<0<T<\<d<l<t<|<
<$<4<<<D<L<T<\<d<l<t<
<$<4<8<<<@<H<L<P<X<\<`<h<l<p<x<|<
<$<4<8<H<L<P<d<h<l<p<x<
<$<4<8<H<L<P<h<l<p<
<$<4<8<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
<$<4<D<H<P<T<h<l<p<
<$<4<D<H<X<\<`<t<x<
<$<A<r<
<$<C<M<\<|<
<$<D<L<T<`<
<$<L<T<\<d<
<$=*=B=Q=
<$=,=0=4=<=@=D=H=p=x=|=
<$=\=
<$=`=
<$=R=
<$u.V
<%<-<w<
<%<F<K<Q<p<
<%<F<K<Q<y<
<%<M<R<_<e<k<
<%=/=A=[=">,>>>X>
<%=\=f=y=
<&<.<><E<R<Z<`<d<j<
<&<?<R<s<X=q=
<&<C<h<
<&=c=
<&=N=0?
<&=T=
<(<,<<<L<P<h<l<
<(<,<0<8<P<T<X<l<p<
<(<,<0<H<X<\<`<h<l<
<(<|<
<(<-<8<=<G<P<V<_<d<n<x<
<(<0<4<8<@<D<H<L<t<|<
<(<1<X<a<
<(<2<
<(<3<?<N<]<l<}<
<(<8<@<H<P<l<t<|<
<(<8<H<L<P<T<\<`<t<x<
<(<8<H<X<h<x<
<(<8<H<X<p<
<(<H<P<\<|<
<(<H<T<t<
<(<L<T<\<d<l<t<|<
<(<W<\<j<o<
<(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
<(=`=
<(=|=
<(=3=8=?=O=Z=h=u=
<(=h=
<(=L=
<(=p=
<(=p>
<(=x=
<(=X=
<(>}>
<)</<A<G<Y<d<j<o<x<
<)<><V<f<m<
<)<9<{<
<)<F<L=k=r=
<)={=
<)=A=T=
<)=T=
<*<:<_<v<
<*<4<R<v<
<*<C<I<M<W<m<}<
<*<h<q<
<*<P<\<
<*=A=F=T=a=m=w=
<*=H=
<*=s=
<,<;<K<e<
<,<;<S<]<r<
<,<_<i<
<,<<<@<D<\<l<p<t<x<
<,<<<@<P<T<d<t<
<,<<<@<P<T<X<`<d<l<
<,<<<@<X<\<t<x<
<,<<<H<P<p<
<,<<<L<P<`<d<h<p<t<|<
<,<0<@<P<T<X<\<`<d<h<l<p<t<x<|<
<,<0<4<<<@<H<L<`<d<t<
<,<0<4<L<P<X<\<p<t<
<,<0<H<X<\<`<d<x<|<
<,<2<><M<R<
<,<4<@<`<h<p<|<
<,<4<<<D<L<T<\<
<,<4<<<D<L<T<\<d<l<t<|<
<,<4<<<D<L<X<x<
<,<4<<<H<h<t<
<,<4<<<h<p<x<
<,<7<J<P<k<
<,<8<X<d<
<,<H<X<d<l<
<,<v<
<,<x<
<,<Y<
<,=}=
<,=1=
<,>L>i>w>
<.<%=,=
<.<I<d<
<.<W<v<}<
<.=B=h=|=
<.=e=
<.=Y=
</===L=c=~=
</=b=q=
</=P=d=m=
</assembly>
<:<m<
<:=[?y?
<:=D=
<:>F>q?
<:t%<,t!</u%
<:u WSj;j
<:u#W
<;<l<q=
<;<P<V<n<}<
<;=R=m=~=
<?<d<
<?xml version='1.0' encoding='UTF-8' standalone='yes'?>
<@<`<
<@<H<L<P<X<\<`<d<
<@<p<
<@=[=
<@=G=[=~=
<@=J=n=
<@=x=
<@t A
<[<x<
<[=%>2>X?
<\<}<
<^=v=
<_u2A;
<{<!=A=W=
<+<@<F<^<m<
<+<3<C<K<[<c<o<{<:=I=X=d=l=|=
<+<B<Y<p<
<+<G<Q<f<{<
<+<W<
<+<X<
<+=?=Z=n=
<+=0=^=
<+=G=Q=e=z=
<+=N=X=b=l=q=
<'<.<G<X<o<
<'<;<H<
<'<@<U<i<}<
<-<_<
<<<`<l<
<<<D<L<T<\<d<l<t<|<
<<<H<h<p<x<
<<<H<l<t<|<
<<<x<
<<=p=z=
<<=S=n=
<'<1<G<]<A=V?
<-<2<A<`<
<-<B<A>
<-<B<Y<
<-<E<U<s<
<-<g<n<
<'<H<a<
<-<s<w<{<
<=<{<0=m=
<==]=
<'=-=F=_=
<==G=Z=u=
<==Z=m=y=
<-=4=M=
<-=C=U=v={=
<'=C=W=
<'=h=
<'=l=
<'=S=
<><]<
<><f<z<
<><H<R<\<c<z<
<><H<R<\<f<p<z<
<><N<T<\<q<}<
<0|*<9
<0|]<8
<0|k<9
<0|Q<7
<0<@<L<T<
<0<]<q<
<0<`<
<0<<<\<d<p<
<0<4<8<@<H<`<d<|<
<0<8<@<H<P<\<|<
<0<8<@<H<P<X<`<l<
<0<8<@<L<l<x<
<0<8<<<@<H<L<P<T<|<
<0<8<D<d<p<
<0<E<]<
<0<G<
<0<l<q<z<
<0<p<
<0<R<
<0<Z<d<}<
<0=z=
<0A0.131
<1<><
<1<Y<
<1=;=\=
<1=f=z=
<1=L=
<1>;>P>e>|>
<2<|<
<2<<<T<
<2<<<U<
<2<7<?<D<M<R<]<b<g<v<
<2<C<g<|<
<2<N<U<j<
<2<p<
<2=<=O=|=
<3<<<3=
<3<8<O<T<g<l<
<3<c<v<
<3=E=f=k=q=
<3=E=m=r=
<3=q=
<4<;<\<k<
<4<<<D<L<T<\<d<l<t<|<
<4<<<D<L<T<`<
<4<<<H<h<p<|<
<4<<<L<X<x<
<4<8<<<P<`<d<|<
<4<D<H<`<p<t<
<4<D<P<t<|<
<4<P<p<x<
<4=;=P=e=|=
<4=@=S=e=
<5<:<a<
<5<F<a<x<
<5<J<
<5=?=d={=
<5=}=
<6<;<A<i<
<6<D<T<d<
<6<i<~<
<6<J<p<
<6<Y<|<
<6=}=
<6=G=W=l=
<6=T=Y=
<6=y=!>
<6>Q>[>
<7?G?a?s?
<7<c<
<7<C<
<7<N<S<z<
<7=N=S=X=m=w=
<8<@<D<H<P<T<X<\<
<8<@<H<P<\<|<
<8<@<H<T<t<|<
<8<@<L<l<t<
<8<\<
<8<B<i<
<8<G<
<8<p<
<8=`=
<8=d=k=
<8=Q=
<8=x=
<9<@<
<9<\<f<p<z<
<9<n<
<9<q<
<9<Y<^<f<q<
<9=z=
<A<{<
<A<c<w<
<A<K<`<u<
<A<U<f<x<
<A<v<
<A=N=o=
<A=Q=
<assembly xmlns='urn:schemas-microsoft-com:asm.v1' manifestVersion='1.0'>
<B<G<T<;>
<B<n<
<B<o<
<b<z<
<b=l=
<b>i>{>
<C?U?v?{?
<C<f<r<
<C=L=
<C=T=_=
<C=U=v={=
<D<h<
<D<I<O<U<[<
<D<L<T<\<d<l<t<
<D<Q<r<w<
<D<X<k<~<
<D=h=
<d=p=
<D=P=c=u=
<D=t=
<e<j<
<E<L<m<|<
<E<O<^<{<
<E<R<
<E<Y<f<t<
<E=|=
<ellipsis>
<f<}<
<f=u=
<G<O<s<
<g<q<
<G=o=
<G=s=
<G=V=
<g>q>
<H<h<
<H<Q<x<
<H<t<
<H=l=
<I<|<
<i<o<y<
<I<q<)=
<I<v<
<J<T<
<J=Q=
<K<X<
<L<\<h<
<L<\<h<p<
<L<l<
<L<Q<\<a<l<q<}<
<L=Q=:>
<M<a<
<M=5?J?
<M=a=
<M=c=|=
<M=Z=
<M>R>!?
<N<v<
<n=v=
<O<c<
<O<T<Y<a<r<~<
<O=j=
<o=S?
<p_fd> is less than 0.
<p_fd> less than 0.
<P<y<
<parse error>
<Pukj
<q<v<
<Q<X<q<
<Q=X=j=
<R<f<|<
<R<k<
<SVWj
<T<%=R=#>P>}>
<T<^<{<
<T<x<
<T=`=s=
<T=l=
<T=o=
<U+%.4X>
<u<|<
<U<a<j<o<
<U>_>t>
<U>Z>
<uninitialized>
<unknown>
<V<h<
<W<{<
<w=|=
<W=r=
<X<n<
<X=b={=
<X=p=z=
<xt"<Xu!
<xt%<Xu$
<Y<^<d<j<
<Y<|<
<z~$<A|
<Z<i<n<~<
<Z<p<
= =$=(=,=@=D=T=X=\=t=x=
= =$=(=,=0=4=8=<=@=D=H=\=`=d=h=l=p=t=
= =$=(=,=0=4=8=<=@=D=H=L=`=d=|=
= =$=(=,=0=4=8=<=@=D=H=L=P=d=h=l=p=t=
= =$=(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
= =$=(=,=0=4=8=<=@=D=H=P=h=l=
= =$=(=,=0=4=8=<=@=P=X=\=`=h=l=p=t=
= =$=,=0=4=8=`=h=l=p=x=|=
= =$=,=0=D=T=X=\=p=t=
= =$=<=@=X=\=d=h=p=t=
= =$=4=8=P=T=l=|=
= =%=2=7=F=T=
= =&=*=0=J=Z=`=h=}=
= =&=>=M=
= =(=@=D=H=P=T=h=x=|=
= =(=@=P=T=l=p=
= =(=0=@=d=
= =(=0=<=\=d=l=x=
= =(=4=T=\=h=
= =(=8=\=d=l=t=|=
= =(=H=d=t=
= =(=H=P=l=|=
= =,=L=T=\=d=p=
= =;=B=I=R=Y=l=r=
= =@=H=d=l=t=|=
= =@=H=P=\=|=
= =@=H=T=t=|=
= =@=L=l=t=|=
= =@=L=l=x=
= =}=i>n>
= =0=@=D=H=`=d=h=|=
= =0=@=P=`=p=
= =0=4=8=L=P=T=X=l=p=
= =0=4=L=P=h=l=p=t=x=|=
= =4=D=H=L=d=h=
= =5=
= =8=<=T=X=p=t=
= =8=H=L=\=`=d=x=
= =D=L=T=\=d=l=t=|=
= =D=T=\=d=l=t=|=
= =s=
= >@>H>P>l>
= >4>d>x>~>
= >I>g>|>
= >P>f>m>
= >x>
=!=%=?=O=~=
=!=`=
=!=+=B=H=Q=[=q={=
=!='=r=
=!=1=A=Q=a=q=
=!=5=I=
=!=8=A=X=a=x=
=!=8=Z=
=!=9=O=]=q=~=
=!=h=q=
=!=H=S=o=
=!=x=
=!>^>
=!>5>
=!>8>=>B>W>a>x>
=!>n>
=!>Y>
="=,=6=@=J=T=v=
="=.=G=M=Q=[=q=
="=:=C=W=
="=7=R=
="=P=y=
="=S=
=">,>E>V>m>
=">`>
=#=(=.=4=A=T=
=#=(=-=5=B=
=#=)=-=7=X=h=n=v=
=#=+=5=A=I=x=
=#===H=W=
=#=-=5=9=A=K=O=U=Y=_=c=s=
=#=3=>=C=L=Q=\=a=k=p=z=
=#=5=]=b=h=n=
=#=W=}=
=#>=>Q>h>m>z>
=#>5>]>b>h>n>
=#>h>
=#>n>w>
=#>U>s>
=$=(=,=0=X=`=d=h=p=t=x=|=
=$=(=,=4=8=<=@=D=H=L=P=T=X=\=`=d=x=|=
=$=(=,=4=8=<=@=h=p=t=x=
=$=(=,=4=L=\=`=x=|=
=$=*=3=9=m=
=$=*=6=B=
=$=,=@=H=\=d=x=
=$=,=4=@=`=h=p=|=
=$=,=4=<=D=
=$=,=4=<=D=L=T=\=d=l=t=|=
=$=,=4=<=D=L=T=\=d=l=x=
=$=,=4=<=D=L=T=\=d=p=
=$=,=4=<=D=L=T=\=h=
=$=,=4=<=D=L=T=\=t=|=
=$=,=4=<=D=L=X=|=
=$=,=4=<=D=P=p=
=$=,=4=<=H=h=p=|=
=$=,=4=<=H=P=t=
=$=,=8=\=d=l=t=|=
=$=,=8=X=`=h=p=|=
=$=,=8=X=d=
=$=,=L=h=x=
=$=<=F=[=p=
=$=0=8=l=|=
=$=0=P=\=d=|=
=$=0=P=X=d=
=$=2=
=$=4=8=<=@=H=L=`=p=t=
=$=4=8=<=P=T=X=\=d=|=
=$=4=D=H=L=d=h=l=t=
=$=D=L=T=\=l=t=
=$=D=P=p=|=
=$>)>/>5>;>p>
=$>6>H>
=$>H>l>
=%=*={=
=%=*=Q=
=%=^=
=%=`=j=
=%=+=`=
=%=+=a=
=%=8=Y=
=%=E=
=%=F=K=Q=p=
=%=K=_=h=
=%=M=R=X=^=
=%>;>[>
=%>`>
=%><>]>
=%>>>T>
=%>V>
=&=,=>=[=2>S>c>n>
=&=,=>=N=X=b=l=
=&=.=3=:=?=L=_=
=&=.=6=
=&=~=
=&=+=1=Y=
=&=0=I=
=&>]>
=&>}>
=&>>>W>a>h>
=&>0>I>
=&>8>_>u>
=&>L>d>
=&>m>s>
=&>V>}>
=(=,=<=@=X=\=t=x=|=
=(=,=0=4=H=X=\=`=x=
=(=,=0=4=H=X=\=l=|=
=(=,=0=H=L=d=t=x=
=(=,=0=H=L=P=d=h=l=
=(=,=0=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
=(=,=0=H=X=\=`=d=h=l=p=t=x=|=
=(=,=0=H=X=\=`=d=x=|=
=(=,=D=T=X=p=
=(=?=V=m=
=(=^=
=(=0=8=H=P=`=p=x=
=(=4=T=`=
=(=8=H=l=t=|=
=(=8=H=P=`=d=t=x=
=(=8=H=X=h=x=
=(=H=P=X=`=p=
=(=H=P=X=d=
=(=H=T=t=
=(=L=d=l=t=|=
=(=T=
=(=V=m=x=
=(>`>
=(><>E>
=(>2>
=(>2>W>k>
=(>A>
=(>c>y>\?h?
=(>h>
=(>k>
=)?7?`?
=)=>=Q=d=w=
=)=3=E=]=
=)=T=Z=r=
=)>.>4>:>d>
=)>/>B>Q>
=)>^>
=)>3>E>e>[?
=)>3>M>a>u>
=)>I>
=)>X>
=*?/?
=*=`=g=
=*=2=G=l=z=
=*=4=G=T=a=
=*=6=@=V=
=*=B=
=*>?>R>^>s>~>
=*>B>o>
=*>H>a>w>
=,=?=K=g=n=s=z=
=,=`=
=,=<=L=P=`=d=t=
=,=<=L=P=h=l=p=t=x=
=,=<=P=`=p=
=,===
=,=0=@=D=T=d=h=x=|=
=,=0=@=P=`=d=t=x=|=
=,=0=@=P=T=X=p=
=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=l=
=,=0=4=H=X=h=l=p=t=
=,=0=H=L=d=h=
=,=0=H=L=d=t=x=|=
=,=P=x=
=,=T=h=
=,>Q>\>p>
=.=C=3>:>J>T>
=.=C=Z=y=
=.=D=
=.>[>#?|?
=.>d>x>
=/=>=y=
=/=5=
=/=6=O=m=
=/=9=R=l=v=
=/=A=I=Q=
=/=J=
=/=L=i=
=/=N=X=h=
=/>@>
=;=H=
=;=S=
=;=v=
=;=y=
=;>}>
=;>x>
=?>x>
=-?q?
=@=J=o=|=
=@=O=
=@=y=
=@>u>
=@>x>
=@>X>`>
=[=.>n>
=[=q=
=[>e>
=[>s>}>
=^=r=
=^>{>
=^>k>
=_=d=r=w=
=`=q=
=`>w>|>
=+?g?
=+=;>x>}>
=+=<=Z=a=z=
=+=B=
=+=P=x=
=+>2>G>\>s>
=<=d=
=<=r=
=<=T=l=t=
=<=X=
=<>T>^>w>
='=.=Z=_=k=w=
='=/=j=
=-=:=E=Q=V=c=
='=<=S=m=x=
===\=n=
===Q=
==> Context: 
==>x>
=-=B=
=-=c=}=
=-=E=O=d=y=
=-=N=x=
=-=Q=X=q=
=-=X=
='=Y>l>
='>;>
=>=H=m=
=>=H=R=\=f=p=z=
=>=S=d=
='>>>x>
=->A>}>
=->b>
='>C?
=->h>
='>J>
='>T>
=0?N?a?
=0=@=D=H=`=d=h=|=
=0=@=D=T=X=\=t=x=
=0=<=\=d=p=
=0=4=8=L=\=l=p=t=
=0=4=L=P=T=X=\=`=d=x=
=0=G=r=
=0=Q=
=0>7>O>b>
=0>8>D>a>v>
=0>G>L>Q>f>p>z>
=0G0\0q0
=1?;?M?g?
=1=F=[=w=
=1=H=b=p=x=
=1>X>q>
=2=7=C=O=
=2=8=
=2=C=q=
=2=J=^=
=2=Q=[=t=
=2>g>
=2>I>N>S>j>q>
=2>M>
=2>T>[>|>
=2>Y>{>
=3?E?`?l?z?
=3=A=
=3=F=
=3=F=f=
=3=g=
=3=H=c=
=3>E>f>k>q>
=4=;=\=k=
=4=@=`=h=t=
=4=[=
=4=_=
=4=<=H=h=p=x=
=4=D=H=X=\=p=t=
=4=D=P=t=|=
=4=D=P=X=x=
=4=K=
=4=V=~=
=4>@>S>e>
=4>9>
=4>Q>
=4>R>j>q>
=5?c?
=5=j=q=
=5=t=
=5>F>
=5>l>v>
=5>o>
=5>R>W>\>a>u>
=6=;=
=6=;=A=`=
=6=;=A=i=
=6=@=X=d=v=
=6=E=
=6=K=f=q=
=7=F=
=7=h=z=
=7=j=
=7>P>&?I?
=7>u>
=8=@=H=P=`=h=
=8=@=H=P=X=h=
=8=@=H=P=X=h=p=
=8=\=
=8=A=h=q=
=8=B=_=i=
=8=H=d=}=
=8=H=T=\=
=8=H=T=t=|=
=8=l=
=8=T=t=
=8>\>a>
=8>p>
=8>x>y?
=9=@=G=h=
=9=F=
=9=P=
=9>n>
=A?[?z?
=A>X>c>c?
=AMDiu!
=B=I=a=
=b>q>
=C?Q?^?f?n?
=C=#?
=c=~=
=c=m=
=c=u=
=C=Z=
=c>#?
=C>J>c>t>
=C>M>f>
=c>u>
=C>U>v>{>
=D=T=`=
=D>[>`>e>z>
=D>_>
=D>a>l>v>
=D>P>c>
=F={=
=f=p=
=f=s=
=G=Q=j={=
=G=t=
=G>j>
=g>k>o>s>w>{>
=Genuu
=H?R?d?
=H>M>_>}>
=h>o>
=h>P?
=I>^>
=I>a>=?
=I>b>g>
=I>S>l>x>
=J=^=r=|=
=J=`=e=
=J>d>x>
=K=`=f=~=
=K>m>
=K>P>
=K>R>k>
=L=x=
=L=X=x=
=M>~>
=M>e>
=M>Z>
=N>f>
=o=t=
=O>T>
=o>v>
=P=p=z=
=p>8?
=P>n>
=P>Z>f>
=Q=g=
=q=v=
=R=<>]>h>
=R=d=k=
=r>w>
=s=8>
=S=x=
=S=y=
=t={=
=T=a=
=T>i>|>
=u=A>c>
=U=m=
=U=x=
=U>k>1?R?g?
=V=]=r=
=V=l=
=v=X>
=V>`>u>
=V>s>
=X>q>
=Z>a>v>
> ?(?,?0?8?<?@?D?l?t?x?|?
> ?4?Z?n?
> ?7?<?A?V?b?}?
> ?H?x?
> ?U?
> ?X?
> ?x?
> >$>(>,>0>4>8><>@>D>H>L>P>T>`>d>h>l>p>t>x>|>
> >$>(>,>0>4>8><>@>D>H>L>P>T>X>\>`>d>h>l>p>t>x>|>
> >$>(>,>0>4>8><>@>D>H>L>P>X>\>p>t>
> >$>(>,>T>\>`>d>l>p>t>x>
> >$>(>0>4>H>L>d>t>x>
> >$>(>0>H>X>\>l>p>
> >$><>@>X>\>t>
> >$>8><>@>X>\>t>
> >$>8><>T>d>h>l>
> >$>8>H>L>d>t>x>|>
> >$>L>T>X>\>d>h>l>p>
> >%>0>>>D>T>[>`>e>p>
> >&>,>Y>
> >(>@>P>`>d>t>x>|>
> >(>@>P>T>X>p>t>x>
> >(><>D>L>T>\>d>l>t>|>
> >(>0>8>@>H>T>t>|>
> >(>0>8>D>d>l>t>|>
> >(>0>8>D>d>p>x>
> >(>4>T>\>d>l>t>
> >(>4>T>`>
> >(>8><>L>P>`>d>t>x>
> >,>4>h>x>
> >,>L>T>`>
> >@>L>l>x>
> >`>N?
> >'>>>O>]>s>
> >0>@>H>X>\>l>p>
> >0>@>P>`>h>x>
> >0>4>8><>P>`>d>h>l>t>
> >0>4>8>P>T>X>\>`>d>h>l>p>t>x>|>
> >0>4>D>H>X>\>`>d>x>|>
> >0>7>G>N>^>e>u>|>
> >0>8>H>P>`>h>p>x>
> >4>8><>T>X>\>`>h>
> >5>I>L?Q?
> >8><>@>H>`>p>
> >8><>@>T>X>\>d>|>
> >D>L>T>\>d>l>t>|>
> >V>[>b>g>u>|>
> >Z>q>
>!?&?
>!?~?
>!?F?q?
>!?j?
>!?J?r?
>!?N?
>!?V?
>!?V?r?
>!>%>)>->1>5>9>=>A>W>r>
>!>:>
>!>@>
>!>`>v>
>!>+>B>H>Q>[>q>{>
>!><>
>!>8>A>X>a>x>
>!>e>
>!>H>
>!>H>Q>x>
>!>I>
>"?>?i?v?
>"?>?K?^?
>"?7?J?V?r?y?~?
>"?A?G?L?R?]?c?n?u?z?
>"?F?n?
>"?o?
>"?O?
>">(>.>]>
>">,>6>@>J>
>">;>
>">2>G>
>">7>L>
>">9>f>r>
>#?*?B?U?u?
>#?U?s?
>#>(>->J>P>U>l>
>#>*>1>?>k>q>
>#>>>H>a>h>
>#>5>V>[>a>
>$?)?
>$?0?C?U?v?{?
>$?L?x?
>$>(>,>0>4>H>L>\>`>p>t>x>
>$>(>,>0>4>H>L>P>T>\>t>x>
>$>(>,>0>D>H>L>`>p>
>$>(>,>0>D>H>L>P>X>p>t>
>$>(>,>D>H>L>T>l>|>
>$>(>,>D>H>L>T>X>l>p>
>$>(>,>D>T>X>h>x>
>$>(>8><>L>X>h>x>
>$>)>1>B>N>W>\>
>$>)>7>C>
>$>,>1>7>@>
>$>,>4>@>`>
>$>,>4><>D>L>T>\>d>l>t>|>
>$>,>4><>D>L>T>\>d>p>
>$>,>4><>D>L>T>\>h>
>$>,>4><>D>L>T>\>h>p>
>$>,>4><>D>L>X>|>
>$>,>4><>D>L>X>x>
>$>,>4><>D>P>p>x>
>$>,>4><>H>P>p>
>$>,>8>@>`>h>
>$>,>8>\>d>l>t>|>
>$>,>8>X>`>h>t>
>$>,>8>X>`>l>
>$>,>P>p>x>
>$>,>T>\>
>$>.>8>w>
>$>=>C>G>Q>r>
>$>0>P>\>|>
>$>0>P>X>`>h>p>|>
>$>0>P>X>`>h>p>x>
>$>0>P>X>`>h>x>
>$>4>@>`>h>t>
>$>4>8><>D>H>P>T>\>t>
>$>4>D>T>X>\>p>t>x>
>$>D>L>T>\>d>l>t>|>
>$>D>L>X>x>
>$>H>
>$>L>T>\>h>
>$>p>
>%?,?A?V?m?
>%?\?f?y?
>%?b?
>%>/>L>b>
>%>+>5>G>i>
>%><>S>j>
>%>b>
>%>M>R>_>e>k>
>&?,?
>&?4?
>&?c?
>&>,>
>&>{>
>&>+>1>P>
>&>2>K>Q>U>_>u>
>&>7>H>_>k>
>&>A>G>O>d>q>
>&>b>
>&>I>
>(?<?
>(?2?S?
>(?L?p?
>(?x?
>(>,>D>H>`>d>|>
>(>,>D>T>d>t>x>|>
>(>]>u>~>
>(>0><>\>d>l>t>|>
>(>0>8>@>H>T>t>|>
>(>1>X>a>x>
>(>3>A>T>f>
>(>4><>T>x>
>(>7>C>K>[>c>s>{>
>(>8><>@>H>L>T>X>l>p>t>
>(>8><>@>X>\>t>
>(>8><>L>\>`>x>|>
>(>8><>L>P>`>d>h>|>
>(>8><>L>P>`>d>t>x>
>(>8>D>L>l>
>(>H>T>t>|>
>(>L>\>h>
>(>L>T>\>d>l>t>|>
>)?/?B?Z?q?
>)?[?
>)?4?S?w?
>)?H?
>)>]>
>)>=>R>i>
>)>7>C>P>\>a>p>
>)>k>r>
>*???P?
>*?[?~?
>*?Z?d?q?
>*>=>]>
>*>C>S>\>e>
>*>E>Y>
>*>E>Y>p>u>z>
>*>m>
>,?\?
>,?3?L?
>,?o?
>,?W?|?
>,><>@>D>H>L>P>X>\>`>d>h>l>p>t>x>|>
>,><>@>P>T>l>|>
>,><>H>h>p>x>
>,>0>4>L>\>`>h>l>p>t>x>|>
>,>0>4>L>\>`>x>
>,>0>8>P>`>p>t>
>,>0>H>L>d>h>|>
>,>1>?>L>X>b>
>,>4><>D>L>T>\>h>
>,>4><>D>L>T>\>l>t>|>
>,>8>X>d>
>,>A>X>
>,>K?
>,>P>t>
>,>q>F?j?
>,>r>y>
>,>T>
>.?8?Q?m?
>.?G?
>.>>>Z>s>
>.>r>
>/?`?
>/?H?X?c?~?
>/?Z?
>/>=>T>
>/>4>K>R>f>l>
>/>D>^>
>/>Q>
>/>V>h>
>:?A?T?n?y?~?
>:?O?a?i?q?
>:?w?
>:>g>
>;?H?Q?V?
>;?H?s?y?
>'???X?b?i?
>??}?
>??k?
>-?`?
>-?=?F?O?z?
>?>[>r>
>?>|>
>?>e>o>
>?>F>_>t>
>?>t>
>?>t>-?
>?>W>p>
>-?4?
>'?a?
>-?b?
>'?d?
>'?Q?{?
>'?U?a?j?
>-?Y?c?v?
>-?Z?
>@?p?
>@>p>
>@>x>
>@>X>o>
>@s5f
>_?q?
>_?y?
>_>f>
>`?j?
>+?:?
>+?[?t?{?
>+?k?}?
>+?L?
>+?o?
>+?v?
>+>0>;>@>K>P>V>\>i>|>
>+>B>
>+>H>!?O?c?z?
>+>Z>
><?y?
><>A>
><>D>H>L>T>X>\>`>
><>D>L>T>\>d>l>t>|>
><>D>L>T>\>h>
><>D>L>T>`>
><>H>h>p>x>
><>M>k>
><>x>
><>y>
><>Z>v>
>=?D?]?w?~?
>=?J?
>=>B>N>Z>
>=>o>
>=>S>
>'>,>1>C>a>
>>?\?m?
>'>@>
>'>7>=>E>Z>f>
>->8>G>
>'>c>
>'>D>Y>}>
>->j>
>'>n>
>'>q>
>->S>
>->W>
>0?G?L?a?n?z?
>0?O?
>0?T?w?|?
>0>:>?>I>P>q>}>
>0>@>d>l>t>|>
>0>4>8>@>X>\>t>
>0>4>D>T>X>h>l>
>0>4>L>\>`>d>|>
>0>5>J>u>
>0>J>
>0>s>
>0>T>x>
>0>y>
>0b0l1"2
>0S0"1C1<2
>1?>?
>1?G?q?
>1?H?M?R?i?
>1>6>u>
>1>B>
>1>E>k>
>1>t>
>2?\?
>2?o?
>2>_>
>3?E?f?k?q?
>3?H?
>3?L?
>3?N?X?
>3?o?
>3?W?p?
>3>E>f>k>q>
>3>g>
>3>M>X>g>
>4?K?P?[?h?t?~?
>4?Q?
>4?X?
>4?z?
>4>@>`>h>x>
>4>@>d>
>4>@>d>l>t>|>
>4><>@>D>L>P>T>X>
>4><>D>L>T>\>d>l>x>
>4>8>@>X>h>l>p>
>4>8>P>T>l>p>
>4>9>?>E>K>x>
>4>A>
>4>C>|>
>4>D>H>L>P>d>h>x>|>
>4>D>T>d>h>l>
>4>K>
>4>S>X>b>v>
>4>w>
>5?B?
>5?b?
>5>:>
>5>i>s>
>5>I>V>d>
>5>P>
>6?S?t?
>6?v?
>6>;>A>`>
>6>4?9?z?
>6>B>N>\>a>n>s>
>6>e>l>
>6>G>^>t>
>6>H>g>o>
>6>K>
>7?^?
>7?O?Y?n?
>7>^>w>
>7>b>
>7>F>
>7>N>
>7>R>m>
>7>T>
>8?^?h?
>8?A?Z?x?
>8?c?
>8?f?
>8?P?Z?s?
>8>@>H>d>l>t>|>
>8>`>
>8>D>d>l>x>
>8>T>d>p>x>
>9>b>l>
>9>J>f>}>
>9>w>
>A?H?]?r?
>A>d>
>A>S>m>
>Anu 
>B>I>a>
>B>u>
>C?[?t?~?
>C?U?}?
>C?U?v?{?
>C>`>{>
>C>M>b>w>
>c>u>
>C>u>
>c>u>
>D?\?o?{?
>d?s?
>D?x?
>d>!?j?
>D>~>
>D>h>
>D>L>P>T>\>`>d>h>
>D>t>
>D>T>`>h>
>D>U>o>
>D>X>`>d?
>E?|?
>F?K?v?}?
>F?V?o?
>F?z?
>G>j>
>G>N>c>x>
>G>s>
>G>U>^>
>G>Y>
>H?|?
>H?q?
>H>h>
>H>N>
>H>R>d>
>J?]?
>j?{?
>J?b?i?
>J?n?
>J>c>
>K?}?
>K>e>u>
>L?y?
>l?y?
>L?Y?
>L>\>h>p>
>L>c>x>
>l>s>
>M>{>
>M>R>_>d>r>w>
>N?e?j?o?
>N>e>
>O>c>k>
>P>`>l>t>
>Q?[?m?
>Q?h?m?r?
>q>{>
>R?c?
>S>_>
>S>`>
>S>e>
>U?[?k?r?w?|?
>U>y>
>W>n?
>X?_?t?
>Y?^?
>y>c?u?
>Z?u?
0 == center_point_box_ || 1 == center_point_box_
0 == memory_size % kMinAllocationSize
0 0 06070>0?0
0 0$0(0,0004080<0@0D0H0L0P0T0h0l0|0
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0d0|0
0 0$0(0,0004080<0@0D0H0L0P0X0\0d0h0p0t0x0|0
0 0$0(0,0004080<0@0D0H0L0T0X0\0`0d0h0l0p0t0x0|0
0 0$0(0@0P0`0d0t0
0 0$0(0@0P0T0d0t0x0|0
0 0$0(0<0L0\0`0p0t0x0
0 0$0(0P0X0\0`0h0l0p0t0
0 0$0,0@0D0\0`0h0
0 0$0,0004080`0h0l0p0x0|0
0 0$0,000D0H0L0d0h0l0
0 0$0<0@0X0h0l0p0t0x0
0 0$0<0@0X0h0l0p0t0x0|0
0 0$04080<0T0d0h0l0p0
0 0$04080H0L0\0h0x0
0 0$040D0T0X0h0l0p0
0 0(0@0D0\0`0d0x0
0 0(0\0l0x0
0 0(00080@0H0P0X0`0h0p0x0
0 0(00080@0L0l0t0|0
0 0(040<0\0x0
0 0(080@0d0|0
0 0(080@0P0X0h0p0
0 0(080\0|0
0 0,0L0X0x0
0 0/070C0V0]0
0 0@0H0P0\0|0
0 0@0H0T0t0|0
0 000@0D0H0P0T0h0l0p0t0
0 000@0P0`0p0
0 0004080<0@0T0X0h0l0p0t0x0
0 0004080L0\0l0p0
0 0004080L0P0`0d0h0l0t0
0 00070<0?0
0 000T0\0d0l0t0|0
0 0'0S0X0d0p0
0 020L0
0 070R0W0l0
0 0d0
0 0D0H0P0T0X0`0d0h0l0
0 0e0
0 0s0
0 1)1<1d4
0 191L1
0 1n1u1
0 1X1
0!0)080:0
0!0)080;0
0!0:0
0!0]0q0
0!0`0
0!010A0K0U0a0q0
0!04090j0
0!060k0u0
0!0I0
0!1<1
0!1=1G1l1
0!1V1g1x1
0!3<3
0"0&0*0;0
0"0(02080B0H0R0X0b0h0r0x0
0"0+0>0D0T0Y0f0r0
0"010:0@0F0O0X0b0l0
0"020B0R0j0
0"060E0b0
0"0G0
0"0H0\0v0
0"0o0
0"161u1
0"1A1H1]1r1
0"1d1
0"1q1
0#0(0;0@0U0}0
0#0;0X0
0#0~0
0#0<0X0
0#000>0L0[0`0i0n0
0#070i1
0#0D0
0#0D0f0u0
0#0E0r0w0}0
0#0M0
0#0s0
0#0T0Z0
0#1,1>1S1e1x1
0#1}1
0#151V1[1a1
0#1B1I1^1s1
0#1c1
0#1c1u1
0#1m1
0#1v1
0$0(0,000X0`0d0h0p0t0x0|0
0$0(0,0D0H0`0p0t0
0$0(0004080@0D0H0P0T0\0`0h0l0t0x0|0
0$0(0-040A0I0N0T0X0]0
0$0(080<0L0P0T0X0l0p0t0
0$0(080H0X0\0l0p0
0$0,0<0D0
0$0,040@0`0h0t0|0
0$0,040<0D0L0d0l0x0
0$0,040<0D0L0T0\0d0l0t0|0
0$0,040<0D0L0T0\0d0p0
0$0,040<0D0L0T0\0h0
0$0,040<0D0L0T0d0l0t0|0
0$0,040<0D0L0X0|0
0$0,040<0D0L0X0x0
0$0,040<0D0P0p0x0
0$0,040<0H0h0p0x0
0$0,040<0H0l0t0|0
0$0,040<0H0P0p0
0$0,080@0t0|0
0$0,080\0d0l0t0|0
0$0,080X0`0h0p0x0
0$0,080X0d0
0$0,0L0h0x0
0$0,0L0T0\0|0
0$000T0\0
0$04080P0`0p0t0x0
0$070J0P0
0$080
0$080\0d0l0t0|0
0$0A0
0$0D0L0T0\0d0l0t0
0$0D0L0T0`0
0$0D0L0X0x0
0$0D0P0p0x0
0$0P0p0
0$1.1>1
0$1[1~1
0$1<1\1
0$1-1M1`1
0$1H1M1W1a1
0$1Q1~1
0$1U1
0%0/090C0s0
0%0_0f0~0
0%040S0X0b0q0{0
0%070F0q0x0
0%0B0k0u0
0%0F0K0Q0p0
0%0r0
0%1N1^1z1
0%1R1
0&0;0q1
0&0?0
0&0+010P0
0&0+010X0
0&0<0J0
0&080b0
0&0g0q0
0&0K1P11292Q2:3D3N3
0&1:1j1~1
0&131
0&171U1b1|1
0(0,0D0H0L0T0l0|0
0(0.0
0(000@0H0X0`0p0x0
0(00080@0T0\0d0p0x0
0(010X0a0
0(040?0X0b0
0(040<0p0
0(080@0P0\0l0|0
0(080<0T0d0h0
0(0D0L0T0`0
0(0L0T0|0
0(0W0
0(0x0
0(1`1
0(121B1a1
0(121G1\1
0(1H1P1X1h1
0)0/070L0X0q0w0{0
0)0@0\0{0
0)0>0J0[0
0)090{0
0)0B0S0j0
0)0H0g0
0)1/1B1Z1q1
0)1:1
0)1;1X1e1
0)1?1A2
0)1A1K1P1U1i1}1
0)1V1y1
0*0/0U0
0*0;0R0
0*040H0]0
0*070C0K0
0*0f0m0
0*0f0o0
0*141z1
0*2u2
0,0<0@0D0H0L0P0T0X0\0`0d0h0|0
0,0<0@0P0T0d0h0x0
0,000@0D0T0X0\0t0
0,0004080<0@0D0H0L0P0T0X0\0`0t0x0|0
0,00040H0L0P0T0\0t0
0,000H0L0d0h0
0,000H0L0d0t0x0|0
0,000H0X0\0`0x0|0
0,040<0D0L0T0\0d0l0t0|0
0,040<0H0h0t0
0,050
0,080X0d0
0,0h0o0
0,0H0X0h0
0,0P0|0
0,1^1
0,1<1H1P1p1
0,111n1
0,131L1
0,141<1D1`1
0,141T1p1x1
0,1e1H2k2Q3
0,1L1V1`1j1q1
0,1t1
0,1W1
0,1X1
0.0:0S0Y0]0g0}0
0.0=0
0.030F0K0`0
0.080Q0
0.0f <= ratio_value && ratio_value < 1.0f
0.1K1h1m1
0/04090P0m0
0/040M0R0i0
0/090^0r0y1
0/0A0S0f0
0/0P0d0m0q1
0/0V0}0
0/1<1X1_1
0:0?0K0X0d0l0q0x0}0
0:0A0V0k0
0:0c0m0
0:0D0P0k0u0
0:1`1
0:1D1]1
0:1R1\1u1
0;0]0
0;0B0Z0m0
0;0c0
0;0l0
0;0ud
0;1S1]1v1
0;1W1^1w1
0?1c1u1
0@0a0
0\0d0l0
0\1a1c5c6
0]0r0
0]0w0
0]1j1
0]1o1
0_^[]
0_0{0
0`0j0
0+000Z0
0+040F0p0
0+060B0N0[0g0m0|0
0+070A0_0z0
0+0F0M0^0r0
0+0m0
0+1^1
0+1H1T1q1
0+1W1y1
0<0<0A0
0<0d0|0
0<0D0L0h0
0<0D0L0T0\0d0l0t0|0
0<0D0L0T0\0d0l0x0
0<0H0h0p0|0
0<1H1X1y1
0=0=0
0=0T0
0=0T0i0
0=0U0^0
0=1D1Y1n1
0=1J1
0>0{0
0>0H0f0
0'0}0
0-0<0
00=0h0n0
000@0D0\0`0x0|0
000@0D0H0`0d0|0
000@0P0`0p0
000@0P0T0X0p0
00000
00000=0=0
0004080@0D0X0\0l0|0
00040L0\0`0p0t0x0|0
00040L0\0`0x0|0
00040L0P0h0x0|0
00060N0]0
00080\0|0
00080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
00080r2
000B0H0`0o0
000D0X0
000F0
000h0
000I0T0c0
000K0f0
000O0
000Q0g0
001}1
001h1
001X1
0-070P0a0x0
0-080G0
00H0]0
00P0\0|0
0'0s0|0
00S0n0
0'1,141=1
0'1?1I1b1|1
0'1^1u1
010;0P0e0
010?0
01050;0;0
01050;0<0A0
01090A0
'010F0[0o0
010m0
010t0
010T0
'010V0
0123456789-
0123456789-+Ee
0123456789ABCDEFabcdef-+Xx
0123456789ABCDEFabcdef-+XxPp
0123456789abcdefghijklmnopqrstuvwxyz
0'1A1
0'1L1
0-1r1
01T1^1q1
0'1Z1
020D0N0Z0
020F0
02191R1e1
021W1l1u1
024<4Q4f4
030|0
030=0V0g0
030B0G0
030D0
030E0f0k0q0
030m0
031_1
031A1N1V1^1
031E1f1k1q1
031S1
031U1
040;0T0
040;0T041
040@0d0l0t0|0
040<0D0L0T0\0d0l0t0|0
040>0W0
04080<0@0H0L0`0d0l0
04080P0T0X0\0`0d0h0l0p0t0x0|0
040904E4
040C0
040D0H0`0p0t0x0|0
040D0H0L0`0d0t0x0|0
040D0P0X0x0
040K0
040p0
041\1
041d1
041l1x1
041o1
041Q1
041s1x1
041T1
050^0n0
050E0
050n0
050S0e0
060;0A0`0
060;0A0L0r0
060@0q0
060]0
060B0Y0
060M0d0{0
061d1
070A0V0k0
070A0Z0k0
070A0Z0t0~0
070a1f1
071D1
071Y1u1
080@0H0l0
080>0V0e0
080D0N0`0z0
080N0
080x0
081f1z1
081I1l1v1
081p1
081S1
081X1
0888@8H8P8X8`8h8p8x8
090>0Z0_0t0y0
090v0
091C1g1
091H1L1P1T1X1\1`1d1h1l1p1t1x1|1
091o1
092|2
09az09AFaf
0A0n0
0A1Y1k1T2
0A1Y1p1}1
0B0s0
0B0u0
0B0X0~0
0b1w1
0B1y2
0C0p0
0C0U0v0{0
0C1|1
0C1U1j1{1
0C1U1v1{1
0D0b0k0
0D0L0T0\0d0l0t0|0
0D0p0
0D0u0z0
0D0x0
0D1h1
0d1p1
0D1P1c1u1
0D1v1
0D1Y1|1
0E1]1
0E1m1
0e2j2
0f0|0
0F0q0
0f1&2<6L7
0f1~1
0F172A2f2z2
0F1d2
0g0{0L1]136E6f6k6q6
0G0j0
0G0N0o0~0
0G3i3
0-g-o-p-
0h,[?
0h0@2b2u2/3>3
0h0~0
0H1e1q1
0H1p1
0h1r1
0H1X1
0I0l0
0J0e0
0j0u0
0J0y0
0J1_1r1~1
0jLh,
0k0x0
0K1_1h1
0K1P1y1
0k1r1
0k3"6
0L0_0o0
0L0r0
0L0V0i0y0
0L1}104A4u4
0L3r3
0M0f0
0M1d1i1t1
0M1u1
0M2h2r2
0N1#3t6
0N1U1(2
0N1y1
0O1h1
0P0`0
-0P0}0
0P1i1y1
0Qh`l<
0R0e0x0~0
0R1e1
0s0S1#2
0S1k1
0Sh\X<
0T0^0v0
0T0p0
0T1Y1V3
0U0_0q0
0U0l0q0~0
0U2_2x2&3>3H3a3r3
0V1[1c1k1r1w1|1
0w1\2;3
0W1p1
0X0|0
0Y0`0y0
0Y1q1x1
0z1v2
0Z2o2
1 == capability.nodes.size()
1 1$1(1,1@1D1H1L1`1d1h1
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
1 1$1(1,1014181<1@1D1H1L1P1X1p1t1x1
1 1$1(1,10181P1`1d1t1x1
1 1$1(1,101D1H1`1p1t1
1 1$1(1,14181<1@1T1X1h1x1|1
1 1$1(1@1D1H1\1`1p1
1 1$1(1<1@1D1H1L1P1d1h1l1p1x1
1 1$1(101D1T1X1h1l1|1
1 1$1(101H1X1\1l1p1t1|1
1 1$1(101H1X1h1l1
1 1$1,101D1H1L1d1h1l1
1 1$1,101D1H1X1\1l1p1
1 1$1<1L1P1`1d1t1
1 1$181<1@1D1L1d1h1
1 1$181<1L1\1`1x1
1 1%1,111>1C1P1U1
1 1%1:1b1
1 1(1,1@1D1\1`1x1|1
1 1(101@1d1l1t1|1
1 1(101<1\1d1l1x1
1 1(10181@1H1P1X1`1h1p1x1
1 1(10181@1H1T1t1|1
1 1(101D1L1T1`1
1 1(141T1\1h1
1 1,1L1T1\1h1
1 1/1K1U1j1
1 1:1a2
1 1@1H1P1`1|1
1 1[1{1
1 101<:@:p:t:
1 1014181P1`1d1|1
1 10141D1H1`1p1t1x1
1 10181H1P1`1h1p1
1 10181H1X1h1x1
1 14143X3
1 14181H1X1\1`1t1x1
1 181<1T1X1p1t1
1 181H1L1\1`1d1h1|1
1 181H1X1\1`1d1x1|1
1 1D1L1T1\1d1l1t1|1
1 1h1
1 2(2,20282<2@2D2l2t2x2|2
1 2)2H2O2Q2_2
1 2_2
1 2~2
1 2X2
1 2x2
1!1(3 4*4C4m4w4
1!1*1
1!1.1A1
1!1@1
1!1+1B1H1R1X1b1h1r1x1
1!1'1-13191?1K1#2
1!181
1!181A1h1q1
1!1B1Q1
1!1H1Q1h1q1
1!1Y1_1r1
1!252F2X2
1!2D2U2q2
1!2L2Y2
1!2m2r2~2
1"1&1*1.12161:1>1H1U1|1
1"1(1.1T1
1"1/1<1
1"111a1w1
1"161u1~1
1"1s1
1"2(22282D2
1"2F2\2
1"334
1#1(1.191]1v4
1#1,131G1U1a1m1s1
1#1~1
1#1=1H1W1
1#1C1^1h1{1
1#1G1
1#1v1{1
1#2*2L2
1#2]2
1#282q2
1#2K2
1#2V2
1$1(1,101@1T1X1h1|1
1$1(1,10181P1T1l1|1
1$1(1,101D1H1X1\1t1
1$1(1,14181L1P1T1l1p1t1
1$1(1,1D1H1L1`1p1t1
1$1(1@1D1\1`1x1
1$1(1@1D1\1l1|1
1$1(1@1P1`1d1t1x1|1
1$1(1@1P1T1d1h1x1
1$1,1@1H1P1\1|1
1$1,10181<1D1H1P1T1\1`1h1l1t1x1
1$1,141@1`1h1t1
1$1,141@1d1l1t1|1
1$1,141<1D1L1T1\1d1l1t1|1
1$1,141<1D1L1T1\1h1
1$1,141<1D1L1T1\1l1x1
1$1,141<1D1L1T1`1
1$1,141<1D1L1X1x1
1$1,141<1H1h1p1x1
1$1,141<1H1l1t1|1
1$1,141<1L1T1\1d1l1t1|1
1$1,141<1T1l1t1|1
1$1,181@1t1
1$1,181\1d1l1t1|1
1$1,1H1X1d1l1
1$1.1u1
1$1=1
1$101P1\1|1
1$101P1X1`1l1
1$101P1X1d1
1$1-121
1$141@1`1h1p1x1
1$1D1L1T1`1
1$1D1P1p1x1
1$1f1x1
1$1N1{1
1$1X1h1t1
1$282t2
1$2J2b2
1%1:1F1_1e1i1s114
1%1=1B1Q1p1
1%141
1%1E1
1%1F1K1Q1y1
1%1j1T2
1%2\2f2y2
1%2{2
1%2G2Y2y2
1%2k2
1&1;1
1&1+111X1
1&1+13181P1x1
1&121N1U1p1u1
1&1F1
1&2=2
1&232^2d2|2
1&282
1&2a2
1&2I2{2
1(1,1@1D1H1`1p1t1
1(1,1014181L1P1h1l1
1(1,14181<1D1H1L1P1x1
1(1,1D1H1`1d1|1
1(1/1W1\1h1u1
1(1@1R1]1c1m1}1
1(101@1D1T1p1
1(101@1H1X1`1p1x1
1(101<1\1d1p1
1(10181T1d1p1x1
1(101d1t1
1(101T1\1d1t1|1
1(1-1;1@1N1S1]1b1)2@2E2J2a2h2
1(131N1g1l1
1(141T1`1
1(171
1(181\1d1l1t1|1
1(181<1@1D1H1P1h1x1|1
1(181<1L1P1`1d1h1
1(181<1T1d1h1x1|1
1(181D1d1p1x1
1(181D1L1l1
1(181H1X1`1p1t1
1(1b1i1
1(1E1J1O1T1r1|1
1(1F1q1
1(1H1
1(1H1P1\1|1
1(1H1P1X1`1h1t1
1(1H1P1X1d1
1(2b2
1(2w2
1)1>1S1
1)131=1B1P1_1n1z1
1)161C1T1i1s1}1
1)1F1s1
1)1O1`1|1
1)1O1c1v1
1)2/2B2Z2q2
1)2@2G2`2l2J3Q3j3
1)242C2a2
1)2a2
1)2B2U2
1)2k2
1)2Q2
1*1@1T1j1
1*151D1R1e1
1*171A1T1o1
1*1A1F1K1s1z1
1*1C1h1o1
1*1I1V1k1
1*1R1
1*2/262;2H2U2b2
1*2:2F2\2q2
1*2]2
1*242w2
1*2B2G2T2_2
1*2R2W2]2f2l2
1,1<1@1P1T1X1p1t1x1
1,101@1D1T1X1h1l1
1,10141<1T1d1h1x1|1
1,101H1L1P1X1\1p1t1x1|1
1,11161M1j1
1,161O1
1,181@1`1|1
1,1a1}1
1,2>2[2
1,232L2
1,2M2b2
1,2n5~;
1,3f3
1.1;1G1S1`1m1z1
1.1B1
1.1s1
1.282u2
1.2I2|2
1.4.0
1.7.1
1.7.210427-2300.1.dml-1.6.8814570
1/1?1G1O1
1/1@1\1s1
1/1^1
1/111
1/191E1`1j1v1
1/1F1
1/1U1u1
1/242t2
1/2G2Q2j2
1:1A1V1k1
1:1L1Y2
1:1U1
1:2]2
1:284=4D6I6
1:2G2h2
1;1N1
1;1v1
1;2c3y3\4h4
1;Shu
1?1`1
1?1R1
1?2_2
1@1p1
1@2x2
1@3^3q3
1[1`1k1p1{1
1[1h1
1[1t1
1[233s335
1]2y2
1_1g1
1_2f2
1`1p1
1`2~2
1|1a2s2
1~1L2V2k2
1+181D1
1+1B1j1
1+1L1
1+1Q1h1x1
1+232
1+282c2i2
1+2C2w2
1+2F2
1+2S2]2o2
1+3Z3^3b3f3j3n3r3v3z3~3
1<1D1H1L1T1X1\1`1
1<1h1
1<1H1h1p1|1
1<1J1z1
1=1E1
1=1p1
1=1Q1U1_1t1y1
1=1w1~1
1=2j2
1=2x2V3
1>1d1|1
1>1R1p1
1>1s1
1>2~2
1>2q2
101@1d1l1t1|1
101@1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
101E1\1
101X1
102`2
102<2p2
102e2c3
10X0a0
1'1,191L1
11;1{1
1'1{1
111A1G1O1d1p1
11282Q2[2s2z2
112X2a2x2
1-131K1Z1
113G3
113V3{3z4
1-1B1Y1t1
1'1n1{1
121>1Z1a1f1m1
12181
121u1
121X1l1
121Y1"2R2v2
122F2
122H2|2
1-2C2P2f2
1-2h2
1-2k2
131?1
131E1f1k1q1
131J1a1x1
131T1
131Z1
132:2S2d2
132=2p2
141@1`1l1
141@1H1
141<1@1D1L1P1T1X1
141<1D1L1T1\1d1l1t1|1
141<1D1L1T1\1d1l1x1
141<1D1L1X1`1
14181<1P1T1d1h1x1
141D1H1`1p1t1
141D1T1X1\1`1t1x1
141g1
141T1\1d1l1t1|1
142:2G2R2k2v2
142@2S2e2
142d2
142L2T2\2d2l2
142n2
142Q2
142X2
151]1
151f1
151S1e1
151t1
151w1
152:273n3s3
152F2
161G1O1\1g1r1
161I1
162p2w2
173w3
181@1H1P1\1|1
181[1
181\1
181d1
181H1T1\1
181p1
181T1d1p1x1
181X1`1h1x1
182p2
191C1\1
191C1X1m1
191E1V1`3}3
191W1a1z1
192C2U2r2
1a1n1
1A2_2
1a2f2'4,4
1A2K2`2u2
1B1U1d1
1B1Y1m1
1B2O2
1B2t2
1b3Z4d4}4
1c1m1
1C1s1
1c1u1
1C2U2k2p2u2z2
1D input tensor
1D output tensor
1-D tensor of 2 elements: [crop_height, crop_width]. All cropped image patches are resized to this size. Both crop_height and crop_width need to be positive.
1-D tensor of axes that `starts` and `ends` apply to.
1-D tensor of ending indices (exclusive) of corresponding axis in axes
1-D tensor of floats
1-D tensor of shape (num_rois,) with each element denoting the index of the corresponding image in the batch.
1-D tensor of starting indices of corresponding axis in `axes`
1D1a1
1D1L1P1T1\1`1d1h1
1D1L1T1\1d1p1x1
1D1r1|1
1D1x1
1D2[2`2e2
1D2N2c2x2
1d2p2
1D2T2d2p2
1E2|2
1F1P1q1
1F2^8g8
1F2a2k2
1G1j1t1~1
1g1l1
1G1L1]1c1
1G1m1#2H2
1G2{2
1G5L5~5
1H1t1
1h2q2
1I1S1e1
1I1t1
1I2d2
1i2I3n4
1i3s3
1J1P1]1
1k1[2
1L1E3
1L1p1
1l2&3
1L2e3
1L2V2h2
1M1a1
1M2[2u2
1M2e2o2
1N1}1(282D2O2\2{2
1N2_4T7u7
1N2d2
1N2U2h2
1O1}1
1O1w1
1O2\2
1o2y2
1P1w1
1P1Z1h1|1
1P2^2
1P2c233
1P2p2
1P3o3
1Q1Y2
1Q2^2
1q3q5
1R1\1~143
1r2{2
1R2i2o2t2z2
1S1Z1r1
1S2n2
1S3\3n3
1t1:2
1T1Y1
1U2k2
1U2z2
1u3z3
1V1e1
1v2{2
1V2n2x2
1V2q2
1W2{2
1X1b1
1X1x1
1Y2f2
1Y2y2
1Z1d1
2 2$2(2,20242<2T2d2h2x2|2
2 2$2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
2 2$2(2,2024282L2\2`2x2
2 2$2(2,242L2P2h2l2p2x2
2 2$2(2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2
2 2$2(2<2@2H2`2p2t2
2 2$2(2024282<2d2l2p2t2|2
2 2$2(2d:l:t:|:
2 2$2,2D2H2L2T2l2p2
2 2$2<2L2P2T2h2l2p2
2 2$282<2T2d2h2l2t2
2 2$282H2L2\2`2x2
2 2%2-222<2\2a2f2u2
2 2(2
2 2(2,20242\2d2h2l2t2x2|2
2 2(202@2d2l2t2|2
2 2(202@2P2X2`2p2x2
2 2(20282@2H2P2
2 2(242T2\2d2l2t2
2 2(242T2\2d2p2
2 2(242T2`2
2 2)2/2U2
2 2*2
2 2*242>2H2R2\2c2
2 2*2K2[2a2i2~2
2 2,2L2T2\2d2l2t2|2
2 2,2L2T2\2d2p2
2 2,2L2T2`2
2 2@2H2T2t2
2 2@2H2T2t2|2
2 2@2L2l2t2
2 2<2L2X2|2
2 202@2D2\2`2x2|2
2 202@2P2`2h2x2|2
2 2024282<2P2`2d2t2
2 2024282P2`2d2h2l2t2
2 20242L2\2l2p2t2x2
2 20242L2P2h2l2p2
2 20282H2P2`2h2x2
2 202T2\2d2l2t2|2
2 282H2L2\2`2d2l2
2 282Q2v2
2 2W2
2 3*3<3V3
2 3;3
2 3\3
2 303<3\3h3
2 3P3
2 3p3
2 3S3
2 3s3
2 3T3
2 3X3
2!2&2?2D2[2}2
2!2@2
2!212<2P2b2k2r2
2!2-292?2Z2o2
2!262i2s2
2!262K2b2
2!2H2
2!2t2~3
2!3)3:3D3a3
2!3>3e3s3
2!3W3
2"2&2*2.22262:2>2B2F2J2\2
2"2(22282C2H2R2c2h2r2
2"2*2/272<2G2L2S2X2f2k2w2
2"2,262@2E2U2Z2o2u2
2"2'2>2[2
2"2'242@2K2W2
2"2'24292
2"2-2K2u2
2"252U2
2"282S2X2m2
2"2d2
2"2J2
2"363i3}3
2"3R3x3
2#2*21282@2H2P2\2e2j2p2z2
2#2:2W2}2
2#2:2x2}2
2#242K2w2
2#2F2]2
2#3)3A3P3
2#3;3E3^3x3
2#353]3b3h3n3
2#3c3
2#3o3
2#3z3
2$2(2,20282P2T2l2|2
2$2(2,242L2P2h2x2|2
2$2(2@2D2\2`2x2|2
2$2(2@2D2H2L2P2d2t2x2|2
2$2(2@2P2T2X2\2`2d2h2l2p2t2x2|2
2$2(2@2P2T2X2p2t2
2$2)232B2G2M2`2s2
2$2,242@2d2l2t2|2
2$2,242@2H2|2
2$2,242<2D2L2T2\2d2l2|2
2$2,242<2D2L2T2\2d2l2t2|2
2$2,242<2D2L2T2\2d2p2
2$2,242<2D2L2T2\2h2
2$2,242<2D2L2T2d2l2
2$2,242<2D2L2X2|2
2$2,242<2D2L2X2x2
2$2,242<2D2P2p2|2
2$2,242<2D2P2p2x2
2$2,242<2H2h2p2x2
2$2,242<2H2h2x2
2$2,242<2H2l2t2|2
2$2,242<2H2P2p2
2$2,282X2`2h2p2x2
2$2.2:2U2_2k2
2$2.282O2
2$2;2
2$202K2P2e2
2$202P2X2`2h2p2x2
2$202P2X2`2l2
2$202T2\2
2$202T2\2d2l2t2|2
2$24282P2`2d2h2|2
2$242D2L2`2h2p2|2
2$272W2
2$282<2@2X2\2t2x2|2
2$2D2L2T2`2h2
2$2F2
2$2G2
2$2X2~2
2$2y2
2$303C3U3v3{3
2$3a3f3n3w3
2$3Q3v3
2$3T3
2%2+2X2
2%2A2X2
2%2F2K2Q2p2
2%2F2K2Q2y2
2%3M3
2%3X3
2%4w4
2&2;2P2
2&2+212P2
2&2+22272>2C2Y2^2k2q2}2
2&2+272C2H2S2X2d2i2v2{2
2&242W2
2&282
2&292I2
2&2E2d2
2&2K2U2g2
2&353
2&3J3
2&3M3
2(2,2<2@2D2L2P2d2t2x2|2
2(2,2<2L2P2`2d2h2
2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
2(2,202H2L2d2h2
2(2,2D2T2X2\2`2h2
2(2024282@2D2H2L2t2|2
2(20282@2H2P2X2h2
2(20282H2l2t2|2
2(212X2a2
2(222G2\2p2
2(282<2@2X2\2`2d2h2l2p2t2x2|2
2(282<2@2X2\2`2h2
2(282<2@2X2h2x2
2(282H2L2P2h2x2
2(282H2X2\2`2x2
2(282H2X2h2x2
2(292P2
2(2H2P2\2|2
2(2H2P2\2d2
2(2H2P2X2`2l2
2(2H2T2t2
2(2L2T2\2d2l2t2|2
2(2P2<3
2(2r2|2
2(3.3F3U3
2(3`3
2(3=3`3q3
2(323K3q3{3
2(3e3
2(3h3
2)2?2j2s2x2
2)2<2B2
2)22282>2G2P2Z2d2~2
2)2e2l2
2)2V2
2)3/3B3Q3
2)3}3
2)3S3|3
2)3Y3
2*2?2R3h3v3
2*2[2b2
2*262H2
2*2G2P2P2`2
2*3h3/4
2*3j3
2*3z3
2,2<2L2P2`2d2h2l2
2,2=2V2
2,20242H2X2\2`2x2
2,202H2L2P2X2\2d2h2|2
2,242P2p2x2
2,272C2b2
2,282X2`2h2p2x2
2,282X2d2
2,2H2X2d2l2
2,3^3
2,3Y3
2.252N2_2
2.2D2
2.333i3
2.353M3`3
2.3a3
2.3B6e6
2.3M3l3
2.3q3
2.3Y3
2.4v4
2/252M2\2
2/272$343A3b3
2/2A2I2Q2
2/2c2
2/2R2w2
2/2W2
2/3E3[3
2/3J3
2/3J3[3
2/3R3
2:2A2f2}2
2:2A2V2k2|2
2:2c2
2:2D2]2z2
2:2W2n2s2
2:2X2a2
2:3F3R3X3d3
2:3v3
2;2@2F2Q2u2
2;2d2n2
2;2k2{2
2;3V3g3
2?2I2g2
2@2d2n2v2
2@2h2
2@2H2P2X2`2l2t2
2@2J2_2t2
2@2L2w2
2@2q2
2@2x2
2@3V3|3
2[3t3{3
2\2p2
2]2}2
2^3l3y3
2`2~2`
2}2L3
2+2B2
2+383c3i3
2+3f3
2+3h3
2+3N3X3b3l3q3
2<2}2
2<2D2L2T2\2d2l2t2|2
2<2H2h2p2|2
2<2i2
2<3w3
2<4m4}4
2=2j2
2=2p2
2=3n3
2>2~2
2>2q2
202@2D2T2d2h2l2
202|3>4H4]4r4
202<2\2d2l2t2|2
2024282<2@2H2L2P2T2\2t2x2|2
20242D2H2L2d2h2
20242D2H2L2T2l2|2
20242L2P2h2l2p2x2|2
20282D2d2l2x2
202G2^2u2
202T2\2d2l2t2|2
202v2
204i5
20i0p0
'212&3C3X3s3
212B2
212D2d2
212k2r2
212m2
21383]3
213E3N3
214Q4?5z5
2'2,2:2?2
2'2,252:2C2H2O2T2b2g2v2|2
2-2:2F2P2Z2p2
2'2<2S2m2x2
2'2-2@2F2Z2`2r2}2
222_2
222<2U2f2
2-22282>2C2N2Z2
22292R2g2
222l2
223<3N3k3
223=3T3k3
223S3a3
223S3X3^3i3
223W3
223Z3
2-2a2
2-2A2F2[2
2-2B2Y2
2'2C2Z2.373a3
2-2F2]2v2
2'2n2{2
2-2R2
2-2s2
2'3>3^3l3
232\4
232E2]2b2q2
232E2X2
232G2j2t2~2
232U2
233[3v3
233c4
233E3[3`3e3j3o3
233E3]3b3q3
233E3f3k3q3
233e3m4
233G3h3
233G3m3
233y3
2'3E3
2'3j3
242`2
242<2H2h2p2|2
24282@2X2h2l2
242B2
242D2H2`2p2
242D2H2L2d2h2p2t2x2
242D2H2X2\2t2
242Q2
243d3
243l3
243Q3
243q3
252:2a2
252:5A5H5[5d5m5
253r3w3
262;2A2i2
262;2I2X2
262[2
262F2
262X2
263=3R3g3~3
263K3P3h3w3
263M3R3W3n3
272~2
272B2L2U2
272K2V2b2
282@2H2P2\2|2
282`2
282c2
282d2
282h2
282L2
282p2
282S2~2
282S2n2
283A3Z3
283u3
283x3
292\2f2p2z2
292_2i2
292C2\2m2
293Q3[3t3
293R3e3
293X3w3
2A3~3
2A3X3b5f5j5n5r5v5z5~5
2a4k4
2b3%4D4o4t4
2b385+6
2B3R3,4
2B3Z3m3y3
2C2e2
2C2S2
2C2s2
2c2u2
2C3h3
2C3J3
2C3o3
2C3p3
2C3u3
2C3U3v3{3
2D matrix with shape (K,N)
2D matrix with shape (M,N)
2D2d2
2D2f2
2D2n2
2D2N2a2
2D2U2\4
2D3^3
2D3K3`3u3
2D3p3
2d3p3
2D3Q3|3
2d3u3#4;4P4
2d4<5A5
2e2l2
2e2o2u2
2E3J3
2f2}2
2f2~8
2f2w2
2F3R3
2G2T2k2
2h2v2
2H2Y2s2
2H3|3
2H3h3p3
2i2~2
2I2t2
2I3`3m3x3
2I3V3
2J2y2~2
2J3e3o3H5R5
2J3m3
2J3x3
2K2i2r2
2K2R2g2|2
2K2v2
2K3`3f3~3
2k3R4g4
2l2}2
2L2T2\2t2
2L2x2
2N3S3
2O2T2
2O3`3
2O3w3
2o3y3
2p2@3
2P2}2
2p354S4Z4t4
2P3h3r3
2Q2^2l2x2
2R2Y2q2
2r3w3
2r4h5r5
2S2e2
2S2w2
2S2y2
2S2Z2s2
2S3n3
2s5.:
2T2q2
2T3m3
2T3o3
2T5L6V6o6
2U2d2{2
2U2i2r2
2U3b3f3j3n3r3v3z3~3
2U4p4z4.5G5
2U6A768
2V2~2
2V2x2
2V3e3
2W2}2
2X2a2s2
2X3\3`3d3
2X3u3
2Y3n3
2Z2{2
2Z3);Y;
2Z3l3
2Z4d4}4+5C5M5f5w5
3 3$3(3,30343<3T3d3h3
3 3$3(3,3034383<3@3D3H3L3P3T3X3\3`3d3h3l3p3t3x3|3
3 3$3(3,303D3H3X3\3`3x3
3 3$3(3<3@3P3T3X3\3`3h3l3t3
3 3$3(3034383<3D3\3l3p3t3
3 3$3(3P3X3\3`3h3l3p3t3
3 3$3,30383<3D3H3P3T3\3t3x3
3 3$3<3@3X3\3t3x3
3 3$3<3L3\3`3x3|3
3 3$3<3L3P3h3x3
3 3$3<3L3P3T3X3l3p3t3
3 3$34383<3@3T3X3\3t3
3 3$34383H3L3\3l3p3
3 3$383<3T3X3p3
3 3$383H3L3\3l3p3t3x3|3
3 3(3,34383@3X3h3l3|3
3 3(303<3\3d3l3x3
3 3(303<3d3p3
3 3(30383@3P3t3|3
3 3(30383D3d3l3t3|3
3 3(30383D3d3p3
3 3(343T3`3
3 3,3|3
3 3,3L3T3\3d3l3x3
3 3,3L3X3x3
3 3,3T3`3
3 3@3H3h3p3
3 3@3P3t3|3
3 3>3K3]3h3
3 303@3D3H3`3p3
3 303@3P3`3p3
3 3034383L3P3T3h3l3p3t3
3 30343D3H3X3\3l3p3
3 30343L3P3T3h3x3
3 3-393
3 34383<3@3T3X3p3
3 353L3
3 363
3 383<3@3H3`3p3t3
3 383<3T3X3p3t3
3 393
3 3h3p3~3
3 3H3P3T3X3`3d3h3l3
3 3O3
3 3s3
3 4 7
3 4*4K4
3 4;4p4
3 4}4
3 4J4
3 4l4
3 4p4
3 4q5 6
3 4X4
3 50585H5P5`5h5p5x5
3!3&3.33383G3Z3
3!3@3
3!31363C3O3\3i3u3{3
3!3d3
3!3H3Q3h3q3
3!3H3Q3x3
3!3K3P3_3n3
3!3L3R3j3y3
3!4&4
3!4&4s4
3!4(4/464V4
3!4`4
3!454v4
3!4N4{4
3!4Q4
3"3&3*3.32363:3
3"3&3*3.32363:3D3
3"3(32383B3H3R3X3b3h3r3x3
3"3*323=3I3U3j3
3"3/3
3"3:3M3m3
3"3'3/343
3"393
3"3P3f3
3"3W3
3"3W4j4
3"4a4
3"4m4
3"4p4
3"4u4
3"4Z4
3#3.3;3G3Q3[3e3o3{3
3#3/3J3T3`3{3
3#3`3j3
3#3<3
3#30393>3}3
3#343H3
3#353z3
3#393
3#3Q3
3#3V3o3
3#4/4^4m4
3#4i4
3#4O4X4
3#4T4
3$3(3,3@3P3T3l3|3
3$3(3,3034383<3@3D3H3L3P3T3X3l3|3
3$3(3,303D3T3X3h3l3p3t3x3
3$3(3,303X3`3d3h3p3t3x3|3
3$3(3@3D3\3`3d3h3|3
3$3(3@3D3H3\3`3p3
3$3(383<3@3H3`3p3
3$3(383<3L3P3T3X3\3`3d3h3l3p3t3|3
3$3(383H3L3\3l3p3
3$3,343@3`3h3t3
3$3,343@3`3l3
3$3,343<3D3L3T3\3d3l3t3
3$3,343<3D3L3T3\3d3l3t3|3
3$3,343<3D3L3T3\3d3l3x3
3$3,343<3D3L3T3\3d3p3
3$3,343<3D3L3T3`3
3$3,343<3D3T3\3d3l3|3
3$3,343<3H3h3p3x3
3$3,343<3H3l3t3|3
3$3,383X3`3l3
3$3,383X3d3
3$3,3D3P3X3x3
3$3,3L3h3x3
3$3.3C3X3v3
3$3.3G3v3
3$3@3]3
3$3<3L3P3`3d3t3x3|3
3$30383l3t3|3
3$30383X3t3
3$303P3\3|3
3$303P3X3`3h3p3
3$303P3X3d3
3$303P3X3h3
3$343D3H3L3T3l3p3
3$383
3$3D3L3T3\3d3l3t3
3$3D3L3X3x3
3$3F3j3r3w3
3$3H3t3
3$3P3t3
3$3S3
3$3U3
3$4,40444<4@4D4H4p4x4|4
3$4D4
3$4h4
3$4H4
3$4N4~4
3$4O4
3$4X4
3%3:3O3
3%3;3j3p3
3%313=3B3O3T3a3f3s3x3
3%31383d3i3u3
3%3E3
3%3F3K3Q3y3
3%4>4Q4
3%494B4h4|4
3%4c4~4
3%4D4
3%4e4
3%4k4
3&3|3
3&383s3
3&3C3k3
3&3D3
3&3J3
3&3X3b3{3
3&4:4j4~4
3&4;4_4
3&404E4Z4q4
3&4-4B4W4s4
3&454
3&4R4
3(3,3<3@3P3`3p3t3x3|3
3(3,30343<3@3T3d3t3x3|3
3(3,3D3H3L3T3h3l3|3
3(3,3D3T3X3\3`3h3l3
3(3;3\3o3
3(303@3H3P3X3`3h3p3x3
3(30383@3H3T3\3|3
3(383<3L3P3T3l3p3
3(393
3(3A3h3s3
3(3E3J3p3
3(3L3T3\3d3l3t3|3
3(3V3p3
3(4.4F4U4
3(4`4
3(424G4\4w4
3(4-4
3(4i4
3(4k4
3(4U4
3)3.363;3
3)3/3M3
3)3e3l3
3)3I3
3)3s3
3)4=4S4a4
3)6A6I6
3*3/343K3h3
3*313H3M3t3
3*3c3|3
3*3I3^3s3
3*4h4
3*4J4
3*4k4
3*4m4`566
3,3<3@3D3H3\3`3x3
3,3<3@3P3T3d3h3
3,303H3L3d3h3
3,303H3L3P3T3h3l3
3,303H3X3\3`3x3|3
3,31363M3j3
3,333C3M3]3g3w3
3,343@3H3`3h3
3,343<3D3L3
3,343<3D3L3T3\3d3l3x3
3,383X3`3h3p3x3
3,3H3X3d3l3
3,4:4s4
3,4@4f4z4
3,4@4T4j4
3,4[4
3,464H4b4n4
3,4B4l4
3,4r4X6f6
3,4T4
3.3[3o3
3.353N3r3y3
3.3C3Z3u3
3.3F3z3
3.3T3
3.405f5
3.4Q4
3.4S4x4
3.6F6
3/3:3W3a3k3s3{3
3/3\3}3
3/343L3V3b3m3
3/363]3
3/393^3"5Y5
3/3D3Q3p3w3
3-:';L;i;
3:4D4Y4n4
3;3c3
3;4<607i:
3;4f4
3?3[3
3?3n3
3?3Q3
3?3R3a3f3
3?3V3g3
3?4y4
3?4Z4
3@3|3
3@3h3|3
3@3H3L3P3X3\3`3d3
3@3I3
3@3k3
3@3m3
3@4T4z4
3[3y3
3[4o4
3\3}3
3\3d3
3\3p3
3\3v3
3]314
3`3j3
3`3m3
3`3v3
3`4e4
3{436
3{5#6Z6
3|7h8f9+;
3+393k3
3+3D3H4h4u4
3+3L3V3o3{3
3+3V3\3t3
3+4~4
3+4G4N4g4x4
3+4W4w4
3+555J5_5v5
3<3d3
3<3D3L3X3|3
3<3D3P3p3x3
3<3D3t3|3
3<3F3m3
3<3p3
3<3U3h3
3<3V3e3
3<3x3
3<4;5-6
3<4g4
3<4j4
3<4Z4x4
3=3=4O4f4
3=3e3
3=3l3
3=3s3
3=4J4f4
3>3d3n3
3>3f3
3>3H3R3f3k3
3>3i3
3>3r3
3>4H4e4
3>4m4
303>3V3b3y3
3034383@3X3\3p3
3034383<3D3H3P3h3x3|3
3034383<3D3L3P3T3h3l3
30343D3H3`3d3|3
30363:3D3e3u3{3
303a3
303m3
304_4
304`4
304K4
306v6
30A0K0
30E0f0k0q0
30U0m0
30v0}0
30v0c4
313~3{405
31363
313q3I4a4k4
313s3
313W3h3
314]4
31484q4x4
314t4
316p6
323]3
323O3v3
323p3
323p3u3
323S3}3
323z3
324J4
32-bit hash value.
3'3,3
3'3<3Q3
3'323;3D3V3[3v3
3'32383B3R3W3f3k3z3
333=3I3e3l3
333=3R3g3~3
333=3V3g3
3-333
3'333=3G3Q3m3
334:4R4e4
334E4f4k4q4
334s4
334U4
3-383G3
3-3C3Y3o3
3-3D3[3r3
3'3H3
3-3I3`3
3'3L3
3-4]4}4
3'4<4O4[4t4{4
3'414C4a4
343@3`3h3p3x3
343@3`3l3
343^3
343<3D3L3T3\3d3l3t3|3
343<3D3L3T3\3d3p3
343<3D3L3T3\3h3
343<3D3M4
343>3W3
343D3P3X3x3
343I3
343K364
343p3
343P3>4
344<4D4L4T4`4
344Q4
345n5x5
3-4C4
3'4J4r4
353D3^3
353X3s3
354e4
354g4
354y4
3-575I5g5
363;3A3`3
363M3f3}3
364;4 :4:A:H:
364E4v425
364G4
364j4
373h3|3
374]4
375O5V5k5
383@3H3P3X3`3h3t3
383@3L3l3x3
383p3
383P3W3p3
383T3\3d3l3t3
383V3t3
383X3`3l3
384\4d4
384x4
393c3
393V3
394C4i4
394F4b4
394n4
394Q4X4q4
3A3K3d3u3
3a3u3
3a3z3
3A4a4
3A4G4T4
3b3}3
3B3v3
3B3w3
3B4Z4d4}4C6M6f6
3C3~3
3c3s4
3c3u3
3C3U3v3{3
3C4}4
3C4H4q4
3C4H4T4Y4`4e4l4q4x4}4
3D3I3W3f3
3d3k3
3d3l3x3
3D4P4c4u4
3E4Y4a4
3F3.4w4
3f3G5L5
3F3n3
3F3P3Z3d3n3x3
3F3q3
3F3q3\4}4
3F4S4~4
3f5p5
3G3|3
3G4_4
3g4l4
3G4Q4f4{4
3H3}3
3h3<4W4m4
3H3i3
3H3P3X3h3x3
3H3R3
3h4o4
3H4p4
3H4x4
3H5f5y5
3I4S4h4}4
3J4e4o4
3J4O4
3K4\4x4
3L3V3i3y3
3l3y3
3L4b4o4
3L4i4
3l4q4
3M3T3l3
3M3V3
3m3z3
3M4_4v4
3M4X4e4q4
3N3b3x3
3n4v4$575
3n4x4
3o3y3
3o4v4
3o7t7
3P4h4r4
3Q4[4
3q4F6a6k6
3Q4s4
3Q4X4q4
3q6,717e7j7D8
3r4|4
3R4k4
3S3\3
3S3b3
3S3e3
3S3j3
3S4g4}4
3S4X4
3T3q3
3t4{4
3T4n4_5'7~7
3U3~3
3U3a3p3
3U3k3
3U3p3
3V3~3
3V3g3
3VQSQ
3W3}3
3w4t5
3WVVQ
3y3;5
3y3^4
3Y4r4
3Z4a4v4
4 4$4(4,4044484<4@4D4H4L4P4d4h4l4p4t4
4 4$4(4,4044484<4@4D4H4L4P4T4X4\4`4d4h4l4p4t4x4|4
4 4$4(4,4044484<4@4N4g4
4 4$4(4,40444H4X4\4l4p4
4 4$4(4@4D4\4l4p4
4 4$4(4<4@4X4\4t4x4
4 4$4,4D4H4`4p4t4
4 4$4<4L4P4`4p4t4
4 4$4<4L4P4T4l4|4
4 4$44484H4L4P4d4h4l4t4
4 4$44484H4L4P4T4h4l4
4 4$484<4@4X4h4l4p4
4 4$484<4L4\4`4d4|4
4 4$484<4T4X4p4t4x4
4 4$484H4L4d4t4
4 4$545
4 4(404<4\4d4l4t4|4
4 4(40484@4L4l4x4
4 4(40484H4l4t4|4
4 4(444<4\4x4
4 4(444T4\4h4
4 4(444T4`4
4 4(484@4P4X4h4p4
4 4(484\4d4l4t4|4
4 4,4L4T4\4d4l4x4
4 4@4H4P4\4|4
4 4@4L4l4t4|4
4 4@4L4t4
4 4<4e4o4
4 4<4L4X4`4
4 404@4D4H4L4T4X4\4`4d4h4l4p4t4x4|4
4 404<4D4d4
4 4044484<4D4\4`4d4h4p4t4
4 4044484P4T4l4|4
4 40444D4P4`4p4
4 404T4\4d4l4t4|4
4 434E4f4k4q4
4 44484<4@4D4X4\4`4d4x4
4 44484<4@4H4L4T4X4l4p4t4
4 44484<4T4d4h4
4 44484H4L4d4h4l4
4 44484H4L4P4X4p4t4
4 4'494N4]4d4k4r4
4 484<4T4d4h4l4p4t4
4 4b4
4 4D4
4 4D4L4T4\4d4l4t4|4
4 4P5f5t5
4 4U4
4 5(5,50585<5@5D5l5t5x5|5
4 5|5
4 5L5x5
4 5X5
4 5x5
4!4&4.434u4
4!4(4R4W4e4t4
4!4:4K4j4
4!4?4Q4t4
4!4@4
4!414?4N4V4[4f4k4
4!424>4
4!434P406
4!444~4
4!484R4`4h4
4!494H4
4!4D4a4
4!4H4Q4x4
4!4I4
4!4J4
4!5&545B5
4!5:5
4!555>5i5
4!5C5f5
4!5s5
4!8/8F8
4"4(4.4T4
4"4(42484F4[4
4"4,4E4V4w4
4"4;4`4g4|4
4"414b4
4"4'4,414k4
4"4'42474G4L4S4X4b4g4o4t4|4
4"4-42484K4P4W4h4{4
4"4Z4d4y4
4"5A5K5d5
4"5J5
4#454]4b4h4n4
4#454V4[4a4
4#4b4
4#4E4`4
4#4E4Y4^4c4
4#4Z4{4
4#5?5
4#555V5[5a5
4#5c5u5
4#5I5S5s5
4$4(4,4044484<4@4D4H4L4P4T4X4\4d4h4l4p4t4x4|4
4$4(4,404D4H4L4d4h4
4$4(484<4@4D4L4P4d4h4l4t4
4$4)42494E4R4^4
4$4*494Z4
4$4*4V4j4
4$4,444@4`4h4p4|4
4$4,444@4`4h4p4x4
4$4,444@4`4h4t4
4$4,444@4`4p4
4$4,444@4d4l4t4|4
4$4,444<4D4L4T4\4d4|4
4$4,444<4D4L4T4\4d4l4t4|4
4$4,444<4D4L4T4\4d4p4
4$4,444<4D4L4T4\4h4
4$4,444<4D4L4T4`4
4$4,444<4D4P4p4x4
4$4,444<4L4X4
4$4,444<4P4X4
4$4,484X4`4h4p4x4
4$4,484X4d4
4$4,4H4l4
4$4,4L4h4x4
4$4/444@4E4P4U4[4a4n4
4$4?4I4U4p4z4
4$4@4P4\4|4
4$4>4l4v4
4$404P4X4`4h4t4
4$44484@4D4X4\4`4x4|4
4$44484<4T4d4h4x4
4$44484P4`4d4h4l4
4$454V4|4
4$4D4`4h4p4x4
4$4D4L4X4x4
4$4L4
4$4P4U4a4j4o4
4$5.5S5j5
4$5:5y5
4$5=5
4$5P5
4$5q5
4%4=4M4
4%414J4P4T4^4
4%4F4K4Q4y4
4%5&7
4%5/5A5[5
4%5`5
4%515=5F5K5X5]5j5o5|5
4%555;5C5X5c5
4%565R5i5n6
4&4+414X4
4&4+414Y4
4&404:4F4M4y4~4
4(4,40444H4X4\4`4d4h4p4
4(4,404H4L4P4T4X4l4p4t4
4(4?4D4h4
4(4|4
4(404<4\4d4p4
4(454=4E4
4(474
4(484<4L4P4`4d4|4
4(484D4L4l4
4(484H4X4h4x4
4(4A4h4q4
4(4b4i4
4(4F4W4
4(4h4
4(4L4T4\4d4l4t4|4
4(5`5
4(525K5
4(5-5b5
4(5C5J5_5t5
4(5C5W5
4(5N5X5h5x5
4(5X5
4(5Z5
4(7<:L:
4)4.4;4Q4|4
4)4>4
4)4D4g4
4)4D4I4^4
4)4s4
4)4U4Z4f4r4
4)505I5
4)505M5 6
4)606I6
4*4/4:4?4K4P4[4`4e4k4x4
4*4^4
4*464/696R6
4*4A4X4o4
4*4E4j4
4*4L4p4
4*595]5m5
4,4:4V4
4,4@4M4
4,4<4L4\4`4x4|4
4,4<4L4P4T4\4d4|4
4,4044484<4@4T4X4\4`4d4h4p4
4,40444L4P4h4x4|4
4,444@4`4|4
4,444<4D4L4T4\4h4
4,44484<4D4H4L4P4x4
4,5\5
4,5b5
4,5C5N5|5
4,5c7e:w>
4,5s5
4,6g6
4.434z6*<R<
4.4A4a4
4.4k4u4
4.4W4^4
4.5K5d5
4.5M5d5x5
4.5r5
4/4?4O4_4
4/4=4N4o4
4/4>4
4/464O4h4
4/4k4
4/4n4
4/4X4i4
4/5!7
4/5~5
4/5=5
4:4A4V4k4
4:5G5r5x5
4:5W5r5w5
4:6k6
4;;0}
4;5B5Z5m5
4;5E5g5
4;5o5
4;6H6
4?4d4q4u4y4}4
4?5|5
4?5}5e6j6
4?6l6
4@4j4
4@4J4T4^4c4{4
4@4p4
4@4P4\4|4
4@4T4f4u4
4@5\5c5y5
4@5d5
4@5T5z5
4@5x5
4\4l4
4\4s4
4\4t4{4
4\5}5
4]4s4
4]5u5
4^5h5
4^5N6X6r6
4^5w5
4}4$6v6
4~6'7
4+4B4
4+525J5]5}5
4+5U5
4<4\4p4H5t5
4<4D4H4L4T4X4\4`4
4<4D4L4T4\4d4p4
4<4D4L4T4\4h4
4<4D4L4T4p4
4<4D4P4t4|4
4<4f4
4<4H4h4t4
4<4l4
4<4t4
4<5\5c5
4<5`5
4<5r5
4<5z5
4=5G5d5
4=5U5\5
4>4q4
4>4W4u4
4>4y4@5J5_5t5
4>5\5
4>5C5F7
4>5y6
4>5z5
404:4S4
404@4D4H4\4`4d4l4
404@4D4H4L4`4d4h4
404\4
40444L4\4`4d4h4|4
40464N4]4
40484@4L4l4t4
40484@4L4l4t4|4
40484X4`4
404D4
404R4o4
405<5|5
405=5
405h5
405x6}6
40G0b0h0
40K0_0
40K0P0[0h0t0~0
40M0R0
40XPhD
414m4
414O4Y4n4
415[6
41585q5x5
415c5
415G5P5m5
415j5{5
416I6
424~4
42575
425J5T5m5~5
425X5b5
434=4V4
43484=4U4e4l4q4{4
434N4S4h4
434p4z4
434Q4^4j4x4}4
434R4
434u4
434U4
435E5f5k5q5
435E6
435N5
435s5
435v5
4'4.4P4m4
4-4@4
4-405:5S5d576t6
4-42474_4z4
444^4
444<4@4D4L4P4T4X4
444<4D4L4T4\4d4l4x4
44484<4@4H4L4`4d4t4
44484P4T4X4`4x4|4
44494]4
444D4H4X4\4`4t4x4|4
444D4P4p4x4
444D4T4X4\4`4h4
444h4
444O4
445@5S5e5
4'454
445J5j5
445Q5
446K6l6
4'4J4U4
4'5.5F5Y5y5
4-5}5
454:4?4T4^4h4r4~4
454?4\4|4
454<4a4
454X4
455]5
455`5m5
455I5R5
4-5h5
4'5U5
464;4A4`4
464;4A4i4
464;4G4S4
464A4X4a4i4x4
464G6N6
465c5
474A4{4
474O4V4{4
475T5[5p5
475x5
475X5_5t5
484@4H4P4`4
484`4
484A4
484p4
484T4p4
484Z4z4
485`5
485K5n5
485O5f5q5v5
485p5
485S5
485x5
494\4
494C4c4
494C4w4
494h4
494y4
4A5c5
4B4b4
4B4G4M4z4
4b4n4
4B5I5^5s5
4C4~4
4C4Q4q4
4c4u4
4C4U4k4p4u4z4
4C4v4
4C5b5~5+6
4C5k5
4C5t5
4C5u5
4C5y5
4D4a4
4D4a4y4
4D4U4f4q4{4
4D5|5
4D5e8w:
4D5X5
4E4f4p4
4E4U4f5
4E4x4
4E5`5
4E5|5
4F4P4e4z4
4f4u4
4f4v4
4f5~5
4f5v5
4g4n4
4G4t4
4G4x4
4G5L5
4g5q5
4h6,7
4I4f4
4I4i4
4I4O4b4q4
4i5C6S6}6
4i5n5
4J4|4
4J4T4i4~4
4J5_5q5y5
4j5s5
4K4q4{4
4K5T5g5t5~5
4L4|4
4L4T4\4d4l4x4
4l4v4
4L5o5
4M4T4l4
4M5a5
4N5e5j5o5
4O4t4
4o4t4
4O4u4
4O5f5
4O5q5
4P5/6
4Q4_4s4
4Q4e4
4q4N6
4q4v4o6
4R4`5
4R4}4
4R4z4
4r5 7
4r5y5
4S4{4
4s5#6O6
4s5$6V6}6
4T4_4e4q4z4
4T4q4
4T5#6B6
4U4a4~5
4U5\5u5
4U6c6p6x6
4V4`4
4V4c4
4W4a4s4
4W4c4l4q4
4W5[6
4W5}6
4Y5q5x5
4Z4~4
4Z5t5
5 5$5(5,5@5P5T5l5p5
5 5$5(5,5054585<5@5D5H5L5`5d5t5
5 5$5(5,5054585<5@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
5 5$5(5,5T5\5`5d5l5p5t5x5
5 5$5(5@5P5T5d5t5x5|5
5 5$5(5<5@5D5\5l5p5
5 5$5(5<5@5D5H5P5T5\5t5
5 5$5(5054585<5d5l5p5t5|5
5 5$5(565H5
5 5$5,5D5T5d5h5|5
5 5$5<5L5P5T5l5p5
5 5$5<5L5P5T5X5l5p5t5
5 5$54585<5D5\5l5p5
5 5$585<5@5X5h5l5|5
5 5$585<5L5P5`5d5|5
5 5$585<5T5d5h5l5
5 5$585H5X5\5`5x5
5 5$5s5
5 5&505B5a5
5 5(5,50545\5d5h5l5t5x5|5
5 5(5@5P5T5X5\5`5h5l5p5t5x5|5
5 5(5\5d5l5t5|5
5 5(5<5D5X5j5r5
5 5(505@5d5l5t5|5
5 5(505<5\5d5p5
5 5(50585@5H5T5t5|5
5 5(50585@5P5
5 5(545T5\5h5
5 5(5-52575<5A5r5
5 5(5H5P5
5 5*5<5V5d5n5
5 5,5L5T5\5d5l5t5
5 5;5V5
5 5@5H5P5X5`5p5
5 5@5L5l5x5
5 5=5[5|5
5 505@5D5H5P5h5x5|5
5 505@5H5X5\5l5x5
5 5054585<5D5\5l5p5t5
5 5'5.5a5f5m5r5}5
5 5-5o5
5 585<5@5T5X5h5l5p5
5 5D5L5T5\5d5l5t5|5
5 5D5T5\5d5l5t5|5
5 5s5
5 6'6<6Q6h6
5 6C6
5 6D6d6
5 6D6p6
5 6h6
5 6O8
5 6x6
5 6Z6
5 8C:
5!5(5/5>5g5x5|5
5!5,5[5
5!5@5
5!5=5V5k5
5!555>5
5!585
5!595
5!5D5a5
5!5H5Q5h5q5
5!5N5~5
5!5Q5e566H6
5!6&6
5!636Y6`6l6
5!6b6
5!6D6g6
5!6F6s6
5!6g6
5!6j6
5!6x6
5!6Y6^6
5!6Y7
5!8!0-g-
5"5*5?5K5d5j5n5x5
5"5/5;5E5O5[5b5
5"5;5Y5w5
5"515i5o5
5"5'5M5
5"5'5N5
5"5c5
5"6)6A6T6t6
5"6.666N6V6c6k6x6
5"6|6
5"636}6
5"6'636?6
5"666=9c9
5"6J6
5#5/565`5e5s5
5#5'5+5/53575;5W5<7M7
5#555]5b5h5n5
5#5F5
5#656V6[6a6l6
5#6a6
5#6c6u6
5$5(5,5054585<5@5D5H5\5`5d5|5
5$5(5,5D5H5L5P5T5X5\5`5d5h5l5p5t5x5
5$5(5,5D5T5X5\5t5x5
5$5(5@5D5H5P5T5\5t5x5|5
5$5(5<5@5X5h5l5|5
5$5,5<5D5L5T5\5d5l5t5|5
5$5,545@5`5h5p5|5
5$5,545@5`5h5p5x5
5$5,545@5H5h5
5$5,545<5D5L5T5\5d5l5t5|5
5$5,545<5D5L5T5\5d5l5x5
5$5,545<5D5L5T5\5h5
5$5,545<5D5L5T5`5
5$5,545<5D5L5T5`5h5
5$5,545<5D5P5X5x5
5$5,545<5H5h5p5x5
5$5,545<5H5h5t5
5$5,585@5`5|5
5$5,585\5d5
5$5,585X5`5h5p5x5
5$5,585X5`5l5
5$5.5w5}5
5$5;5
5$5<5a5r5
5$5<5D5`5
5$505T5\5d5l5t5|5
5$54585<5@5D5X5\5t5x5
5$545D5T5d5t5
5$585X5d5
5$5A5_5q5
5$5D5L5T5`5
5$5I5P5i5
5$5L5T5
5$5S5b5
5$6.636
5$606C6U6v6{6
5$6k6
5%5:5O5o5
5%5^5t5
5%5=5B5S5
5%515:5?5G5L5
5%565H5
5%595B5
5%5F5K5Q5p5
5%5F5K5Q5y5
5%5j5
5%5K5_5&626l6
5%6=6g6|6
5%626A6_6n6{6
5&5+515P5
5&5+515Y5
5&5+515Z5
5&585R5
5&595_5p5
5&5Z5d5v5
5&6>6Q6d6v7
5&606I6
5&626
5&637m7
5&6F6`6e6
5&6J6
5&6s6
5&6S6n6
5(5,5<5@5D5\5l5|5
5(5,5<5L5P5h5l5
5(5,5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
5(505@5H5X5`5p5
5(50585H5l5t5|5
5(505P5l5|5
5(545
5(575C5P5\5b5t5
5(585<5@5D5L5d5t5x5
5(585<5@5X5\5t5
5(585D5L5l5
5(585H5X5h5x5
5(5F5
5(5H5P5\5|5
5(5H5P5X5`5h5t5
5(5H5P5X5d5
5(5H5T5t5|5
5(5L5T5\5d5l5t5|5
5(6`6
5(656`6f6~6
5(676S6l6z6
5(6q6
5(6t6
5)5:5b5
5)5|5
5)51565C5H5
5)595{5
5)5k5r5
5)5R6
5)6/6B6Q6
5)636L6
5)6s6
5*5;5H5_5
5*5?5
5*5A536B6R6Y6
5*5j5p5
5*5t5
5*5Z5v5
5*6\6
5*656i6w6
5*6l6
5*6s6
5*6y6
5,5@5`5l5
5,5<5@5D5L5d5h5
5,5<5L5T5d5h5x5
5,5<5V5g5~5
5,5054585@5X5h5l5t5
5,5054585<5@5D5H5L5P5T5h5l5|5
5,50545L5\5`5p5t5x5
5,50545L5P5T5X5`5x5|5
5,505H5X5\5`5d5h5|5
5,535X5o5
5,585@5d5l5t5|5
5,5C5
5,5C5]5h5w5
5,5H5X5d5l5
5,5P5t5
5,5T5
5,6C6H6M6b6n6
5,6l6
5,6R6j6
5.5=5I5V5\5b5l5
5.595L5
5.6[6r6
5.6B6
5/5D5[5v5
5/5R5m5
5/6<6g6m6
5:5?5
5:5K5
5:5L5
5:6`6
5;6k6
5;6T6j6
5?*BL?fff?
5?5e5o5
5?5z5u7
5?6L6w6}6
5?6u6
5@5p5
5@5x5
5@6H7
5@6X6b6{6
5@6Z6
5[:p:v:
5[5e5
5[6e6w6
5\5|5
5]556
5^6y6
5`6w6
5`6x6
5+5d5
5+5F5a5
5+5J5
5+6@6
5+6[6p6v6
5+656J6_6s6
5+676
5+6A6
5+6D6g6
5+6Q6[6
5<5`5
5<5D5L5T5\5d5l5t5|5
5<5i5
5<5y5
5<6F6[6p6
5<6I6g6{6
5<6P7W7p7z7
5<7F7_7
5=5{5
5=5o5
5=5Q5e5
5=6D6K6V6
5>6|7
505@5D5H5L5T5l5|5
505@5D5T5X5\5d5|5
505@5L5T5
505_5v5{5
505`5
5054585@5X5h5l5|5
5054585<5@5D5H5L5P5T5X5\5`5d5h5l5t5x5|5
5054585<5@5T5X5\5`5h5
50545L5\5`5d5x5
50555
50585@5H5P5\5|5
50585@5L5l5t5|5
505o5
505P5X5`5h5p5|5
506b6
506D6j6~6
506h6
506h8
506J6
506P6X6d6
506X6
50l0v0
51516C6q6
515A5Q5a5
515G5r5
515m5
515S5]5z5
525<5F5P5U5m5r5}5
525r5
526]6
526k6
526q6
526s6
535:5_5v5
535;5(666?6E6
535k5
536l6w6
536s6
537Y7
545<5D5L5T5\5d5l5t5|5
545<5D5L5T5\5h5
545<5D5L5T5\5l5t5|5
545>5
545>5J5e5o5{5
54585T5X5t5x5
545D5H5L5P5X5\5p5t5
545E5v5
545G5S5^5
545J5O5U5[5a5g5m5s5y5
545p5
546O6
546O6Y6
547_7
5'5,575D5P5Z5d5n5x5
5'5.5P5Z5s5
5-5]5
5'5<5G6a6v6
5'5<5S5m5x5
5'5>5{5
5'5>5U5l5
5'515;5E5Q5}5
5'525]5s5
5-525A5F5U5Z5t5
5-525C5r5
5-525T5
555?6I6[6x6
555{5
555<5a5x5
555i5
555n6
556:6I6Y6
5'565E5
5'585
5-5B5`5j5|5
5'5B5I5P5Y5b5v5~5
5'5h5
5-5i5p5
5-5N5d5
5'60696B6^6g6{6
565;5
565;5A5i5
565@5\5f5{5
565a5
565h5
566c6
566L6o6
5'6c6
5'6e6z6
5'6L6V6h6m6|6
5'6Q6v6
5-6z6
5-6Z6i7
575A5Z5k5
575b5}5
576O6Y6r6
577<7J:O:
5'7s9
585@5H5P5X5h5
585_5i5{5
585`5
585A5h5q5
585D5d5l5x5
585I5
585r5y5
585T5
586f6
586h6
586x6
596g6
596T6
5A5d5
5A5l5
5a5u5~5
5a5v5
5B5~5
5B5a5
5b5i5~5
5B5m5
5C5I5a5p5
5c5p5
5c5u5
5c6C7C8
5C6f6x6
5C6L6
5C6u6
5C8U8v8{8
5D6N6c6x6
5d6p6
5D6q6
5D6v6{6
5E5`5g5
5E5J5U6Z6
5F5{5
5F5l5
5F5R5
5F5y5
5f6d7
5F6Z6
5G5m5
5G5N5g5x5
5G5r5
5G6_6i6
5Genu
5H5a5
5H5f5
5H5M5R5Z5k5w5
5H5P5X5`5h5t5
5H5Q5
5h6{6
5I5Z5
5I6N6
5I6q6
5ineI
5J5i5w5~5
5j6h7>8e:5=
5j6q6
5J6Q6j6{6
5j6s6
5J6x6
5J7p7
5K5\5o5
5K5f5w5
5K5P5\5h5v5{5
5K5R5j5}5
5K5U5j5
5K7.8
5k7u7
5L5m6t6
5L6z6
5M5q5
5M5W5i5
5M6a6
5m6w6
5m8w8
5N5[5|5
5N5m5
5N5U5j5
5n6#7N7U7`7l7
5N6f6p6
5N6i6
5N7h7
5O5/797N7c7z7
5O6f6
5O6f6k6p6
5P;u;}<#=*=C=x=
5P6d6
5P7n7
5Q6v6
5R6p6
5R6s6
5r6y6
5S5e5
5S5g5
5S5u5
5s6|6
5S6l6
5S6n6x6
5S7u7
5T5q5
5T6^6
5t6~6
5T7p7z7
5U6s6
5V6J718
5X5x5
5X6|6
5X6l6
5x6P7
5X6q6
5Y6`6u6
5Z5d5w5
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6t6x6|6
6 6$6(6,6064686<6@6D6H6L6P6X6\6`6d6h6l6p6t6x6|6
6 6$6(6,60646H6X6\6l6p6
6 6$6(6,606D6H6L6d6t6x6|6
6 6$6(6,646L6\6`6d6h6p6
6 6$6(6<6L6\6`6h6
6 6$6(606H6X6\6`6x6|6
6 6$6(6P6X6\6`6h6l6p6t6
6 6$6,606D6H6`6d6|6
6 6$6,6D6T6X6\6t6x6|6
6 6$6<6@6D6X6h6l6|6
6 6$6<6@6X6h6x6|6
6 6$64686H6X6h6x6
6 6$686<6@6X6h6l6|6
6 6(6,646<6T6d6h6
6 6(6@6D6\6l6|6
6 6(606<6\6d6l6t6
6 6(60686@6H6P6X6
6 6(60686@6L6l6t6|6
6 6(60686D6d6l6t6
6 6(60686D6d6l6t6|6
6 6(646T6\6d6l6t6
6 6(646T6\6h6
6 6(646T6`6
6 6*646p6
6 6;6~6
6 6@6\6d6l6t6|6
6 6@6H6P6X6`6h6x6
6 6@6L6l6t6
6 6<6
6 606@6P6`6p6x6
6 606@6P6T6X6`6x6
6 6064686L6P6T6X6l6p6
6 60646D6H6L6P6X6\6p6t6
6 60686\6l6t6|6
6 60686H6P6
6 60686H6T6d6t6
6 686<6@6H6`6d6h6|6
6 686H6L6P6d6h6x6|6
6 686H6L6P6T6\6t6
6 686H6L6P6T6X6\6p6t6x6
6 6D6L6T6\6d6l6t6|6
6 6e6|6
6 6H6P6T6X6`6d6h6l6
6 6s6
6 6T6\6d6l6t6
6 7%7*727C7O7^7c7j7o7}7
6 7^7
6 797F7W7d7v7
6 7d7
6 7l7
6 7P7
6 7x7
6!6(6N6n6
6!6,6R6
6!6?6Q6t6
6!6@6
6!6]6q6
6!6=6f6p6
6!666K6
6!666M6N7S7
6!6b6
6!6H6Q6h6q6
6!6H6S6f6o6t6~6
6!6K6
6!6x6
6!7]7
6!707?7e7r7
6!7l7
6!7O7y7
6!7q7
6"6(6.6T6
6"6/6
6"6/6E6
6"6'6,6A6K6U6a6
6"676N6
6"696>6e6
6"6A6O6
6"6D6
6"6f6u6
6"6s6
6"7?7V7[7
6"7\7w7
6"7A7
6#6_6
6#606N6X6q6
6#61696T6Z6^6q6
6#656@6F6P6b6
6#6'6+6/63676;6?6C6G6K6O6S6W6[6
6#696
6#6Y6p6
6#757]7b7h7n7
6#757V7[7a7
6#7c7u7
6#7S7C8
6#7U7s7
6$6(6,6@6P6T6X6`6x6
6$6(6,60686P6`6p6t6x6
6$6(6@6P6T6h6l6
6$6,6@6H6u6
6$6,646@6`6l6
6$6,646<6D6L6T6\6d6l6t6|6
6$6,646<6D6L6T6\6d6l6x6
6$6,646<6D6L6T6\6d6p6
6$6,646<6D6L6T6`6
6$6,646<6D6L6X6|6
6$6,646<6D6L6X6x6
6$6,646<6H6h6p6|6
6$6,646<6H6P6p6x6
6$6,686\6d6l6t6|6
6$6,686X6`6h6p6|6
6$6,6L6h6x6
6$6.686B6G6_6d6o6t6
6$6.6R6
6$6;6X6
6$6+6B6
6$6=6
6$60686X6t6|6
6$606P6\6|6
6$606P6X6`6h6p6x6
6$606P6X6`6l6
6$606T6\6d6l6t6|6
6$646:6B6W6c6|6
6$64686<6@6T6d6h6x6|6
6$64686H6L6P6T6h6l6
6$646D6H6L6P6d6h6
6$646D6T6d6t6
6$696
6$6A6K6O6S6W6[6_6c6g6k6o6s6w6{6
6$6C6d6
6$6D6L6T6x6
6$6D6L6X6x6
6$6L6T6\6d6
6$7,70747<7@7D7H7p7x7|7
6$7:7n7
6$7`7
6$7+7@7S7^7d7n7u7
6$7P7|7
6$7t7
6$7T7x7
6%6*6/6D6N6Z6
6%6<6[6e6w6
6%6=6B6S6y6
6%608(929K9u9
6%656;6C6X6d6}6
6%6-6<6O6
6%666=6D6`6n6
6%6F6K6Q6p6
6%6F6K6Q6y6
6%6J6
6%7/7D7Y7p7
6%7+7C7R7
6&6?6P6
6&6+616Y6
6&6=637=7V7
6&606F6M6y6~6
6&626>6J6V6b6n6z6
6&656
6&666A6`6g6|6
6&7}:
6&7-7B7W7
6&7c7t7
6(6,6<6@6P6`6d6h6
6(6,6064686@6X6h6l6|6
6(6,606H6X6\6t6
6(6,6D6H6`6p6
6(606<6\6d6p6
6(60686H6l6t6|6
6(62676<6P6\6a6k6p6z6
6(626X6k6
6(646T6\6d6l6x6
6(686@6H6X6h6p6x6
6(686<6@6X6\6t6x6
6(686<6L6P6T6X6\6`6t6x6
6(686H6X6h6x6|6
6(6H6X6|6
6(6L6T6\6d6l6t6|6
6(6P6
6(7{7
6(7c7{7
6(7H7
6(7p7
6(9];
6)636?6Z6d6p6
6)646B6H6P6Y6j6p6u6
6)646D6K6[6b6r6y6
6)6c6j6
6)6I6
6)6w6
6)7B7[7
6*6=6C6Q6V6e6k6w6
6*6>6
6*7A7F7m7
6*7q7
6*7X7
6*8?8Q8Y8a8
6*9/9
6,6^6
6,6<6D6P6p6
6,606@6D6T6X6h6l6|6
6,6064686<6P6`6d6|6
6,60646L6P6T6X6`6x6
6,606H6X6\6`6d6l6
6,626F7R7
6,646
6,646@6`6h6p6x6
6,686\6|6
6,6E6Y6n6
6,6H6X6d6l6
6,6O6j6
6,6T6
6,6X6
6,7@7
6,717
6,767[7o7t7
6,7g7
6,7l7
6,7M7b7
6,7M7b7t7~7
6,7R7
6,7X7
6.=5=<=S=^=j=y=
6.6C6Z657?7T7i7
6.7{7
6.7>7}7
6.7C7U7v7{7
6/6:6H6Z6
6/6K6R6v6
6/787N7q7
6/7I7
6/7J7
6:6j<'=>=L=a=
6:6x6
6:7b7v7
6;697
6;6A6
6;6b6
6;6B6[6l6
6;w,u
6?6F6^6q6
6?6Q6g6q6
6?7D7
6?7S7
6@6E6
6@6H6L6P6X6\6`6d6
6@6j6n7p9
6@6m6
6@6q6
6@7\8
6@7G7\7q7
6[6`6k6p6x6}6
6\6a6l6q6z6
6\6Q7f7
6\7t7~7
6]6s6
6]7w7
6^;M=
6`6g6|6
6`8w8
6|7*859<9b9v9
6+6>6Q6
6+606=6B6O6T6\6a6
6+656N6_6f8^9h9
6+656N6h6r6
6+6U6
6+7W7
6<6@6\6`6
6<6D6L6T6\6d6l6t6|6
6<6D6P6p6x6
6<6F6Y6i6
6<6M6i6
6<7\7r7
6<7i7
6<7k7
6<7N7
6=6h6
6=6m6O7|7
6>6W6
6>7K7n7t7
606@6D6H6\6`6d6h6p6
606@6D6T6d6h6l6p6x6
606@6P6T6\6`6h6
606\6b6i6v6}6
606^6u6
606`6
606<6\6h6
6064686L6P6h6l6p6x6
60656:6\6
60666N6]6
60686@6\6d6l6t6|6
60686@6H6P6\6|6
60686@6H6P6`6
60686@6H6T6t6
606E6
606t6
606X6
607]7B8
607|7
607h7
607H7R7k7
607p7
616;6Q6a6q6{6
616\6c6
616>6S6y6
616H6`6
616H687{7
61767
627>7G7L7
627M7
627R7
636:6S6d6
636]6
636`6
63686=6b6
636j6
636N6
637:7S7
637b7
637E7f7k7q7
637V7
637Y7c719X9a9
639r9
646<6D6L6T6\6d6l6t6
646<6D6L6T6`6
64686<6P6T6X6p6
646D6H6L6d6t6x6
646D6P6p6x6
646O6j6
646P6`6p6
647{7e9
647Q7
656<6U6
656f6
657L7
6'6@6d6x6
6-6_6
6'6<6S6
6-6=6
6'616=6i6n6z6
6-626C6r6
666;6A6h6
666;6A6s6
6'666J6
666E6
6-676>6K6\6d6s6}6
6-676k6
667L7U7
6'7"8U8P9
6'7^8
6-7_7
6'7>7t7
676=6U6d6
676J6V6q6w6|6
676V6
677]7t7
6-777L7a7
677H7
677l7
677w7|7
6-7n7x7
6-7O7
6'7z7
686@6`6h6
686@6D6H6P6T6X6\6
686A6h6q6
686d6w6
686H6X6|6
686x6
687p7
687x7
696j6
696W6
6a6k6
6A7U7l7q7v7
6a7x7
6A7X7a7x7
6B;-=
6B6b6
6B6f6
6B6G6
6B6G6L6g6
6B6j6
6B6Z6|6
6C6U6
6c7u7
6C7U7v7{7
6c8g9{9;:
6D6o6
6D6U6
6D7I7
6d7j7p7v7|7
6D7P7c7u7
6D7r7
6E6O6k6
6E6Z6
6E7|7
6e7|7
6F6M6b6w6
6F7w7
6F7z7
6g:2;
6G6N6f6y6
6G7r8
6G9_9i9
6H6R6k6
6h7q7z7
6i6n6
6J6r6
6J6u6
6J7!8
6JCy7JCy7JCy7JCy7
6L7Q7
6M6~6g8
6m6r6
6M6u6
6M7a7
6M869s9
6N6S6Z6_6p6|6
6N8\8g8u8
6O6|6
6P:<;V?
6P6U6
6P6v6
6q6v7
6Q6X6p6
6Q7~7
6q7~7
6q7v7
6r6y6
6R7_7
6S6\6n6
6S6e6
6S748a8k8
6S7j7o7t7
6S7k7u7
6T6\6d6l6t6|6
6T6q6
6T7`7s7
6T7p7y7
6T7q7
6T7z7
6u6&7
6U6_6t6
6U7_7
6U7l7w7|7
6V697O7
6V6v6
6V7[7
6V7_7
6W6^6v6
6W6a6~6
6W6m6
6x6f7
6X7_7x7
6Y6c6
6Y758k8
6Z7g7
6Z7u7
6Z8o8
7 7$7(7,7074787<7@7D7H7L7P7T7X7
7 7$7(7,7074787<7@7D7H7L7P7T7X7\7`7d7h7l7p7t7x7|7
7 7$7(7,7074787<7@7D7H7L7P7T7X7`7x7
7 7$7(7,7074787<7@7D7H7L7P7W7
7 7$7(7,7074787<7@7D7H7T7
7 7$7(7,7074787<7@7D7L7d7t7x7|7
7 7$7(7,747L7\7`7d7|7
7 7$7(7,747L7P7T7h7l7
7 7$7(7@7P7T7l7|7
7 7$7,74787<7@7H7P7h7l7
7 7$7,7D7T7X7h7l7p7x7|7
7 7$7,7D7T7X7p7
7 7$74787H7L7\7`7d7l7
7 7$787H7L7P7h7x7|7
7 7&7*737P7W7c7u7z7
7 7&7>7M7
7 7&7w7
7 7(7
7 7(7,70747H7L7P7h7x7|7
7 7(7@7P7`7p7t7x7|7
7 7(7\7l7x7
7 7(707@7d7l7t7|7
7 7(707<7\7h7
7 7(70787H7l7t7|7
7 7(747T7\7d7
7 7(747T7\7h7
7 7*7>7C7[7`7k7p7|7
7 7,7j7o7}7
7 7@7H7d7
7 7@7H7P7\7|7
7 7@7H7P7`7
7 7@7H7T7t7|7
7 707@7P7`7p7
7 70747L7P7X7p7t7
7 70787@7H7X7`7h7x7
7 737E7f7k7q7
7 74787H7L7P7h7x7|7
7 74787P7`7d7|7
7 757
7 787<7@7H7L7T7l7|7
7 787H7L7\7`7x7
7 7s7
7 8>8Z8q8
7 8E8
7 8h8
7 8H8
7 8N8
7 8X8
7 8x8
7 8X8
7!7/747<7A7
7!7:7D7_7i7
7!7@7
7!7+757A7x7}7
7!767M7
7!787A7h7
7!7A7{7
7!7a7k7
7!7d7
7!7w7
7!8?8
7!8+8=8B8P8[8q8
7!8<8a8
7!8A8
7!8n8x8
7!8z9
7"7*7;7G7S7
7"7.7:7F7V7]7l7t7|7
7"7.7z8
7"7;7
7"747k7
7"777L7c7
7"7A7
7"7D7T7_7{7
7"8\8
7"8'8
7"8k8
7#7(7C7M7W7a7k7u7
7#7*797Y7~7
7#7\7f7y7 8U8
7#7|7
7#7'7+7/73777;7?7C7G7K7O7S7W7[7_7c7g7k7o7s7w7{7
7#7F7
7#7G7~7
7#7G7U7
7#808N8
7#858]8b8h8n8
7#858d8i8o8u8{8
7#858V8[8a8
7#8-8B8W8n8
7#8c8u8
7#8M8
7#8N8U8`8l8
7#8U8s8
7#8x9
7#9-9F9
7$7(7,70747H7X7\7`7x7
7$7(7,707D7H7L7T7X7`7d7x7|7
7$7(7,74787<7@7H7L7P7T7X7`7d7h7l7p7t7x7|7
7$7(7,74787<7@7h7p7t7x7
7$7*737?7e7t7
7$7,7<7D7P7t7|7
7$7,747@7`7h7p7x7
7$7,747<7D7L7T7\7d7l7t7|7
7$7,747<7D7L7T7\7d7l7x7
7$7,747<7D7L7T7\7h7
7$7,747<7D7L7T7`7
7$7,747<7D7L7t7|7
7$7,747<7D7L7X7x7
7$7,747<7H7h7p7x7
7$7,747<7H7l7t7|7
7$7,747<7H7P7p7
7$7,747<7L7T7\7l7t7|7
7$7,747T7\7d7l7
7$7,787X7`7h7p7|7
7$7,7D7L7T7\7d7l7t7|7
7$7,7L7h7x7
7$7`7
7$7<7D7\7d7x7
7$70787l7t7|7
7$707P7\7|7
7$707P7X7`7h7p7|7
7$707P7X7`7h7p7x7
7$707P7X7d7
7$747<7D7L7T7\7d7l7t7|7
7$74787H7X7h7l7p7x7
7$74787P7T7X7l7p7t7x7
7$747D7T7d7t7
7$757F7W7h7y7
7$7D7L7T7\7h7
7$7D7P7p7x7
7$7h7m7
7$8:8S8
7$8@8
7$8<8F8_8y8
7$8C8U8
7$8D8`8
7$8H8a8f8
7$8s8
7$8T8x8
7%7.757P7[7c7k7s7{7
7%717
7%7B7b7l7
7%7F7K7Q7y7
7%7F7K7Q7z7
7%8@8G8]8n8
7%8_8&9N9
7%8`8
7%8H8^8s8
7&7+717P7
7&7+717Y7
7&7=7K7b7p7
7&7=7z7
7&7d7~7
7&7e7
7&8[8
7&8}8
7&8M8T8i8
7&8o8{8
7&8U8
7&8x8}8
7(7#9
7(7,7<7@7P7T7l7p7
7(7,7<7L7P7h7l7
7(7,70747H7L7P7h7x7
7(7,70787<7D7\7`7x7
7(7,70787<7D7\7`7x7|7
7(7,7D7T7X7\7`7h7l7
7(7/777L7
7(7;7C7
7(707@7L7\7l7|7
7(707P7X7x7
7(717x7
7(727<7C7Z7_7v7{7
7(747O7Y7e7
7(787@7P7\7l7|7
7(787\7d7l7t7|7
7(787<7D7L7P7d7h7
7(787<7L7P7T7\7t7x7
7(787<7L7P7T7X7\7d7|7
7(787<7L7P7T7X7`7x7
7(787a7z7
7(787D7L7l7
7(787H7X7\7`7h7p7t7|7
7(7H7P7X7`7h7p7x7
7(7H7T7t7|7
7(7L7T7\7d7l7t7|7
7(7O7^7
7(7T7
7(7T7e7
7(7X7
7(7y7
7(8`8
7(8-868;8B8G8L8T8e8q8
7(8C8
7(8s8
7)7<7H7b7u7
7)7E7Z7
7)7i7
7)7q7
7)8i8
7)8v8
7*7=7
7*767K7U7k7{7
7*7D7
7*7F7]7
7*7I7X7
7*8>8z8
7*8U8q8
7*8z8
7,7<7H7P7p7
7,707@7D7H7`7d7h7|7
7,707@7D7H7L7T7l7|7
7,7074787L7P7`7d7|7
7,70747H7L7d7t7x7
7,70747L7P7h7x7
7,707H7X7\7l7|7
7,707H7X7\7l7p7t7x7
7,727J7Y7
7,747@7`7h7p7x7
7,747@7`7h7t7
7,747<7D7L7T7\7d7l7t7|7
7,74787<7D7H7L7P7x7
7,767O7`7w708H8R8k8-:7:P:
7,7A7\7
7,7c7
7,7h7
7,7L7T7\7d7l7t7|7
7,7q7
7,838L8a8
7,8C8
7,8O8
7,8P8|8
7,8R8j8
7.7@7
7.7_7f7{7
7.767>7F7
7.7A7g7x7
7.7E7
7.7J7
7.7Q7
7.7T7t7
7.8J8
7.8o8
7/767O7Y7p7
7/7D7L7T7|8O9Y9n9
7/8K8
7:7A7Y7
7:7x7
7:8I8
7;:u:
7;3tQ
7;7^7
7;7E7Z7o7
7;7Q7
7;8T8
7?7x7
7?7y7
7@7L7l7t7|7
7@7o7
7@7P7\7d7
7@7x7
7@8x8
7@PVW
7]8o8
7^7f7w7
7^7i7q7|7
7^8c8f:
7_7i7o7{7
7_7z7
7_8j8
7`8j8
7|8b9
7+:u<
7+707;7H7T7^7~7
7+7N7w7
7+7X7
7+828G8\8s8
7+8m8
7+8X8
7<7B7Z7i7
7<7C7\7
7<7C7J7U7
7<7D7P7p7x7
7<7F7Y7i7
7<7H7P7
7<7X7h7t7|7
7<8\8r8
7<9P9g9l9q9
7=8Q8j8
7=8Q8w8
7=8X:t:
7>7d7n7
7>7R7f7x7
7>7s7
707@7d7|7
707@7D7H7\7`7d7h7|7
707=7X7_7w7
70747D7H7X7h7l7|7
70757:7O7Y7e7
70757<7M7`7
70777P7a7|7
707e7
707E7\7w7
707G7
707L7l7t7
707T7d7p7x7
707T7x7
708[8
708D8
708h8
708K8
708S8
708x8
717:7a7q7
717;7T7e7|758M8W8p8
717c7
717F7^7l7
717F7c7u7
717G7
717H7Y7
717t7
717T7q7
717T7w7
718C8x8
718t8
719A9M:
71P1q1
727;7]7
727G7^7
728A8
728w8
737=7K7c7m7{7
737H7
737p7
738E8f8k8t8
738J8h8|8
738K8U8n8Z9r9|9
738O8Y8n8
738r8
738U8
739=9V9
747;7O7j7
747<7@7D7L7P7T7X7
747<7D7L7T7\7d7l7t7|7
747<7D7P7p7x7
747>7P7U7c7n7
747n7
747N7S7o7t7
747O7T7{7
747z7
748@8S8e8
748p8
748Q8
757;7S7b7
757F7]7
758]8
758B8
758P8W8
767;7@7U7_7k7
767;7A7i7
767;7A7s7
767;7b7
767@7Q7
767y7
768C8
768j8
768R8
768w8
7'7,757:7E7J7U7d7r7
7-7@7L7
7'7_7
7'70797B7K7T7]7f7o7{7
7-727>7J7
7-72787>7d7
7'747>7H7R7\7f7
777`7
777{7
777q7
778^8e8
7799<
7-7B8I8b8
7'7c7j7
7-7G7d7~7
7'7G7Q7j7{7
7'7s7|7
7-7X7a7f7q7v7}7
7'7X7b7t7
7-7Z7
787a7k7}7
787A7X7a7
787A7X7a7}7
787r7y7
787t7
787T7\7d7l7t7
788`8
788p8
788v8
788x8
7-8A8
7'8b>o>
7-8b8v8
7-8U8
797C7S7s7
797I7O7W7l7x7
797m7
798@8!9
798w8
7A:g:q:
7A7Q7a7s7
7a7u7
7A7X7p7x7
7a8,;M;`;o;t;
7B7i7
7B7I7n7
7B7k7
7B7L7a7v7
7B7L7q7
7B7m7
7B7y7
7B7Z7b7
7B8K8
7B8L8
7b8w8
7C;^$|
7c7u7
7C8\8
7C8U8v8{8
7D7I7O7U7[7
7D8f8p8
7D8L8T8`8
7D8P8c8u8
7d8s8p9
7E7p7
7E7Y7f7u7
7e8|8
7e8o8
7E8t8
7F7g7t7
7F8o8
7F8U8l8
7F8v8
7F8W8}8
7g7u7
7G8h8y8
7G8o9
7GY;}
7H7t7
7H7z7
7h8t8
7h9z9
7I7N7
7I7O7b7q7
7i7z7
7I8[8
7i8}8
7I8h8r8
7J7g7
7J7u7
7J7X7d7s7
7J8s8:9:=C=Z=f=t=
'7JCy7
7K7[7a7i7~7
7K7_7
7K7|7
7K7a7
7K7h7
7K8P8d8i8
7K8U8_8
7L7T7\7d7l7t7|7
7L8V8o8x9
7M7a7
7M8|8
7M8a8
7M8R8
7M8s8
7M8Z8
7N8[8
7N8b8
7n8v8~8
7N9X9k9
7O:\:u:
7O7e7t7y7
7O8j8
7o8y8
7P7t7|7
7P7y7
7P8a8
7P8v8
7Q7c7
7q85=j=
7R7z7
7R8L9
7R8Y8r8
7S7_7h7m7
7S7i7
7S7W7[7_7c7g7k7o7s7w7{7
7S8X8m9
7s8z8
'7SimplifiedLayerNormalization
'7SSSRQh
7t7M8
7T7t7
7T7z7
7t8|8
7U;Z;
7u7|7
7V8e8
7W7^7v7
7W8p8
7X7x7
7X9v9
7Y8q8x8
7Z8a8v8
7Z8a8z8
8 8$8(8,8084888<8@8D8H8L8P8d8h8l8p8x8|8
8 8$8(8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
8 8$8(8,8T8\8`8d8l8p8t8x8
8 8$8(8084888<8d8l8p8t8|8
8 8$8(808D8H8L8d8t8
8 8$8(808H8X8\8l8p8
8 8$8,8084888<8@8D8L8d8h8l8p8t8|8
8 8$8<8L8P8`8d8t8x8
8 8$8<8L8P8T8\8t8x8|8
8 8$84888<8@8H8L8`8p8t8
8 8$84888<8T8d8h8l8
8 8$8H8L8T8X8\8d8h8l8p8
8 8%8
8 8(8,80848\8d8h8l8t8x8|8
8 8(8,8084888<8@8H8L8P8X8`8h8l8t8x8|8
8 8(8\8d8l8t8|8
8 8(808<8\8d8l8t8
8 8(80888@8L8l8x8
8 8(808D8T8X8\8t8
8 8(888H8P8X8h8x8
8 8(8P8X8`8h8|8
8 8;8V8q8
8 8@8H8P8X8`8h8p8|8
8 8@8H8P8X8`8h8x8
8 8@8H8T8t8|8
8 8@8L8T8
8 8~8
8 8+8;8E8O8s8x8
8 80848D8H8X8l8|8
8 808T8\8d8l8t8|8
8 848<8D8L8T8\8d8p8x8
8 84888H8X8\8t8x8
8 84888P8`8d8|8
8 848D8H8L8P8d8h8
8 858 9=9R9k9
8 8'8<8Q8
8 888<8@8T8X8h8x8|8
8 888<8D8\8`8x8|8
8 888<8T8d8h8x8|8
8 888H8L8P8d8h8
8 898J8
8 898J8f8
8 8B8_8
8 8D8L8d8|8
8 8D8L8T8\8d8l8t8|8
8 8s8
8 9(9x9
8 9*949u9
8 919S9d9m9s9
8 969x9
8 9'9<9Q9
8 9F9s9
8 9r9
8 9X9
8!8%8/8M8]8c8k8
8!8&84898D8S8f8
8!8&888A8L8d8{8
8!8@8
8!8+8@8U8i8?9D9
8!818A8Q8a8q8
8!868J8
8!888K8Q8Y8f8w8
8!8'8H8P8`8g8t8|8
8!8H8a8
8!9^9r9{9
8!9_9v9
8!909J9]9~9
8!9A9K9
8!9q9
8"8,838\8a8r8}8
8"8/8<8I8U8
8"8>8g8q8
8"8'8>8[8
8"8'80858>8C8\8h8t8y8
8"888D8
8"999V9]9
8"9c;
8#8(80858:8.9N9
8#858]8b8h8n8
8#868Q8m8
8#8'8+8/83878;8?8C8G8K8O8S8W8[8_8c8g8k8o8s8w8{8
8#888M8q8
8#8C8
8#8I8
8#9:9N9
8#9_9
8#959V9[9a9
8#9c9
8#9c9u9
8#9E9t9y9
8#9U9s9
8#9Z9~:
8$;[<o<
8$8(8,8D8T8X8\8p8
8$8(8@8P8T8l8|8
8$8(888H8L8\8`8x8|8
8$8(888H8L8P8h8x8|8
8$8(888H8X8h8x8
8$8,8`8p8
8$8,848@8`8l8
8$8,848<8D8L8d8l8
8$8,848<8D8L8T8\8d8l8
8$8,848<8D8L8T8\8d8l8t8|8
8$8,848<8D8L8T8\8d8l8x8
8$8,848<8D8L8T8\8d8p8
8$8,848<8D8L8T8\8h8
8$8,848<8D8L8T8`8
8$8,848<8D8L8X8|8
8$8,848<8D8L8X8x8
8$8,848<8H8h8p8x8
8$8,848<8H8h8x8
8$8,888X8`8h8
8$8,888X8`8h8p8x8
8$8,888X8d8l8
8$8.888D8K8w8|8
8$8<8T8\8d8l8t8
8$8=8W8a8z8
8$808P8\8d8
8$808P8`8
8$808R8x8}8
8$848<8D8L8T8\8h8
8$84888<8@8D8H8L8P8T8X8\8`8d8h8l8p8x8|8
8$84888H8L8P8h8x8
8$848D8H8L8d8h8
8$848D8H8X8\8l8|8
8$858F8W8h8y8
8$8D8L8T8\8d8l8t8
8$8D8L8T8\8d8p8
8$9^9
8$909C9U9v9{9
8$9H9l9
8$9i9
8$9q9
8$u V
8%8*8@8E8U8[8l8s8
8%8*82878
8%8/8H8Y8p8)9A9K9d9
8%8?8e8
8%868B8k8
8%868C8O8[8j8o8w8|8
8%8F8K8Q8p8
8%8F8K8Q8y8
8%8H8s8
8%8M8R8X8^8
8%9c9
8%9K9c9
8%9L9^9m9s9y9
8%9O9
8%9Q9
8%9t9
8&:<:
8&8*8C8P8T8Y8^8c8h8m8r8
8&8+818Y8
8&8-8F8X8w8
8&898Y8
8&8E8U8e8n8
8&8J8y8|9
8&9@9H9
8&929
8&989n9
8&9-9F9j9q9
8&9A9R9
8&9E9
8&9f9
8&9T9
8(8,8<8@8X8\8t8x8
8(8,80848<8T8d8h8x8|8
8(8,808D8H8X8\8`8x8|8
8(8,808D8T8X8p8t8
8(8<8
8(80888D8d8l8t8|8
8(828>8s8x8}8
8(888\8d8l8t8|8
8(888H8X8\8`8d8h8l8p8x8|8
8(8A8
8(8d9w9
8(8H8P8X8d8
8(8H8T8t8|8
8(9`9
8(9h9
8(9H9l9
8(9p9
8(9x9
8)898I8
8)8s8
8)9L9o9
8*8_8
8*848C8S8_8k8w8
8*848M8^8s8
8*8F8
8*8x8
8*9I9V9w9
8*9w9
8,8<8@8D8X8\8l8p8t8
8,8<8@8P8`8d8h8l8p8t8x8|8
8,8<8L8\8`8x8
8,8<8L8P8T8X8\8`8d8h8l8t8
8,8<8L8T8d8p8
8,808@8D8\8`8x8
8,8084888<8@8D8H8L8P8T8X8\8`8h8
8,8084888L8\8`8p8
8,848<8D8L8T8\8d8l8t8|8
8,848<8D8L8X8|8
8,8C8]8h8w8
8,8C8c8
8,8H8X8d8l8
8,8T8
8,8W8]8u8
8,9G9b9
8,t'V
8.8=8
8.8C8p8
8.9;9^9d9|9
8.979
8.9d9
8/8A8I8Q8
8/8d9w9
8/8U8i8v8
8:8W8n8s8
8:9|9
8:9n9
8;8B8W8l8
8;8E8b8
8;8J8O8
8;8V8
8;9O9N;"=+={=L>
8?8S8y8
8?9\9R<
8?9^9
8?9S9
8?9v9
8?u'@
8@8p8
8@8x8
8@9E9
8@9t9
8@u)@
8@u-@
8[8x8
8[90:W:
8\8d8l8t8
8\u"j
8]8{8
8]8j8s8x8
8^u(@
8_,t*j
8_:r:
8_^[]
8_8t9j
8_9n9
8_Ttl9_Xv`Sh,1>
8{un@
8}879F9f9z9
8}8I9
8~8G9
8+858?8]8b8z8
8+9@9S9_9t9
8+9A9J9
8+9F9
8<8D8L8T8\8d8l8t8|8
8<8D8L8T8`8
8<8F8X8u8
8<9_9
8=8b8
8=8f8
8=8G8W8g8w8
8=8Q8
8=9B9
8=9G9
8=9J9
8=9r9
8>8D8\8k8
8>8U8Z8_8
8>9i9
8>9W9j9
80:9:X:
808@8D8H8L8`8d8t8x8|8
808@8D8T8X8\8p8t8x8|8
808<8\8d8p8
80848L8P8h8l8
80868:8D8e8u8{8
80888D8d8l8x8
80888L8T8\8d8x8
808C8L809
808I8n8u8
808J8X8b8t8
808L8\8h8
808W8
808X8
80K0h0
818[8
818<8B8L8^8
818f8p8
819C9
819H9V9[9h9t9~9
819Y9c9o9{9
81V1i1
828@8H8
828A8y8
828C8E<{<
82999Q9d9
829B9
83898Q8`8
838D8`8w8q9v9
838H8
839=9R9g9~9
839E9f9k9q9
839s9
839y9
848<8D8L8l8t8|8
848<8D8L8T8`8
84898?8N8
848D8H8`8d8l8p8t8x8|8
848D8H8L8d8t8x8|8
848W8
849@9S9e9
849@9V9
849D9
849g9
849Q9
849Y9m9w9
858i8|8
858o8v8
868;8A8j8
868I8
869a9
869J9
878A8_8
878r8
879\9
879R9\9y9
879z:
8'8,8:8?8M8R8W8l8{8
8'8,8?8Q8n8
8'8:8E8b8m8
8'8@8
8-8<8G849Z9
8'8<8P8i8t8
8'808p8T9
888`8
888=8I8U8
888A8h8q8
888D8d8p8x8
888H8
888H8T8t8|8
888p8
889D9R9X9^9h9
889h9
889i9
889S9
889X9
889x9
8'8D8
8'8E8S8
8-8m8
8-8P8f8
8'8s8|8
8'9:9U9i9
8-9?9V9
89}4w
8'9}9
8-9>9
8-929l9
8-949;9F9
898[8n8|8
898C8`8
898r8
899\9
899|9
899a9
899C9`9
899C9X9m9
899O9
899R9e9
899R9f9
8-9A9
8-9A9}9
8-9J9e9
8'9w9
8a8f8
8a8h8
8A8k8
8a8v879_9x9
8A9e9
8A9t9
8a9t9
8b<n<z<
8B8a8
8B8b8
8B8L8e8
8B8Z8
8b9{9V:
8B9e9
8B9o9
8C8i8
8c8m8
8c8u8
8C9^9h9
8C9P9{9
8c9u9
8C9U9v9{9
8D8h8
8D8i8
8D8K9r9
8D9\9i9
8d9G:*;
8d9p9
8d9x9
8E8{8
8E8-9
8E8Q8]8
8E8T8d9
8F,t,j
8f8k8
8F8o8y8
8f9p9
8F9P9_9{9
8G8[8d8y8
8giP9giP9giP9giP9
8h Q3
8h8C9>:\:e:}:
8H8h8
8H8x8
8H8X8d8l8
8H9`9g9
8H9L9P9T9l9p9
8H9W9k9
8I8q8
8I9|9
8I9c9
8j9.:c:
8J9l9
8J9T9
8K8U8
8k9.:
8K9b9
8K9s9
8L:m:
8l8w8
8L9J:Y:}:
8L9Y9B:
8M:^:
8M;_<
8M<t(
8m8z8
8M9@;
8N:X:n:
8N9f9
8N9X9m9
8o:g;q;
8o8q9
8O8Y8v8
8O9v9
8P8|8
8P8t8
8q:v:
8q8v8
8Q9k9u9
8R9{9
8R9W9V:
8S8e8
8S8s8
8S9`9
8T8q8
8T9!:m:
8u8;9
8U8_8
8U9}9
8U9p9
8-uJ@
8V9]:
8V9^9j9q9
8v99:
8V9d9
8W8a8
8W9o9y9
8W9s9
8X8q8
8Y:&;4;\;
8Y8a8
8Z9d9y9
9 :%:
9 :P:
9 :x:
9 ;>;Q;w;
9 9$9(9
9 9$9(9,9@9D9\9`9h9
9 9$9(9,9@9P9T9X9p9
9 9$9(9,9094989<9@9D9H9L9P9T9h9l9p9t9x9|9
9 9$9(9,9094989<9@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
9 9$9(9,9094989<9@9D9H9L9P9T9X9`9d9h9|9
9 9$9(9,909D9H9L9d9t9x9
9 9$9(9@9D9\9l9p9t9
9 9$9(9@9P9T9d9h9x9|9
9 9$9(9<9@9D9H9P9h9x9
9 9$9,9D9T9X9\9t9
9 9$9<9@9D9L9d9h9l9
9 9$9<9L9P9T9X9\9`9d9h9l9p9t9x9|9
9 9$94989H9L9P9h9l9p9
9 9$94989P9`9d9t9x9|9
9 9$989<9L9P9h9l9p9
9 9%939@9L9V9t9{9
9 9(9,90949@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
9 9(90989@9H9T9t9|9
9 9(989@9H9P9X9`9p9
9 9,9L9T9\9d9l9t9
9 9,9L9T9\9h9
9 9@9H9P9\9|9
9 9@9H9P9X9`9l9
9 909@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
9 909@9P9`9p9
9 9094989<9P9T9X9\9d9h9l9t9x9|9
9 9094989L9P9T9\9`9t9x9|9
9 90949D9H9`9p9t9
9 959W9o9
9 969L9
9 979R9{9
9 989H9L9P9h9l9p9x9|9
9 989H9X9\9`9d9h9|9
9 9-949A9K9j9
9 9D9L9T9\9d9l9t9|9
9 9f=m=
9 9H9
9 9H9P9T9X9`9d9h9l9
9 9K9Q9i9x9
9!:*:H:
9!:';
9!:8:=:H:U:a:k:
9!9(959=9
9!919A9Q9a9q9
9!969
9!979
9!989Q9{9
9!9G9[9c9{9
9!9h9
9!9I9
9!9P9^9B:
9":{:
9":8:l:
9":b:_;x;
9":X:q:
9";5;X;i;
9"9(9.949:9@9F9L9R9X9^9d9j9p9v9|9
9"9:9I9
9"979L9c9q9
9#:]:
9#:5:V:[:a:
9#:c:u:
9#:E:t:y:
9#:h:
9#:i;
9#:U:s:
9#:U:x:
9#:v:}:
9#9(9
9#9(939@9L9V9`9l9s9
9#9<9M9`9m9
9#949E9V9g9x9
9#959]9b9h9n9
9#9D9}9
9#9V9e9
9$:,:0:4:<:@:D:H:p:x:|:
9$:[:
9$:+;9;J;y;
9$:0:C:U:v:{:
9$:G:z:
9$:J:b:
9$:R:
9$:T:
9$9(989<9@9H9L9`9d9h9
9$9(989<9L9P9`9d9t9x9
9$9)9
9$9*:
9$9,949@9H9h9
9$9,949<9D9L9T9\9d9l9|9
9$9,949<9D9L9T9\9d9l9t9
9$9,949<9D9L9T9\9d9l9t9|9
9$9,949<9D9L9T9\9d9l9x9
9$9,949<9D9L9T9\9d9p9
9$9,949<9D9L9T9\9h9
9$9,949<9D9L9X9x9
9$9,949<9H9h9p9
9$9,949<9H9P9p9x9
9$9,949<9P9X9`9h9|9
9$9,989\9|9
9$9,989X9`9h9
9$9,989X9`9h9p9|9
9$9,989X9`9h9t9
9$9,9L9h9x9
9$9,9L9T9\9d9
9$9,9T9\9d9l9t9|9
9$9@9h9
9$9<9H9h9p9x9
9$9<9L9P9T9l9p9
9$9<9T9`9
9$9=9
9$9>9^9
9$909@9P9`9h9x9
9$909\9d9
9$909P9X9`9h9p9x9
9$909P9X9h9
9$949<9h9
9$94989H9L9\9l9p9t9x9
9$989@9E9K9]9h9
9$9A9i9
9$9D9`9
9$9D9`9p9|9
9$9D9L9T9\9h9p9
9$9D9P9p9|9
9$9p9
9$9W9e9
9$u?A
9%:9:_:s:
9%;*;
9%9,9E9V9v9
9%9?9
9%9?9I9b9s9&;
9%9>9O9
9%909<9v9}9
9%919J9P9T9a9}9
9%939?9K9X9e9r9~9
9%9C9
9%9F9K9Q9p9
9%9F9M9T9[9
9%9J9a9
9&:0:U:l:
9&:r:|:
9&9+919P9
9&9=9w9
9&9>9J9h9
9&9>9Q9q9
9&969?9S9[9c9k9t9}9
9&9-9:9B9H9L9R9q9y9
9&999I9
9&9b9
9&9f9v9c:
9&9K9b9o9
9(:@:J:O:Z:h:}:
9(:`:
9(:1:h:v:
9(:1:X:a:
9(:2:f:m:
9(:K:
9(:t:
9(9,9094989<9@9D9H9L9P9T9X9\9`9h9
9(9,90989P9T9l9p9t9
9(9,9D9H9`9d9|9
9(9{9
9(90989@9H9T9t9|9
9(90989D9d9l9t9
9(949T9\9d9l9t9
9(949T9`9
9(989<9@9T9X9p9
9(989<9T9X9\9p9
9(989H9X9h9x9
9(9-9:9Y9
9(9-959:9
9(9b9i9
9(9C9U9v9{9
9(9g9n9
9(9H9P9X9`9h9p9|9
9(9K9
9(9L9T9\9d9l9t9|9
9(9m93<C<v<
9(9v:
9(9V9
9):D:_:
9):M:
9)9:9Q9
9)93989A9F9\9b9s9
9)9d9
9)9n9
9)9q9{9
9)9s9
9*:1:::?:J:O:W:\:h:u:
9*:7:f:u:
9*:E:z:
9*:F:V:
9*9;9R9
9*9>:E:^:
9*919J9c9
9*9A9
9*9E9{9
9*9O9
9,:3:::E:
9,:9:G;
9,:E:
9,9?9
9,9<9@9P9T9l9p9t9x9|9
9,9<9@9X9\9`9d9l9
9,9<9@9X9\9p9t9
9,9<9@9X9h9l9|9
9,909@9D9\9l9p9x9
9,9094989@9D9X9\9`9x9
9,9094989@9X9\9`9h9l9p9t9|9
9,90949H9X9\9`9d9x9|9
9,90949H9X9h9x9|9
9,949@9`9h9p9x9
9,949@9`9h9t9
9,949<9D9L9\9h9
9,949<9D9L9T9\9d9l9t9|9
9,989X9d9
9,9a9h9
9,9n9
9,9U9_9t9
9,9U9J:
9.:f:z:
9.:o:
9.9A9N9o9
9.9Y9
9/:~:
9/95999C9a9q9w9
9/9G9
9/9U9~9
9':;:V:j:
9:9o9
9':r:
9;9y9
9?:r:
9?9I9p9
9@:q;
9@9c9m9w9
9@9H9L9P9X9\9`9d9
9\$4r3
9\9r9
9] |]
9] |`
9] |W
9]=~=
9]9j9
9]9q9
9^ u_
9^$~C
9^$uzQQ
9^(v)
9^(v_
9^,|0
9^,vc3
9^:5;
9^:t:
9^LtO
9_ u%
9_(s3Sj
9_,~6
9`:d:h:l:p:t:x:|:
9`; =x>
9{;1<D<W<]<
9{Hw@
9|$$uz
9}$v'
9}$v&
9}(|-
9}(|.
9~$|O
9~$v\
9~$v}
9~$v9
9~,~@
9+:?:j:
9+:@:F:^:m:
9+:0:,>
9+:J:i:
9+:X:
9+:Z:p:
9+9;9K9d9u9
9+9@9
9+9^9z9
9+90969A9e9
9+9O9V9o9
9<:\:
9<:^;c;
9<2sX
9<9\9d9l9t9|9
9<9|9
9<9D9L9T9\9d9l9t9|9
9<9D9L9T9\9d9l9x9
9<9e9
9<9F9Y9i9
9<9H9h9p9x9
9<9R9W9~9
9=:P:s:
9=:Q:m:
9=:x:
9=;B;P>U>
9=9D9\9o9
9=9G9V9v9
9>:o:
9>:O:q:
9>9t9
9>9Z9~9
90:R:q:
90:X:
909:9^9
909:9S9
909@9P9T9X9p9
909\9
909`9
909|9
9094989L9\9`9d9|9
90949L9P9T9h9x9|9
90989<9@9H9L9P9T9|9
90989D9d9l9x9
909A9P9c9v9
909k9u9
909P9
90S0l0x1
90t(j0h
90t>P
90uMiH
91:8:Q:t;{;
919e9H:
919M9i9
919t9
919V9s9
92:]:x:
92;q;x;
92;T;[;t;
929<9U9
929S9w9
929Z9
93:=:O:q:
93:8:
93:E:f:k:q:
93:g:
93:I:
93:s:
93:w:
93;E;];b;q;
939^9
939=9I9W9l9
939D9g9z9
939E9m9r9x9~9
94:;:o:
94:@:V:
94:i:
94:Q:
94:x:
949<9D9L9T9\9d9l9t9|9
949<9D9L9T9\9h9
949<9D9L9T9`9
949<9D9P9p9x9
94989<9D9\9l9p9
94999?9E9K9
949O9q9
95:l:v:
95;)<
959:9a9
959:9E9R9^9h9
959H9
959R9f9s9}9
96:f:
961c151d2e87f2686a955a9be24d316f1362bf21 3.7.1
969;9A9`9
969;9A9i9
969E9
969I9o9
969M9k:
97:>:V:i:
97:R:
979_9
98:f:
98:O:m:
98:p:
98:S:
98:W:^:w:
98:x:
98:X:`:h:t:
98:z:
989@9D9H9P9T9X9\9
989@9H9P9d9l9t9
989@9H9P9X9d9
989@9H9P9X9d9l9
989A9r9
989H9l9t9|9
989H9T9t9|9
989I9`9m9
989Q9h9|9
989v9
98uHj
9'9<9S9e9
9-9>9V9
9'969;9
999[9e9z9
999^9
999j9z9
999M9V9h9
999w9
9-9B9T9l9
9'9C9Z9w;
9'9d9
9-9F9U9
9-9M9
9'9s9
9-9U9Y9]9x9
99um9q
9a::<
9A:F:
9A:K:
9A:r:
9a:X=y=
9b:{:
9b:}:
9B:I:b:
9B:I:b:s:
9B9e9
9B9I9a9
9B9L9l9
9b9v9
9c:#;~;3<
9C9f9t9
9C9H92:
9C9J9c9
9C9M9^9
9c9u9
9CHw@
9CHwC
9D$,u_
9d:x:
9D9I:w:
9D9n9
9d9v9
9E:|:
9e:o:
9E:O:
9F$tA
9F,t&
9F:R:
9F\t&
9F9|9
9G,t.+G,
9G:r:
9G:s:%;
9G9_9
9g9c;m;
9G9m9w9
9g9q9
9G9Q9
9G9Q9j9
9G9x9
9H t!
9H:`:g:|:
9H:p:
9h9|9
9H9k9
9HLvB
9I:o:
9I:v:
9I9p9
9J ue
9J:r:
9J9[9
9K9c9
9L:S:h:}:
9L:V:o:
9M(vu
9M,||
9m9t9
9m9z9
9N:p:z:
9p u"
9P9`9l9t9
9P9m9
9pHt*
9q u#
9q u$
9Q:a:q:
9Q:r:
9Q9|9
9Q9v9
9r9,:
9r9y9
9s$~&Vj
9S:k:~:
9S9b;
9S9e9
9T$$u
9t$(vU
9T$@u
9t$0u
9T:\:d:l:x:
9t:{:
9T:x:
9T9q9
9U ur9M$um
9u(v7+M
9u(vZ
9u,|>
9u,|a
9u4wn
9u4wt
9V9~9N:Z:
9w,~D
9W:w:
9W@RR
9w0tM
9W9m9
9X u+
9x:@;
9X9_9f9m9u<
9X9p9
9Y u%9_ u
9Y u:
9Z:o:
9zLvr
A != nullptr && B != nullptr
A ;A$u
A + (M * K) <= A_end
A + (M * lda - (lda - K)) <= A_end
A 0-D tensor containing a single value corresponding to the number diagonals above or the main diagonal to exclude or include.Default value is 0 if it's not specified.
A 1-D input tensor that is to be processed.
A 1-D INT64 tensor containing the the count of each element of 'uniques' in the input 'x'
A 1-D INT64 tensor of the same size as 'x' containing the indices for each value in 'x' in the output 'uniques'
A 1-D tensor of the same type as 'x' containing all the unique values in 'x' sorted in the same order that they occur in the input 'x'
A 1-D values of (height, width).
A 1-D values of (leftBorder, topBorder, rightBorder, bottomBorder).
A 2D Matrix that represents the distance between each pair of the two collections of inputs.
A collection of intercepts.
A collection of weights of the model(s).
A Conv/ConvTranspose node has both 'auto_pad' and 'pads' attributes
A dimension cannot be less than -1, got 
A dso with name 
A float.
A high-performing neural network activation function.The GELU nonlinearity is
A list of 2 (or 4 if bidirectional) activation functions for update, reset, and hidden gates. The activation functions must be one of the activation functions specified above. Optional: See the equations for default if not specified.
A list of 3 (or 6 if bidirectional) activation functions for input, output, forget, cell, and hidden. The activation functions must be one of the activation functions specified above. Optional: See the equations for default if not specified.
A list of floats.
A list of integers, along which to reduce. The default is to caculate along axes [0,2,3] for calculating mean and variance along each channel. Two variables with the same C-coordinate are associated with the same mean and variance.
A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor.
A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data).
A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given.
A list of ints.
A list of labels.
A list of strings. One and only one of 'keys_*'s should be set.
A list of strings. One and only one of 'value_*'s should be set.
A node with a function body within a subgraph within another function body is currently not supported in ORT
A shape tensor must be a vector tensor.
A string indicating the desired element type of the output tensor, one of 'TO_FLOAT', 'TO_STRING', 'TO_INT64'.
A string to use when an input integer value is not found in the map.<br>One and only one of the 'default_*' attributes must be defined.
A string vocabulary array.<br>One and only one of the vocabularies must be defined.
A string.
A value that needs replacing.
A$+A 
A$G+A 
A,+A(
A,+A(j
A\+AX
a_scale
A_scale
A_zero_point
a_zero_point
A`+A\j
A|+Ax
A|+Axjt
A|+AxVW
A<+A8j
A0;A4u6
A0W;A4t
a0w0[2`2
A8+A4
A8+A4P
A88QP
Access violation - no RTTI data!
ACLExecutionProvider
Acosh
AcquireSRWLockExclusive
across_channels
activation
activation and leaky_relu_alpha.
activation.
activation_
activation_alpha
activation_beta
activation_func_names.size() == static_cast<size_t>(num_directions_) * 2
activation_func_names.size() == static_cast<size_t>(num_directions_) * 3
activation_gamma
activation_params
activation_params count mismatch
activation_size
ActivationDescCount
ActivationDescs
activations
activations, avoiding computation if the input is invalid (as in, the
activations_.size() == static_cast<size_t>(num_directions)
adapterLuidHighPart
adapterLuidLowPart
add_B_tensor_proto
AddFoldedRange recurses too much.
Adding default CPU execution provider.
AddInitializedTensor already has tensor with name 
addition
Additional elements added to the side with higher coordinate indices in the output. Each padding value in "output_padding" must be less than the corresponding stride/dilation dimension. By default, this attribute is a zero vector. Note that this attribute doesn't directly affect the computed output values. It only controls the selection of the computed values, so changing this attribute only adds or removes output elements. If "output_shape" is explicitly provided, "output_padding" does not contribute additional size to "output_shape" but participates in the computation of the needed padding amount. This is also called adjs or adjustment in some frameworks.
address family not supported
address in use
address not available
Adlam
Af98u
Affine
affine
Affine takes one input data (Tensor<T>) and produces one output data
aggregate_function
AH;A@
Ah+Ad
AHRPV
ai.onnx
ai.onnx.ml
ai.onnx.preview.training
ai.onnx.training
AL;AD|
Al+Ah
align_corners
All implicit inputs should have OrtValue instances by now. 
All inputs must have the same shape
All inputs to Concat must have same rank
All inputs to 'Range' op must be of the same type
All nodes have been placed on [
All scan outputs MUST be tensors
All Tensor and Sequence types
All Tensor types
all types
Allocated memory at 
Allocation of memory pattern buffer for 
Allocation of tensor types requires a shape.
allocator
allocator != nullptr
Allocator already registered for 
Allocator with this OrtMemoryInfo is already registered.
allocator_ptr_
AllocPlan(ml_value_idx).program_counter.Ends().back() == program_counter
ALLOW_RELEASED_ONNX_OPSET_ONLY
allowed_activations.find(activations_[direction]) != allowed_activations.end()
allowed_directions.find(direction_) != allowed_directions.end()
alpha
Alpha
alpha == 1.0f && (beta == 0.0f || beta == 1.0f)
alpha_ > 0.0f
already connected
An attribute specifying the number of scan_inputs M. 
An axes tensor must be a scalar or a 1-D tensor.
An axes tensor must be a vector tensor.
An axis tensor must be a scalar tensor.
An input tensor to hash.
An integer to use when an input string value is not found in the map.<br>One and only one of the 'default_*' attributes must be defined.
An integer vocabulary array.<br>One and only one of the vocabularies must be defined.
An integer.
An optional list of K flags, one for each scan_output. The i-th element of the list specifies whether the i-th scan_output should be constructed by appending or prepending a new value in each iteration: 0 indicates appending and 1 indicates prepending. If omitted, all scan_output tensors will be produced by appending a value in each iteration.
An optional list of K flags. The i-th element of the list specifies the axis for the i-th scan_output. The scan outputs are accumulated along the specified axis. If omitted, 0 will be used as the scan axis for every scan_output.
An optional list of K flags. The i-th element of the list specifies the axis for the i-th scan_output. The scan outputs are accumulated along the specified axis. If omitted, 0 will be used as the scan axis for every scan_output. Negative value for an axis means counting dimensions from the back. Accepted range is [-r, r-1].
An optional list of M flags. The i-th element of the list specifies the axis to be scanned (the sequence axis) for the i-th scan_input. If omitted, 0 will be used as the scan axis for every scan_input.
An optional list of M flags. The i-th element of the list specifies the axis to be scanned (the sequence axis) for the i-th scan_input. If omitted, 0 will be used as the scan axis for every scan_input. Negative value for an axis means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
An optional list of M flags. The i-th element of the list specifies the direction to be scanned for the i-th scan_input tensor: 0 indicates forward direction and 1 indicates reverse direction. If omitted, all scan_input tensors will be scanned in the forward direction.
an optional list of strings attribute that contains a list of separators - regular expressions to match separators Two consecutive segments in X connected by a separator would be divided into two tokens. For example, if the input is "Hello World!" and this attribute contains only one space character, the corresponding output would be ["Hello", "World!"]. To achieve character-level tokenization, one should set the 'separators' to [""], which contains an empty string.
An optional string. Token's regular expression in basic POSIX format (pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_03). If set, tokenizer may produce tokens matching the specified pattern. Note that one and only of 'tokenexp' and 'separators' should be set.
An OrtValue for this name has already been added.
An split tensor must be a vector tensor.
Anatolian_Hieroglyphs
Another operand has a dim value of 
Ap+Al
api-ms-win-core-com-l1-1-0.dll
api-ms-win-core-debug-l1-1-0.dll
api-ms-win-core-delayload-l1-1-0.dll
api-ms-win-core-delayload-l1-1-1.dll
api-ms-win-core-errorhandling-l1-1-0.dll
api-ms-win-core-fibers-l1-1-0.dll
api-ms-win-core-file-l1-1-0.dll
api-ms-win-core-file-l1-2-0.dll
api-ms-win-core-handle-l1-1-0.dll
api-ms-win-core-heap-l1-1-0.dll
api-ms-win-core-interlocked-l1-1-0.dll
api-ms-win-core-libraryloader-l1-2-0.dll
api-ms-win-core-localization-l1-2-0.dll
api-ms-win-core-memory-l1-1-0.dll
api-ms-win-core-path-l1-1-0.dll
api-ms-win-core-processenvironment-l1-1-0.dll
api-ms-win-core-processthreads-l1-1-0.dll
api-ms-win-core-processthreads-l1-1-1.dll
api-ms-win-core-processthreads-l1-1-3.dll
api-ms-win-core-processtopology-obsolete-l1-1-0.dll
api-ms-win-core-profile-l1-1-0.dll
api-ms-win-core-rtlsupport-l1-1-0.dll
api-ms-win-core-string-l1-1-0.dll
api-ms-win-core-synch-l1-1-0.dll
api-ms-win-core-synch-l1-2-0.dll
api-ms-win-core-sysinfo-l1-1-0.dll
api-ms-win-core-sysinfo-l1-2-0.dll
api-ms-win-core-util-l1-1-0.dll
api-ms-win-core-winrt-error-l1-1-0.dll
api-ms-win-core-winrt-error-l1-1-1.dll
api-ms-win-core-winrt-string-l1-1-0.dll
api-ms-win-crt-locale-l1-1-0.dll
api-ms-win-crt-math-l1-1-0.dll
api-ms-win-crt-private-l1-1-0.dll
api-ms-win-crt-runtime-l1-1-0.dll
api-ms-win-crt-string-l1-1-0.dll
api-ms-win-eventing-provider-l1-1-0.dll
apply softmax to elements for dimensions softmax_axis or higher
APQRV
APQVW
AQQPVQh
AQQQh
AQQQj
AQQQQPQhP
AQQQQWRQh 
AQQQRSQh
AQQQRSQhl
AQQQRVQSRQQQRVQh
AQQRh
AQQRh,
AQQRSQh
AQQSPh
AQQSPh<@?
AQQSPQh
AQQSRQh
AQQSWQh(
AQQWRQh
AQQWRQh(j?
AQQWRQPj
AQRQQj
AQSQQSRQh8
Arabic
arg_num < arg_counts.size()
ArgMax
ArgMin
Argument is not a tensor
argument list too long
Argument mismatch when removing edge.
argument out of domain
Argument type mismatch when adding edge.
Armenian
ArmNNExecutionProvider
array
Array of sequence lengths.  len(seq_lengths) should equal batch size N.
ArrayFeatureExtractor
AScaleTensor
Asinh
asymmetric
At least one element in the sequence is of a type different from others.
At least one output should be requested.
At most one dimension can be -1.
AT+AP
At+Ap
Atanh
ATensor
Attempt to retrieve final output before it was set.
Attempt to use DefaultLogger but none has been registered.
Attempted a typeid of nullptr pointer!
Attempting to broadcast an axis by a dimension other than 1. 
Attempting to get an input that does not exist.
Attempting to get an output that does not exist.
Attempting to get index for an input which does not exist.
Attention
Attention layer weight shape error! Expected: {
Attention mechanism memory sequence lengths must have shape {
Attention mechanism memory sequence lengths value must in (0, 
Attention mechanism memory shape error! Expected: {
Attention memory layer weight shape error! Expected:{
Attention query layer weight shape error! Expected:{
Attention v weight shape error! Expected:{
AttentionFusion
Attibute name and type don't match
AttnLSTM
Attribute 
Attribute '
Attribute (name: 
Attribute `broadcast=1` needs to be passed to enable broadcasting.
Attribute axes has incorrect length
Attribute blocksize is not set.
Attribute border needs to be specified with four border elements, got 
attribute case_change_action has invalid value
attribute case_change_action is not set
Attribute dilations has incorrect size
Attribute dtype should be of integer type and specify a type.
Attribute expected to have a one-dim tensor
Attribute expected to have tensor type
attribute is_case_sensitive is not set
Attribute kernel_shape has incorrect size
Attribute kernel_shape has incorrect size.
Attribute kernel_shape must be specified
Attribute kernel_shape must be specified.
attribute mark is not set
attribute mincharnum is not set
attribute mincharnum must have a positive value
Attribute name and type don't match for '
attribute pad_value is not set
Attribute pads has incorrect length
Attribute pads has incorrect size
Attribute pads has incorrect size.
Attribute perm of Transpose has an invalid value. Value 
Attribute pooled_shape has incorrect length
Attribute pooled_shape must be specified
Attribute 'scales' is required.
Attribute 'scales' must have floats type.
Attribute shape is not set.
Attribute specification type mismatch.
Attribute strides has incorrect size
Attribute strides has incorrect size.
Attribute to is not set.
Attribute value for pads is required
Attribute 'value' of Constant node must exist with 'Tensor' data.
Attribute 'value_float' expect a float.
Attribute 'value_floats' expect a list of floats.
Attribute 'value_int' expect an integer.
Attribute 'value_ints' expect a list of integers.
Attribute 'value_string' expect a string.
Attribute 'value_strings' expect a list of strings.
Attribute: 
auto_pad
auto_pad == AutoPadType::NOTSET
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = ceil(input_shape[i] / strides[i])` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER.
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = input_shape[i] * strides[i]` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER.
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that the output size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the beginning for SAME_LOWER. VALID mean no padding. DEPRECATION NOTE: auto_pad is only intended to support legacy uses, and for framework authors, one is explicitly encouraged to use explicit padding specified in the pads attribute.
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that the output spatial size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the beginning for SAME_LOWER. VALID mean no padding.
Available memory of 
AVERAGE
AveragePool
Avestan
AWQQWRQPj
axes as an input and attribute cannot be specified at the same time.
'axes' attribute must not contain any duplicates
'axes' has a duplicate axis
'axes' has an axis outside of the tensor dimension count
'axes' has an out of range axis
'axes' has duplicates
Axes input is null
Axes that `starts` and `ends` apply to. It's optional. If not present, will be treated as [0, 1, ..., len(`starts`) - 1].
axes_right_stride >= 0 && static_cast<uint64_t>(axes_right_stride) < std::numeric_limits<size_t>::max()
axes_tensor != nullptr
axes_tensor->Shape().NumDimensions() == 0 || axes_tensor->Shape().NumDimensions() == 1
axes_tensor->Shape().NumDimensions() == 1
axis 
axis <= X_NumDims && axis >= -X_NumDims
axis == 1 || axis == largest
axis >= -tensor_rank && axis <= tensor_rank - 1
Axis has less than the requested k elements.
'axis' must be in [
axis must be in [-r, r-1]
'axis' must be in [-rank(indices), rank(indices)-1]
'axis' must be in [-rank(indices)-1, rank(indices)]
axis must be in [-rank, rank-1].
axis must be in [-rank, rank-1]. input rank was 
Axis must be within range [
axis tensor can be int32 or int64 only
Axis tensor must be provided to the CumSum op
Axis tensor should be 0D or 1D
Axis tensor should be of type `int32_t` or `int64_t`
axis_tensor->Shape().IsScalar()
AxisCount
AxisDirection
AZeroPointTensor
B + (N * ldb - (ldb - K)) <= B_end
B SVW
B(90t:
B(9J,u
B,+J0+B(
B_scale
b_scale
B_zero_point
b_zero_point
B|+Bx
B09J4u
B0V0u0
B1.2&3
B8Wdur
bad address
bad allocation
Bad arg in kInstAltMatch: 
Bad arg in kInstCapture: 
Bad args: nsubmatch=
bad array new length
Bad call to ParseState::ParsePerlFlags
bad cast
bad conversion
bad dimensions
bad exception
bad file descriptor
Bad final char: 
bad function call
Bad hex digit 
bad locale name
bad message
Bad node spec: 
bad optional access
Bad optional access
Bad read pointer - no RTTI data!
Bad reference count 
bad repetition operator
Balinese
Bamum
Base values for classification, added to final class score; the size must be the same as the classes or can be left unassigned (assumed 0)
base_values
Bassa_Vah
Batak
Batch dimension should match for MatMul;
batch_axis
batch_axis != time_axis
batch_axis < 2
batch_dims
batch_indices
batch_indices shape input tensor has wrong dimension
batch_size is < 1
BatchDimensionCount
BatchIndicesTensor
BatchNormalization
BatchNormalizationAddFusion
BatchNormalizationMulFusion
Begin execution
Bengali
beta is expected to have 1 dimension, got 
beta is expected to have 1 dimensions, got 
beta is expected to have size of 
Beta should be of shape (hidden_size). 
beta_ > 0.0f
Beta1
Beta2
bfloat16
Bhaiksuki
Bias applied to each channel, same size as C.
Bias Gelu.
bias is expected to have 1 dimension, got 
Bias size (
Bias tensor.
BiasGelu
BiasGeluFusion
BiasSoftmax
BiasSoftmaxFusion
BiasTensor
bidirectional
bilinear
BILINEAR
Binarizer
BinForSize(bin_size * 2 - 1) == BinFromIndex(b)
BinForSize(bin_size * 2) != BinFromIndex(b)
BinForSize(bin_size + 255) == BinFromIndex(b)
BinForSize(bin_size) == BinFromIndex(b)
BinFromIndex(c->bin_num)->free_chunks.erase(h) > 0
BitShift
Blocks of [blocksize, blocksize] are moved.
blocksize
BlockSize
Blocksize must be positive
bn_B_tensor_proto
bn_mean_tensor_proto
bn_scale_tensor_proto
bn_var_tensor_proto
Bool to determine if hidden state is zeroes or passed along for timesteps past the given sequence_length.
boolean
Boolean whether to mark the beginning/end character with start of text character (0x02)/end of text character (0x03).
Boolean. Indicates whether upper or lower part of matrix is retained. Default is true.
Boolean. Whether the identification of stop words in X is case-sensitive. Default is false
Bopomofo
border
'Border' attribute must be present and must contain exactly 4 values - (left_border, top_border, right_border, bottom_border)
Both `data` and `indices` input tensors in GatherND op need to have rank larger than 0.
both data and indices tensor need to have rank larger than zero.
boxes
boxes and scores should have same num_batches.
boxes and scores should have same spatial_dimension.
boxes must be a 3D tensor.
boxes_tensor
Bp+Bl
Brahmi
Braille
BRANCH_EQ
BRANCH_GT
BRANCH_GTE
BRANCH_LEQ
BRANCH_LT
broadcast
broadcast bias across input for dimensions broadcast_axis to softmax_axis-1
Broadcast Output range [
broadcast_axis
BroadcastLooper requires two tensors as input.
broken pipe
BRRQWQh<
BRRRPQh
BRRRSQQh
BRRRVSQh
BRRRWQh
BRRSQQh
BRRSQQh$
BScaleTensor
BTensor
Buffer containing the initializer must be owned by the user.
buffer size is too small for string
buffers_.find(location) == buffers_.end()
BUG! Report to onnxruntime team.
Buginese
Buhid
bumped the operator version but 
BWRRWSQh
by either re-generating the model with latest exporter/converter 
bytemap range 
BZeroPointTensor
C + (M * ldc - (ldc - N)) <= C_end
C = ((A - A_zero_point) * (B - B_zero_point)) * (A_scale * B_scale)/C_scale + C_zero_point
C = (A_scale * (A - A_zero_point) + B_scale * (B - B_zero_point))/C_scale + C_zero_point
C$V;C(t
C(;C,u
C,;C,
C,+C(
C,+C(3
C,+C(j
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\controlflow\defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\controlflow\old.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\generator\defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\generator\old.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\logical\defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\logical\old.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\math\defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\math\old.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\nn\defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\nn\old.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\object_detection\defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\object_detection\old.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\quantization\defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\quantization\old.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\reduction\defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\reduction\old.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\rnn\defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\rnn\old.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\sequence\defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\tensor\defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\tensor\old.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\traditionalml\defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\onnx\onnx\defs\traditionalml\old.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\protobuf\src\google/protobuf/parse_context.h
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\arena.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\generated_message_util.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream_impl.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream_impl_lite.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\message_lite.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\repeated_field.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\re2\re2/walker-inl.h
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\re2\re2\bitstate.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\re2\re2\compile.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\re2\re2\dfa.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\re2\re2\nfa.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\re2\re2\onepass.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\re2\re2\parse.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\re2\re2\prog.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\re2\re2\re2.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\re2\re2\regexp.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\re2\re2\simplify.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\re2\re2\tostring.cc
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\wil\include\wil/wrl.h
C:\apilot\agent\_work\5\s\engine\lotus\cmake\external\wil\include\wil\resource.h
C:\apilot\agent\_work\5\s\engine\lotus\include\onnxruntime\core/common/const_pointer_container.h
C:\apilot\agent\_work\5\s\engine\lotus\include\onnxruntime\core/common/logging/logging.h
C:\apilot\agent\_work\5\s\engine\lotus\include\onnxruntime\core/framework/data_types.h
C:\apilot\agent\_work\5\s\engine\lotus\include\onnxruntime\core/framework/data_types_internal.h
C:\apilot\agent\_work\5\s\engine\lotus\include\onnxruntime\core/framework/ml_value.h
C:\apilot\agent\_work\5\s\engine\lotus\include\onnxruntime\core/framework/op_kernel.h
C:\apilot\agent\_work\5\s\engine\lotus\include\onnxruntime\core/framework/tensor.h
C:\apilot\agent\_work\5\s\engine\lotus\include\onnxruntime\core/graph/graph.h
C:\apilot\agent\_work\5\s\engine\lotus\include\onnxruntime\core/optimizer/graph_transformer.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops/cpu/activations.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops/cpu/crop_and_resize.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\bahdanau_attention.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\deep_cpu_attn_lstm.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\deep_cpu_attn_lstm.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\attention.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\attention_cpu_base.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\bias_gelu.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\embed_layer_norm.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\cdist.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\cpu_contrib_kernels.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\crop.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\element_wise_ops.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\expand_dims.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\fused_conv.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\fused_gemm.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\image_scaler.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\layer_norm.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\matmul_integer16.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\maxpool_with_mask.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\murmur_hash3.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_binary_op.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_global_average_pool.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_lookup_table.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\attention_quant.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\dynamic_quantize_matmul.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\nhwc_max_pool.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\skip_layer_norm.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\tokenizer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\trilu.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\trilu.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\contrib_ops\cpu\word_conv_embedding.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/common/safeint.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/framework/bfc_arena.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/framework/execution_frame.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/framework/execution_providers.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/framework/feeds_fetches_manager.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/framework/func_kernel.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/framework/mem_pattern_planner.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/framework/node_index_info.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/framework/op_kernel_context_internal.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/framework/ort_value_tensor_slicer.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/framework/sequential_execution_plan.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/framework/TensorSeq.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/graph/function_impl.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/graph/model_load_utils.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/optimizer/attention_fusion_helper.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/optimizer/initializer.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/platform/path_lib.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/common.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/activation/activations.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/controlflow/scan_utils.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/element_wise_ranged_transform.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/generator/constant_of_shape_base.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/generator/random.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/math/clip.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/math/element_wise_ops.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/math/gemm.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/math/matmul_helper.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/ml/cast_map.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/ml/category_mapper.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/ml/dictvectorizer.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/ml/feature_vectorizer.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/ml/label_encoder.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/ml/ml_common.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/ml/normalizer.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/nn/batch_norm.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/nn/conv_attributes.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/nn/conv_transpose_attributes.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/nn/dropout_op.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/nn/flatten.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/nn/instance_norm.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/nn/lp_norm.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/nn/lrn.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/nn/pool_attributes.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/nn/pool_base.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/nn/roi_pool.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/nn/shrink.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/nn/unpool.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/reduction/reduction_ops.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/deep_cpu_gru.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/rnn.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/rnn_helpers.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/concat.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/gather.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/identity_op.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/mean_variance_normalization.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/pad.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/reshape.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/slice.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/space_depth_ops.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/split.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/squeeze.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/transpose.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/unsqueeze.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/upsample.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/utils.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/MLOperatorAuthorHelper.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/OperatorHelper.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/SchemaInferenceOverrider.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core/session/inference_session.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\common\logging\logging.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\common\path.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\common\profiler.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\common\status.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\common\str_helper.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\common\threadpool.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\flatbuffers\flatbuffers_utils.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\allocation_planner.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\allocator.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\allocatormgr.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\bfc_arena.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\data_transfer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\data_transfer_manager.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\data_types.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\endian_utils.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\ex_lib_loader.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\execution_frame.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\execution_provider.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\feeds_fetches_manager.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\fuse_nodes_funcs.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\graph_partitioner.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\kernel_registry.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\mldata_type_utils.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\node_index_info.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\op_kernel.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\op_kernel_info.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\op_node_proto_helper.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\ort_value_tensor_slicer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\parallel_executor.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\provider_bridge_ort.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\sequential_executor.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\session_options.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\session_state.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\session_state_utils.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\simple_tensor_allocator.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\tensor.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\tensor_allocator_with_mem_pattern.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\tensor_shape.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\tensor_type_and_shape.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\tensorprotoutils.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\framework\utils.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\graph\contrib_ops\contrib_defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\graph\contrib_ops\nhwc_schema_defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\graph\contrib_ops\quantization_defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\graph\dml_ops\dml_defs.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\graph\function.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\graph\graph.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\graph\graph_flatbuffers_utils.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\graph\graph_utils.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\graph\graph_viewer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\graph\model.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\graph\schema_registry.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\attention_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\bias_gelu_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\bias_softmax_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\common_subexpression_elimination.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\constant_folding.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\conv_activation_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\conv_add_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\conv_bn_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\conv_mul_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\dynamic_quantize_matmul_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\embed_layer_norm_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\fast_gelu_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\free_dim_override_transformer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\gelu_approximation.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\gelu_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\gemm_activation_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer_mgr.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer_utils.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\initializer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\insert_cast_transformer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\layer_norm_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\matmul_add_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\matmul_integer_to_float.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\matmul_scale_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\nhwc_transformer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\optimizer_execution_frame.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\relu_clip_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\reshape_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\rule_based_graph_transformer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\skip_layer_norm_fusion.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\transformer_memcpy.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\optimizer\unsqueeze_elimination.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\platform\windows\env.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\if.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\loop.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_8.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_9.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_utils.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\cpu_execution_provider.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\generator\constant_of_shape.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\generator\random.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\clip.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\cumsum.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\det.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_auxiliary_ops.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_compute_preprocessor.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_typed_compute_processor.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\element_wise_ops.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\gemm.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\gemm_helper.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\hardmax.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\matmul.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\matmul_integer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\quantize_linear_matmul.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\softmax.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\math\top_k.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\ml\cast_map.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\ml\feature_vectorizer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\ml\imputer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\ml\linearclassifier.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\ml\linearregressor.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\ml\ml_common.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\ml\onehotencoder.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\ml\scaler.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmclassifier.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmclassifier.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmregressor.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\ml\tree_ensemble_aggregator.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\ml\tree_ensemble_common.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\ml\zipmap.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv_integer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv_transpose.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\nn\instance_norm.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\nn\lrn.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\nn\pool.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\nn\qlinearconv.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\nn\roi_pool.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\nn\string_normalizer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\nn\tfidfvectorizer.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\nn\Unpool.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\non_max_suppression.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\non_max_suppression.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\roialign.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\reduction\reduction_ops.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\deep_cpu_gru.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\deep_cpu_lstm.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\lstm_base.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\lstm_base.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\rnn.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\rnn_helpers.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\sequence\concat_from_sequence.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\sequence\sequence_ops.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\cast_op.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\compress.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\concat.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\dynamicquantizelinear.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\eye_like.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_elements.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_elements.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_nd.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\isinf.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\nonzero_op.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\onehot.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\pad.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\quantize_linear.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\reshape_helper.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\reverse_sequence.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\reverse_sequence.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\scatter.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\scatter_nd.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\slice.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\space_depth_ops.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\split.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\tile.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\transpose.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\unsqueeze.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\upsample.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\dml_provider_factory.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\AbiCustomRegistry.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\BucketizedBufferAllocator.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\CommandAllocatorRing.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\CommandQueue.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DescriptorPool.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DmlCommandRecorder.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DmlCommon.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ExecutionContext.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ExecutionProvider.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/ApiHelpers.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/ApiTraits.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/GeneratedSchemaHelpers.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/SchemaHelpers.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\FusedGraphKernel.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GpuEvent.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphDescBuilder.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphPartitioner.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphTransformer.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\MLOperatorAuthorImpl.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\MLOperatorAuthorImpl.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperator.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorActivation.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorAffine.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorBatchNormalization.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCast.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConcat.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConstantOfShape.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConvInteger.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConvolution.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCopy.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCrop.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCumSum.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorDepthToSpace.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorDynamicQuantizeLinear.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorEinSum.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorElementWise.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorExpand.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorEyeLike.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorGemm.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorInstanceNormalization.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorLocalResponseNormalization.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorLpNormalization.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMatMul.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMatMulInteger.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMeanVarianceNormalization.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMemcpy.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorNeg.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorOneHot.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorPadding.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorPooling.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearAdd.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearConv.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearMatMul.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRange.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRecurrentNeuralNetwork.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorReduce.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorResize.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorReverseSequence.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRoiAlign.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRoiPooling.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorScatter.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSlice.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSpaceToDepth.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSplit.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTile.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTopk.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTranspose.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorValueScale2D.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\OperatorRegistration.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\OperatorUtility.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\PooledUploadHeap.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ReadbackHeap.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\TensorDesc.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\OperatorAuthorHelper\MLOperatorAuthorHelper.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\OperatorAuthorHelper\OperatorHelper.cpp
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\OperatorAuthorHelper\OperatorHelper.h
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\session\custom_ops.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\session\environment.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\session\inference_session.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\session\inference_session_utils.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\session\IOBinding.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\session\onnxruntime_c_api.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\session\ort_env.cc
C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\util\math_cpu.cc
C:\apilot\agent\_work\5\s\engine\lotus\winml\adapter\winml_adapter_dml.cpp
C:\apilot\agent\_work\5\s\engine\lotus\winml\adapter\winml_adapter_session.cpp
C;_,|
C\+CX
C_scale
c_shape != nullptr
c_shape is required if c_data is provided
C_zero_point
C|+Cx
C++/WinRT version:2.0.190620.2
c->in_use() && (c->bin_num == kInvalidBinNum)
C0N1l1u1
C0PRj
C0QVP
C0U0v0{0
c2->prev == h1
C4;{L|
C4+C0
C8+C4
C8F+C4
CallContext:[%hs] 
called_ == 1
cAMDt
Can broadcast 0 by 0 or 1. 
Can not digest separators: 
Can not digest tokenexp: 
Can not find the execution provider 
Can not find the node 
Can not get shape initializer data!
Can only add a new input at the end of the current ones.
Canadian_Aboriginal
candidate_output.Shape().Size() == output_shape.Size()
candidate_output_dims[iter] == 1
Cannot apply CumSum operator on a scalar
cannot compare iterators of different containers
Cannot concatenate scalars
cannot find allocator
cannot get value
Cannot parse the diagonal elements along dims 
Cannot replace concat node with initializer:
Cannot scale 0 by any factor to generate a non-zero value. 
Cannot slice scalars
Cannot split using values in 'split' attribute. Axis=
cannot use at() with 
cannot use erase() with 
cannot use key() for non-object iterators
Cannot use 'reflect' mode to pad dimension with a value of 0. Input shape:
Cannot use SearchOnePass for unanchored matches.
Cannot use user supplied initializer with name: (
Can't 
can't constant fold 
Can't find node with index 
Can't happen
Can't merge shape info. Both source and target dimension have values but they differ. Source=
Can't reduce on dim with value of 0 if 'keepdims' is false. Invalid output shape would be produced. input_shape:
Can't remove node 
Can't slice a non-tensor OrtValue. Type was 
Can't use func with null ptr
Carian
Carries out batch normalization as described in the paper
Case not handled in ComputeSimple: 
case_change_action
Cast Input from int64 to int32
Cast mask from int64 to int32
cast node to cast from float16 to float32 on cpu
cast_to
CastElimination
CastFloat16Transformer
CastMap
CategoryMapper
cats_int64s
cats_strings
Caucasian_Albanian
Caught exception while destructing CustomOpsLoader with message: 
Caught exception while loading custom ops with message: 
CblasNoTrans Unexpected CBLAS_TRANSPOSE for TransB of 
CblasTrans Unexpected CBLAS_TRANSPOSE for TransB of 
CD;0|<
CDist
CDWVj
ceil_mode
ceil_result
ceil_result_relu
ceil_result_relu_bool
ceil_result_relu_int
Cell clip threshold. Clipping bounds the elements of a tensor in the range of [-threshold, +threshold] and is applied to the input of activations. No clip if not specified.
CellMemInitTensor
center_point_box
center_point_box only support 0 or 1
CF;t$
CGSSSSRQh8
Chakma
ChannelCount
channels_last
char 
Char embedding size does not match char_embedding_size attribute.
Char embedding size does not match conv kernal size 2.
char_embedding_size
char16_t
char32_t
char8_t
CHECK failed: !is_closed_: 
CHECK failed: (backup_bytes_) == (0): 
CHECK failed: (buffer_used_) == (buffer_size_): 
CHECK failed: (count) <= (buffer_used_): 
CHECK failed: (count) >= (0): 
CHECK failed: (min_bytes) <= (std::numeric_limits<size_t>::max() - kBlockHeaderSize): 
CHECK failed: (new_size) <= ((std::numeric_limits<size_t>::max() - kRepHeaderSize) / sizeof(old_rep->elements[0])): 
CHECK failed: (scc->visit_status.load(std::memory_order_relaxed)) == (SCCInfoBase::kRunning): 
CHECK failed: backup_bytes_ == 0 && buffer_.get() != NULL: 
CheckNodesInPathK returns false
CheckNodesInPathQ returns false
CheckNodesInPathV return false
CheckSliceParameters return false
CheckSliceParameters return false for slice2
CheckSliceParameters returns false for last_slice
CheckSliceParameters returns false for mask_slice
CheckSliceParameters returns false for slice1
checksum
Cherokee
Child node if expression is false
Child node if expression is false.
Child node if expression is true
Child node if expression is true.
Chosen support vectors
CHPQW
class 
Class labels if using integer labels.<br>One and only one of the 'classlabels_*' attributes must be defined.
Class labels if using string labels.<br>One and only one of the 'classlabels_*' attributes must be defined.
Class labels when using integer labels. One and only one 'classlabels' attribute must be defined.
Class labels when using string labels. One and only one 'classlabels' attribute must be defined.
class_ids
class_nodeids
class_treeids
class_weights
classes.size() == 2 || classes.size() == 1
classes_strings
classlabels_int64s
classlabels_ints
classlabels_strings
classlabels_strings_.empty() ^ classlabels_int64s_.empty()
classlabels_strings_.size() > 0 || classlabels_ints_.size() > 0
cli::array<
cli::pin_ptr<
clip_ > 0.f
Clipped_ZeroPoint_FP
ClipThreshold
close() failed: 
CloseHandle
CNbYJ
CoalesceWalker::ShortVisit called
coclass 
CoCreateFreeThreadedMarshaler
code != static_cast<int>(common::OK)
Coefficient of ELU default to 1.0.
Coefficient of ELU.
Coefficient of leakage default to 0.01.
Coefficient of leakage.
Coefficient of SELU default to 1.0507.
Coefficient of SELU default to 1.05070102214813232421875 (i.e., float32 approximation of 1.0507009873554804934193349852946).
Coefficient of SELU default to 1.6732.
Coefficient of SELU default to 1.67326319217681884765625 (i.e., float32 approximation of 1.6732632423543772848170429916717).
coefficients
coefficients_.size() > 0
cointerface 
com.microsoft
com.microsoft.dml
com.microsoft.experimental
com.microsoft.mlfeaturizers
com.microsoft.nchwc
Common
CommonSubexpressionElimination
CompanyName
CompareStringEx
Compiled kernel hashes must be provided
compiled_kernel_hashes != nullptr
Compiler::Copy called!
complex128
complex64
ComplexMul
ComplexMulConj
Compress
Compute_
compute_info_->create_state_func(&context, &func_state_) == 0
Computed size: 
ComputePad: pad type not supported.
Computes the indices of the {name} elements of the input tensor's element along the
Concat
concat first input value is not -1
Concat of 
concat_after_gather does not have expected number of inputs or output edges
concat_after_gather input 2 does not have expected value
concat_result
ConcatFromSequence
Concretely, given the (fused) inputs X (TxNxD), the previous hidden
cond_out
condition
ConditionTensor
Config key is empty or longer than maximum length 128
Config value is longer than maximum length 1024
Conflicting free dimension overrides.
connection aborted
connection already in progress
connection refused
connection reset
const
const 
const_ignore_index
const_one
const_one_float
const_zero
const_zero_float
const_zero_target_typed
constant
Constant
Constant initializer NodeArg shape should not be null. NodeArg: 
constant_value
ConstantFill
ConstantFolding
ConstantOfShape
Constrain bias type to 32-bit integer tensor.
Constrain filter type to 8-bit integer tensor.
Constrain index tensor to int64
Constrain indice type to int32 or int64
Constrain indices to integer types
Constrain input a and its zero point data type to 8-bit integer tensor.
Constrain input A data type to 8-bit integer tensor.
Constrain input A data types as 16-bit integer tensor
Constrain input A, b_scale and output Y data type as float tensor.
Constrain input a_scale, b_scale and output Y data type as float tensor.
Constrain input and output  types to float tensors.
Constrain input and output float tensors types.
Constrain input and output integer tensors types
Constrain input and output to all tensor types.
Constrain input and output types (except mean and inv_std_var) to float tensors.
Constrain input and output types to 8 bit signed and unsigned tensors.
Constrain input and output types to 8 bit tensors.
Constrain input and output types to all numeric tensors and bool tensors.
Constrain input and output types to all numeric tensors.
Constrain input and output types to all numerical tensor types.
Constrain input and output types to all tensor types.
Constrain input and output types to all tensors.
Constrain input and output types to any tensor type.
Constrain input and output types to float and 8 bit tensors.
Constrain input and output types to float or half tensors.
Constrain input and output types to float tensors
Constrain input and output types to float tensors.
Constrain input and output types to float/int tensors.
Constrain input and output types to float32 tensors.
Constrain input and output types to floating-point tensors.
Constrain input and output types to high-precision and 8 bit numeric tensors.
Constrain input and output types to high-precision numeric tensors.
Constrain input and output types to int8 tensors.
Constrain input and output types to integer tensors.
Constrain input and output types to numeric tensors.
Constrain input and output types to signed numeric tensors.
Constrain input and output types to singed/unsigned int8 tensors.
Constrain input and output types.
Constrain input b and its zero point data type to 8-bit integer tensor.
Constrain input B data type to 8-bit integer tensor.
Constrain input B data types as 16-bit integer tensor
Constrain input 'ratio' types to float tensors.
Constrain input type to 8-bit integer tensor.
Constrain input type to unsigned or signed 32-bit integer tensor, or string tensor. It should be utf-8 encoded if using unicode.
Constrain input types to 8 bit signed and unsigned tensors.
Constrain input types to all tensor types.
Constrain input types to any tensor type.
Constrain input types to common numeric type tensors.
Constrain input types to float tensors.
Constrain input types.
Constrain input types. Casting from complex is not supported.
Constrain input types. Casting from strings and complex are not supported.
Constrain input types. Strings and complex are not supported.
Constrain input w and its zero point data type to 8-bit integer tensor.
Constrain input x and its zero point data type to 8-bit integer tensor.
Constrain input X and output types to float/int tensors.
Constrain input 'X' and output 'Y' to all tensor types.
Constrain input Y types to float/int tensors.
Constrain input, weight, and output types to floating-point tensors.
Constrain input0 and output types to float tensors
Constrain mask index to integer types
Constrain mean and inv_std_var to be float tensors.
Constrain mean and inv_std_var to float tensors.
Constrain output data type to 32-bit integer tensor.T2 must be tensor(uint32) when T1 is tensor(uint8),or must be tensor(int32) when T1 is tensor(int8).
Constrain output mask types to boolean tensors.
Constrain output 'mask' types to boolean tensors.
Constrain output to int64 tensor, which should be a scalar though.
Constrain output to int64 tensor.
Constrain output to integral tensor. It must be a scalar(tensor of empty shape).
Constrain output type to 8-bit integer tensor.
Constrain output type to unsigned and signed 32-bit integer tensor.
Constrain output types to 32 bit tensors.
Constrain output types to all tensor types.
Constrain output types to any tensor type.
Constrain output types to be numerics.
Constrain output types to bool, int32, int64, float16, float, double tensors.
Constrain output types to boolean tensors.
Constrain output types to float tensors.
Constrain output types to integral tensors.
Constrain output types. Casting to complex is not supported.
Constrain output types. Casting to strings and complex are not supported.
Constrain output types. Strings and complex are not supported.
Constrain output y and its zero point data type to 8-bit integer tensor.
Constrain output Y data type as 32-bit integer tensor.
Constrain output y data type to 32-bit integer tensor.
Constrain output Y data types as 32-bit integer tensor.T3 must be tensor(uint32) when both T1 and T2 are tensor(uint16),or must be tensor(int32) when either T1 or T2 is tensor(int16).
Constrain position to integral tensor. It must be a scalar(tensor of empty shape).
Constrain repeat's type to int64 tensors.
Constrain roi type to float or double.
Constrain seq_lens to integer tensor.
Constrain seq_lens to integral tensors.
Constrain split size to integral tensor.
Constrain target to integer types
Constrain tiles and axis's type to int64 tensors.
Constrain to all tensor types.
Constrain to any tensor type.
Constrain to any tensor type. If the dtype attribute is not provided this must be a valid output type.
Constrain to boolean tensors.
Constrain to integer types
Constrain to tensor(float).
Constrain to tensor(int32).
Constrain types to float tensors.
Constrain types to int tensors.
Constrain weights types to 8 bit tensors.
Constrain 'x' and 'x_zero_point' to 8-bit integer tensors.
Constrain 'x' to float or int32 tensor.
Constrain 'x' to float tensor.
Constrain 'x', 'y_scale' to float tensors.
Constrain 'x_zero_point' and 'x' to 8-bit/32-bit integer tensor.
Constrain 'y', 'x_scale' to float tensors.
Constrain 'y_zero_point' and 'y' to 8-bit integer tensor.
Constrain 'y_zero_point' and 'y' to 8-bit integer tensors.
Constrain 'y_zero_point' and 'y' to 8-bit unsigned integer tensor.
Constrains input and output to only numeric types.
Constrains input to boolean tensor.
Constrains input to float tensors.
Constrains input to integral tensors.
Constrains input to only numeric types.
Constrains input types to all numeric tensors.
Constrains input/output to boolean tensors.
Constrains output to boolean tensor.
Constrains to boolean tensors.
consumed_inputs
context does not contain text
Conv filter size does not match embedding_size attribute.
Conv kernal size 1 does not match conv_window_size attribute .
conv_B_tensor_proto
conv_W_tensor_proto
conv_window_size
ConvActivationFusion
ConvAddFusion
ConvAddFusion_Add_B_
ConvAddFusion_B_
ConvBNFusion
ConvBnFusion_BN_B_
ConvBnFusion_W_
Conversion Error
ConvInteger
ConvMulFusion
ConvMulFusion_Mul_B_
ConvMulFusion_W_
ConvTranspose
ConvTransposeWithDynamicPads
coordinate_transform_mode:[
coordinate_transformation_mode
Coptic
Copy from/to host memory
copy_info.size() == num_feeds
CoreMLExecutionProvider
corrupted protobuf data: tensor shape size(
CoTaskMemAlloc
Could not finalize session options while constructing the inference session. Error Message: 
Could not find a CPU kernel and hence 
Could not find an implementation for the node 
Could not find chunk in bin
Could not find OrtValue with idx '
Could not find OrtValue with name '
Could not find Region for 
Could not infer data type from input tensor with data type 
Could not parse model successfully while constructing the inference session
Could not write a profile because no model was loaded.
count == 1
count_include_pad
counts
Couple the input and forget gates if 1, default 0.
Couple the input and forget gates if 1.
CoupleInputForget
Cp_^[
Cp9Clu
CPUExecutionProvider
CQVSSVRQh<
Create_State_
CreateDirectoryA
CreateDirectoryW
CreateDXGIFactory2
CreateEventW
CreateFeedsFetchesManager must be called prior to execution of graph.
CreateFile2
CreateMutexExW
CreateSemaphoreExW
Creating 
Creating and using per session threadpools since use_per_session_threads_ is true
Creating BFCArena for 
Crop and image to the specified spatial dimensions. If scale is given,
crop_size
crop_size shape input tensor has wrong dimension
CropAndResize
cross device link
CrossChannel
CRSSj
CSSRhL
CSSRPQh|&?
CSSRQQh
CSSSRQhP
CSSSSRQhP
CSSWQQh
CSSWQQh0
CTensor
cubic
'Cubic' mode only support 2-D inputs ('Bicubic') or 4-D inputs with the corresponding outermost 2 scale values being 1 in the 
cubic_coeff_a
CUDA execution provider is not enabled.
CUDAExecutionProvider
cudaMalloc
CudaPinned
CumSum
Cuneiform
cur + size <= end
cur_index == &*indices_data.cend()
cur_input == end_input || cur_input->first >= 0
cur_iteration_ < num_iterations_
cur_out == end_out
cur1 == end1
current
current <= buffer_size_
custom op registered at runtime
custom_logger != nullptr
CX;C\t
Cypriot
Cyrillic
CYY;^
D D R R z | 
D$ ;D$$
D$ ;D$\
D$ ;E
D$ ;M
D$ |N
D$ |U
D$ 9\$
D$ 9D$
D$ 9D$$
D$ 9D$Hr+
D$ j@P
D$ Pj
D$ PR3
D$ SVW
D$ VW
D$#VQ
D$$;|$(
D$$;D$
D$$;D$d
D$$;D$H|
D$$+D$ @
D$$9D$
D$$9D$H|7
D$$9F
D$$H#
D$$j@P
D$$jhP
D$$YY9D$
D$(;L$
D$(+D$
D$(+M@
D$(9~
D$(9F
D$(9L$
D$(Pf
D$(PR3
D$(S3
D$(SV
D$(SVW3
D$(WSP
D$(xs
D$,;D$0|
D$,;F
D$,9V
D$,l6<
D$,PQ
D$,PR
D$,QR
D$,VP
D$,VW
D$,xO
D$@;D$D
D$@;F
D$@9r
D$@SV
D$\;D$`
D$\VW3
D$`|]
D$<;F
D$<PQ
D$0+|$$
D$09~
D$09K
D$09L$
D$0QQ
D$0SV
D$0SVW
D$0VW
D$4;A
D$4;F
D$4_^[
D$48H
D$49D$
D$4PQ
D$4PRPj
D$4PS
D$4VW
D$8;D$<
D$8;Wtu
D$8;Wxu
D$8|T
D$8|X
D$89S
D$8Ph
D$D;B
D$d;D$(
D$DH6<
D$DPQ
D$DQP
D$h;r
D$H+D$
D$h9},
D$hSSQRP
D$HSVW
D$HxV
D$L+D$
D$P|[
D$P+D$
D$p9L$
D$T;B
D$TPVP
D$TVP
D$tVWRQ
D$x;r
D$X|]
D$X9}0
D$X9L$
D$XSV
D{:9}
D{49}
D0I0s1}1
d3d12.dll
Data of TensorProto ( tensor name: 
data overflow
data tensor must have rank >= 1
data type 
Data type for starts and ends inputs' is not supported in this build. Got 
data type is different from updates type
data type is not supported
Data type mismatch
Data type of the input tensor MUST be same as that of the input sequence. Sequence data type (
Data types of the inputs must match for MatMul
data_0
data_1.Shape() == shape
DATA_BATCH
data_n.Shape() == shape
data_rank * 2 == pads.size()
data_rank > 0
data_scale
data_transfer registered is nullptr.
data_type
data_zero_point
DCR (default) for depth-column-row order re-arrangement. Use CRD for column-row-depth order.
DD$<PRQ
DeadState in RunStateOnByte
DebugBreak
DecodePointer
decomposed_QLinearSigmoid_DequantizeLinear_
decomposed_QLinearSigmoid_input_
decomposed_QLinearSigmoid_output_
decomposed_QLinearSigmoid_QuantizeLinear_
decomposed_QLinearSigmoid_Sigmoid_
default 1; Pooled output Y's height.
default 1; Pooled output Y's width.
Default logger already set. 
default_float
default_int64
default_logger_id must be provided if instance_type is InstanceType::Default
default_string
Defines behaviour if 'axes' is empty. Default behaviour with 'false' is to reduce all axes. When axes is empty and this attribute is set to true, input tensor will not be reduced,and the output tensor would be equivalent to input tensor.
Defines how to aggregate leaf values within a target. <br>One of 'AVERAGE,' 'SUM,' 'MIN,' 'MAX.'
DelayLoadFailureHook
DeleteCriticalSection
DeleteFile() failed - path: 
DeleteFileW
delta
delta in Range operator can not be zero!
delta in Range operator should be scalar like tensor, yet got shape:
delta_casted
Denote x_resized as the coordinate of axis x in the resized tensor, x_original as the coordinate of axis x in the original tensor, length_original as the length of the original tensor in axis x, length_resized as the length of the resized tensor in axis x, roi_x = (start_x, end_x) of the axis x in input "roi", scale = length_resized / length_original, <br/>
DENSE
depth
Depth is negative.
DepthToSpace
DepthToSpace op: only 'DCR' and 'CRD' modes are supported
DequantizeLinear
DequantizeLinear with type int32 should have no zero point or all zero points should be 0
deque<T> too long
Describes the axis of the inputs when coerced to 2D; defaults to one because the 0th axis most likely describes the batch_size
Describes the axis of the inputs when coerced to 2D; defaults to one because the 0th axis most likely describes the batch_size. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
Deseret
Deserialize tensor 
destination address required
detect_negative
detect_positive
Devanagari
device or resource busy
Device:[
DeviceType:
DFA out of memory: size 
DictVectorizer
Did not find session options in the model file to be used while running the model
Dilation not supported for AutoPadType::SAME_UPPER or AutoPadType::SAME_LOWER.
Dilation value along each spatial axis of filter.
Dilation value along each spatial axis of filter. If not present, the dilation defaults to 1 along each spatial axis.
dilation value along each spatial axis of the filter.
dilation value along each spatial axis of the filter. If not present, the dilation defaults is 1 along each spatial axis.
dilation value along each spatial axis of the filter. If not present, the dilation defaults to 1 along each axis.
dilation value along each spatial axis of the filter. If not present, the dilation defaults to 1 along each spatial axis.
Dilations
dilations
Dilations dimensions should match kernel shape
dilations.size() == kernel_shape.size()
dim_iter == rank
dim_param value with no name. Invalid ORT format model.
dim_size > 0
dim0_offset < dim0_size
dimension <= num_dims
Dimension could not be inferred: incompatible shapes
dimension for each axis in the list of axes, it uses this information to
Dimension mismatch in unification between 
Dimension of input 
Dimension on which to do the sort.
Dimension on which to do the sort. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
Dimension value inferred (
dimension. If the value passed to start or end is larger than the `n` (the
Dimension: 
DimensionCount
dims.size() == extents.size() && dims.size() >= steps.size()
dims.size() == extents_.size()
dims.size() == starts.size()
dims.size() == starts.size() && dims.size() == extents_.size() && dims.size() >= steps.size()
dims.size() == steps.size()
dims.size()=
dims[d_i] < d_max
dimstart <= dimend && dimend <= size()
direction
Direction
Direction of moving bits. It can be either "RIGHT" (for right shift) or "LEFT" (for left shift).
directions
directions.size() == num_entries
DirectML.dll
directory not empty
discarded
Div and Shape does not have edge
Div and Shape1 does not have edge
div_inputs.size() == 2
div_result
Divide by zero
DML allocator
DML CPU
Dml::ExecutionProviderImpl::CopyTensors
Dml::GraphTransformer::ApplyImpl
DML_OPERATOR_ACTIVATION_CELU
DML_OPERATOR_ACTIVATION_ELU
DML_OPERATOR_ACTIVATION_HARD_SIGMOID
DML_OPERATOR_ACTIVATION_HARDMAX
DML_OPERATOR_ACTIVATION_IDENTITY
DML_OPERATOR_ACTIVATION_LEAKY_RELU
DML_OPERATOR_ACTIVATION_LINEAR
DML_OPERATOR_ACTIVATION_LOG_SOFTMAX
DML_OPERATOR_ACTIVATION_PARAMETERIZED_RELU
DML_OPERATOR_ACTIVATION_PARAMETRIC_SOFTPLUS
DML_OPERATOR_ACTIVATION_RELU
DML_OPERATOR_ACTIVATION_RELU_GRAD
DML_OPERATOR_ACTIVATION_SCALED_ELU
DML_OPERATOR_ACTIVATION_SCALED_TANH
DML_OPERATOR_ACTIVATION_SHRINK
DML_OPERATOR_ACTIVATION_SIGMOID
DML_OPERATOR_ACTIVATION_SOFTMAX
DML_OPERATOR_ACTIVATION_SOFTPLUS
DML_OPERATOR_ACTIVATION_SOFTSIGN
DML_OPERATOR_ACTIVATION_TANH
DML_OPERATOR_ACTIVATION_THRESHOLDED_RELU
DML_OPERATOR_ADAM_OPTIMIZER
DML_OPERATOR_ARGMAX
DML_OPERATOR_ARGMIN
DML_OPERATOR_AVERAGE_POOLING
DML_OPERATOR_AVERAGE_POOLING_GRAD
DML_OPERATOR_BATCH_NORMALIZATION
DML_OPERATOR_CAST
DML_OPERATOR_CONVOLUTION
DML_OPERATOR_CONVOLUTION_INTEGER
DML_OPERATOR_CUMULATIVE_SUMMATION
DML_OPERATOR_DEPTH_TO_SPACE
DML_OPERATOR_DEPTH_TO_SPACE1
DML_OPERATOR_DIAGONAL_MATRIX
DML_OPERATOR_DYNAMIC_QUANTIZE_LINEAR
DML_OPERATOR_ELEMENT_WISE_ABS
DML_OPERATOR_ELEMENT_WISE_ACOS
DML_OPERATOR_ELEMENT_WISE_ACOSH
DML_OPERATOR_ELEMENT_WISE_ADD
DML_OPERATOR_ELEMENT_WISE_ADD1
DML_OPERATOR_ELEMENT_WISE_ASIN
DML_OPERATOR_ELEMENT_WISE_ASINH
DML_OPERATOR_ELEMENT_WISE_ATAN
DML_OPERATOR_ELEMENT_WISE_ATANH
DML_OPERATOR_ELEMENT_WISE_BIT_AND
DML_OPERATOR_ELEMENT_WISE_BIT_COUNT
DML_OPERATOR_ELEMENT_WISE_BIT_NOT
DML_OPERATOR_ELEMENT_WISE_BIT_OR
DML_OPERATOR_ELEMENT_WISE_BIT_SHIFT_LEFT
DML_OPERATOR_ELEMENT_WISE_BIT_SHIFT_RIGHT
DML_OPERATOR_ELEMENT_WISE_BIT_XOR
DML_OPERATOR_ELEMENT_WISE_CEIL
DML_OPERATOR_ELEMENT_WISE_CLIP
DML_OPERATOR_ELEMENT_WISE_CONSTANT_POW
DML_OPERATOR_ELEMENT_WISE_COS
DML_OPERATOR_ELEMENT_WISE_COSH
DML_OPERATOR_ELEMENT_WISE_DEQUANTIZE_LINEAR
DML_OPERATOR_ELEMENT_WISE_DIVIDE
DML_OPERATOR_ELEMENT_WISE_ERF
DML_OPERATOR_ELEMENT_WISE_EXP
DML_OPERATOR_ELEMENT_WISE_FLOOR
DML_OPERATOR_ELEMENT_WISE_IDENTITY
DML_OPERATOR_ELEMENT_WISE_IF
DML_OPERATOR_ELEMENT_WISE_IS_INFINITY
DML_OPERATOR_ELEMENT_WISE_IS_NAN
DML_OPERATOR_ELEMENT_WISE_LOG
DML_OPERATOR_ELEMENT_WISE_LOGICAL_AND
DML_OPERATOR_ELEMENT_WISE_LOGICAL_EQUALS
DML_OPERATOR_ELEMENT_WISE_LOGICAL_GREATER_THAN
DML_OPERATOR_ELEMENT_WISE_LOGICAL_GREATER_THAN_OR_EQUAL
DML_OPERATOR_ELEMENT_WISE_LOGICAL_LESS_THAN
DML_OPERATOR_ELEMENT_WISE_LOGICAL_LESS_THAN_OR_EQUAL
DML_OPERATOR_ELEMENT_WISE_LOGICAL_NOT
DML_OPERATOR_ELEMENT_WISE_LOGICAL_OR
DML_OPERATOR_ELEMENT_WISE_LOGICAL_XOR
DML_OPERATOR_ELEMENT_WISE_MAX
DML_OPERATOR_ELEMENT_WISE_MEAN
DML_OPERATOR_ELEMENT_WISE_MIN
DML_OPERATOR_ELEMENT_WISE_MODULUS_FLOOR
DML_OPERATOR_ELEMENT_WISE_MODULUS_TRUNCATE
DML_OPERATOR_ELEMENT_WISE_MULTIPLY
DML_OPERATOR_ELEMENT_WISE_POW
DML_OPERATOR_ELEMENT_WISE_QUANTIZE_LINEAR
DML_OPERATOR_ELEMENT_WISE_QUANTIZED_LINEAR_ADD
DML_OPERATOR_ELEMENT_WISE_RECIP
DML_OPERATOR_ELEMENT_WISE_ROUND
DML_OPERATOR_ELEMENT_WISE_SIGN
DML_OPERATOR_ELEMENT_WISE_SIN
DML_OPERATOR_ELEMENT_WISE_SINH
DML_OPERATOR_ELEMENT_WISE_SQRT
DML_OPERATOR_ELEMENT_WISE_SUBTRACT
DML_OPERATOR_ELEMENT_WISE_TAN
DML_OPERATOR_ELEMENT_WISE_TANH
DML_OPERATOR_ELEMENT_WISE_THRESHOLD
DML_OPERATOR_FILL_VALUE_CONSTANT
DML_OPERATOR_FILL_VALUE_SEQUENCE
DML_OPERATOR_GATHER
DML_OPERATOR_GATHER_ELEMENTS
DML_OPERATOR_GATHER_ND
DML_OPERATOR_GATHER_ND1
DML_OPERATOR_GEMM
DML_OPERATOR_GRU
DML_OPERATOR_JOIN
DML_OPERATOR_LOCAL_RESPONSE_NORMALIZATION
DML_OPERATOR_LP_NORMALIZATION
DML_OPERATOR_LP_POOLING
DML_OPERATOR_LSTM
DML_OPERATOR_MATRIX_MULTIPLY_INTEGER
DML_OPERATOR_MAX_POOLING
DML_OPERATOR_MAX_POOLING_GRAD
DML_OPERATOR_MAX_POOLING1
DML_OPERATOR_MAX_POOLING2
DML_OPERATOR_MAX_UNPOOLING
DML_OPERATOR_MEAN_VARIANCE_NORMALIZATION
DML_OPERATOR_MEAN_VARIANCE_NORMALIZATION1
DML_OPERATOR_NONZERO_COORDINATES
DML_OPERATOR_ONE_HOT
DML_OPERATOR_PADDING
DML_OPERATOR_QUANTIZED_LINEAR_CONVOLUTION
DML_OPERATOR_QUANTIZED_LINEAR_MATRIX_MULTIPLY
DML_OPERATOR_RANDOM_GENERATOR
DML_OPERATOR_REDUCE
DML_OPERATOR_RESAMPLE
DML_OPERATOR_RESAMPLE_GRAD
DML_OPERATOR_RESAMPLE1
DML_OPERATOR_REVERSE_SUBSEQUENCES
DML_OPERATOR_RNN
DML_OPERATOR_ROI_ALIGN
DML_OPERATOR_ROI_POOLING
DML_OPERATOR_SCATTER
DML_OPERATOR_SCATTER_ND
DML_OPERATOR_SLICE
DML_OPERATOR_SLICE_GRAD
DML_OPERATOR_SLICE1
DML_OPERATOR_SPACE_TO_DEPTH
DML_OPERATOR_SPACE_TO_DEPTH1
DML_OPERATOR_SPLIT
DML_OPERATOR_TILE
DML_OPERATOR_TOP_K
DML_OPERATOR_TOP_K1
DML_OPERATOR_UPSAMPLE_2D
DML_OPERATOR_VALUE_SCALE_2D
DMLCreateDevice1
DmlExecutionProvider
DmlFusedNode_
DmlFusedNodeDomain
DnnlExecutionProvider
DO\;O\t
DoCoalesce failed: r1->op() is 
DoCoalesce failed: r2->op() is 
does not have the graph for key 
Dogra
Domain already set in registry
domainToVersionMap
Done saving initialized tensors
Done saving OrtValue mappings.
double
double_data
drop_states
Dropout
dst_implicit_input_idx < (int)node->ImplicitInputDefs().size()
dtype
dtype_ != nullptr
dtype_attribute->second.has_i()
dup_replacements.find(&arg) == dup_replacements.end()
Duplicate constant node sparse initializer name: '
Duplicate initializer (dense or ConstantNode): '
Duplicate initializer (dense, sparse or ConstantNode): '
Duplicate ngram detected, size: 
Duplicate of FusedMatMul. Going forward FusedMatMul should be used. This OP will be supported for backward compatibility.
Duplicate sparse_tensor_initializer: '
Duplicate stopwords not allowed
Duplicate type constraint name
duplicated allocator
duplicated allocator: 
duplicated location
duplicated ort_value index:
Duployan
dxgi.dll
DynamicQuantizeLinear
DynamicQuantizeLSTM
DynamicQuantizeLSTM : 
DynamicQuantizeMatMul
DynamicQuantizeMatMul : input B scale must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
DynamicQuantizeMatMul : input B zero point must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
DynamicQuantizeMatMulFusion
DynamicSlice
E QQQQ
E$VWj
E(QQPQPQ
E,PQW
E_Xsquared
E<9ED
E1X1k1q1
Each element of the sequence should be either tensor or map.
Egyptian_Hieroglyphs
Einsum
Einsum expression string.
Einsum op: An implementation for the input type 
Einsum op: Could not copy the intermediate output's buffer into the op's output buffer. Error: 
Einsum op: Exception during MatMul operation: 
Einsum op: Input dimensions must be equal along an axis to be reduced across all inputs
Einsum op: Input shapes do not align
Einsum op: The candidate output cannot be reshaped into the op's output
Einsum op: The candidate output does not match the actual output's shape
Einsum op: There must be atleast one input
Einsum op: Transpose failed: 
Einsum op: Unsupported data type for Diagonal 
Einsum operands could not be broadcast together. Please check input shapes/equation provided.Input shape of operand 
Einsum subscripts does not contain enough subscript labels and there is no ellipsis for input 
Einsum subscripts string contains too many subscript labels when compared to the rank of the input 
Either both scale and offset can be of feature size (
Either one of the separators OR tokenexp attributes required but none is set
Either scales or sizes MUST be provided as input.
Either the key tensor or the value tensor has NumDimensions > 1
EL$ ;
EL$(;
EL$0;
Elbasan
elem_proto != nullptr
elem_type
elem_type_ != nullptr
element index is out of bounds
Element type of input 
Element type of input was unknown
Element type of inputs are expected to be the same.
elements, but feeds has 
EliminateDropout
EliminateIdentity
EliminateSlice
Ellipsis must indicate a fixed number of dimensions across all inputs
Ellipsis represents incompatible dimensions.
else_branch
Elu_Result
embedding_size
EmbedLayerNormalization
EmbedLayerNormFusion
Empty dimensions for input tensor
Empty graph proto from deserialization of ORT format model
Empty input dimensions.
Empty scale in attributes
Empty stopwords not allowed
Empty value of imputed values.
Enable broadcasting
enable_profiling
enable_profiling option in the model file must be an integer
enabled_
EncodePointer
Encountered nullptr.
Encountered unknown exception in Initialize()
Encountered unknown exception in Load()
Encountered unknown exception in Run()
end >= starts_.back()
end of a dimension with unknown size, it is recommended to pass in `INT_MAX`.
end of input
end_idx >= start_idx && end_idx <= total_items
Ending indices (exclusive) of corresponding axis in axes`
EndPadding
endpos: 
Ends must be a 1-D array
Engine failed to create a model!
ENGINE_ERROR
EnterCriticalSection
entiu
entry != initialized_tensors_to_allocate.end()
entry != kernel_create_info_map.cend()
entry != kernel_create_info_map_.cend()
entry != node_to_subgraph_ss.second.cend()
entry != nullptr
Entry exists in node 
entry.program_counter.HasValidEntries()
enum 
en-US
Env is null
env_ptr == p_instance_
Environment dependent string that denotes the locale according to which output strings needs to be upper/lowercased.Default en_US or platform specific equivalent as decided by the implementation.
EP_FAIL
epsilon
Epsilon
epsilon_ >= 0
Equal
equal const not matched.
equation
ERROR
Error compiling '
Error during EndProfiling(): 
Error mapping feeds: 
Error mapping output names: 
Error merging shape info for output. '
Error parsing '
Error reverse compiling '
Error: Duplicate definition-site for (
errorCategory
errorCode
errorMessage
ESession must be initialized to create session state.
Ethiopic
euclidean
EvaluationStart
EvaluationStop
EventRegister
EventSetInformation
EventUnregister
EventWriteTransfer
EX_squared
Example 1:
Example 2:
Example 3:
Example 4:
Exceeded max transformer level. Current level is set to 
Exception
Exception caught: 
Exception during initialization: 
Exception during loading: 
Exception running nodes starting at 
exclude_outside
exclude_outside can be set to 1 only when mode is CUBIC. Current mode is set to 
exclusive
exec_plan_index
exec_plan_ptr
executable format error
Execution frame was null
Execution providers must be registered before the session is initialized.
Execution providers must be registered before the session is initialized. 
Execution type '
execution_mode
execution_mode is not valid
execution_mode option in the model file must be an integer
ExecutionProviderEvent
executionProviderIds
Existing entry in compiled kernel hashes for 
existing_entries.find(attribute_name) == existing_entries.cend()
existing->second == &tensor
Exiting due to terminate flag being set to true.
EXP;Jx
Expand
ExpandBroadcastLooper should only have a shape for the second input.
ExpandDims
ExpandDims echo operator.
expanded
expanded_target
expanded_target_int64
ExpandElimination
Expect mask data type is uint8 or float
Expect to have present state output when past state input is given
Expected a single float value!
Expected a single int64 value!
Expected AllocateFinalOutput to have been called to before we increment the iterator
Expected AllocateFinalOutput to have been called to before we read the OrtValue from the iterator.
Expected 'replace_value_int64' attribute since 'imputed_values_int64' is specified
Expected 'replaced_value_float' attribute since 'imputed_value_floats' is specified
Expected value:
Expecting a non-empty tokenexp
Expecting activation to be one of Affine, Relu, LeakyRelu, ThresholdedRelu, Tanh, ScaledTanh, Sigmoid, HardSigmoid, Elu, Softsign, Softplus. Got 
Expecting all elements to be tensors. Got: 
Expecting indices to be either int32_t or int64_t
Exponent
ExponentTensor
Extended allocation by 
Extending BFCArena for 
extents.size()=
extern "C" 
External data type cannot be UNDEFINED or STRING.
External data type must not be UNDEFINED or STRING.
EXtn:
extra_shape
extrapolation_value
EyeLike
EyeLike : Input tensor dimension is not 2
F F ~ ~ 
F$;F$tR
F$;F(t
F( ';
F(;F,
F(;F,tG!]
F(+F$
F(+F$j
F(9x@t{
F(9x`ti
F(d&;
F,+F(
F,PWSR
F;u$|
F@+F<j
F\SWj
F\tn:
F\upQQ
F|+Fx
F|G+Fx
F<+F8j
F<SWj
F0;F4t;
F08VHt8
F09F8
f0i2n2
F4;F8t4
F4+F0
F4+F0P
F89^L|&
F9q<u
Factor used in computing the running mean and variance.e.g., running_mean = running_mean * momentum + mean * (1 - momentum), default is 0.9f.
Factor used in computing the running mean and variance.e.g., running_mean = running_mean * momentum + mean * (1 - momentum).
Faild to find path to qkv_matmul
Faild to find path v
Faild to find path v to Split
Faild to match concat node for Gather paths
Faild to match gemm gather path
Faild to match gemm path
Faild to match path 1 for unidirectional mask
Faild to match path 2 for unidirectional mask
Faild to match path 3 for unidirectional mask
Faild to match path 4 for unidirectional mask
Faild to match the path (Div-->Where-->Add) for unidirectional mask
Failed in match input mask subgraph
Failed in match Transpose attribute perm. Expected: 0, 2, 1, 3
Failed in match v_matmul and v_add input shape
Failed in match v_transpose attribute perm. Expected: 0, 2, 1, 3
Failed since multiple edges matched:
Failed to add kernel for 
Failed to allocate memory for requested buffer of size 
Failed to analyze start state.
Failed to construct locale with name:
Failed to convert dense initializer to sparse
Failed to convert mask to int32
Failed to copy tensor to 
Failed to create output tensor for 
Failed to create output tensor for If output 
Failed to create output tensor for output #
Failed to create the inter-op thread pool for the parallel executor, setting ExecutionMode to SEQUENTIAL
Failed to find a free memory block despite calling Extend. rounded_bytes=
Failed to find allocator for device 
Failed to find input name in the mapping: 
Failed to find kernel for 
Failed to find mask path
Failed to find path 1 of position shape.
Failed to find path 2 of position shape.
Failed to find path for k
Failed to find path for mask
Failed to find path for past_k
Failed to find path for present_k
Failed to find path for present_v and past_v
Failed to find path for q
Failed to find reshape shape path 1
Failed to find reshape shape path 2
Failed to find shape path
Failed to find Softmax node
Failed to find symbol in library, error code: 
Failed to get allocator for initializer '
Failed to get allocator for location: 
Failed to get allocator for optimizer
failed to get first output!
Failed to get initializer tensor.
Failed to get position embedding weights.
Failed to load library, error code: 
Failed to load model because protobuf parsing failed.
Failed to load model with error: 
Failed to load Q, K and V bias tensors, or data type is not float or float16.
Failed to load Q, K and V weights, or data type is not float or float16.
Failed to match position embedding subgraph.
Failed to match position subgraph.
Failed to match Shape node. 
Failed to match v_concat
Failed to obtain detect_negative
Failed to obtain detect_positive
Failed to parse model file!
Failed to parse model stream!
Failed to parse path root: 
Failed to serialize model!
Failed to unload DSO: 
Failed to unload library, error code: 
Failed to write value with snprintf().
FailFast
false
false literal
fast_gelu_output
FastGelu
FastGeluFusion
FATAL
Fatal error: 
Fatal error: 0 count processors from GetLogicalProcessorInformation
Fatal error: 0 count processors from GetSystemInfo
fbs_attr cannot be null
fbs_node_arg_names cannot be null
Feature id for each node.
FeatureVectorizer
feed_locations.size() == copy_info.size()
feeds.size() == feed_mlvalue_idxs.size()
feeds_fetches_manager_
feeds_fetches_manager_ && info_
fetch_alloc_info.size() == copy_info.size()
Fetches vector passed to GetOutputs contains 
fetches.empty() || fetches.size() == fetch_mlvalue_idxs_.size()
fetches.size() == node->OutputDefs().size()
fff?C
Fh+F`
FH+FDj
Field '
file exists
File is too large.
File not found!
file too large
file_path == nullptr
FileDescription
filename too long
FileVersion
filter number not equal to input channel number.
filter_info_ == nullptr
FilterScaleTensor
FilterTensor
FilterZeroPointTensor
final_output_mlvalue_
final_state_and_scan_outputs
FindClose
FindFirstFileW
FindNextFileW
First dimension (num_rois) of batch_indices and rois don't match
First input does not have rank 2
first input tensor has wrong dimension
First input tensor must have rank 3
First set of probability coefficients.
First, offset by this.<br>Can be length of features in an [N,F] tensor or length 1, in which case it applies to all features, regardless of dimension count.
Fj hX
FL9^8u
Flag indicating whether the regression is a one-class SVM or not.
Flatten
float
FLOAT
float_data
float16
FLOAT16 is not supported
FLOATFLOATSGRAPHGRAPHSINTINTSSPARSE_TENSORSPARSE_TENSORSSTRINGSTRINGSTENSORTENSORSUNDEFINED
FLOATS
floor
Floor
FlsAlloc
FlsFree
FlsGetValue
FlsSetValue
FLSWj
Flush-to-zero and denormal-as-zero are 
fmod attribute must be true for float, float16 and double types
fmod must have value either 0 or 1
fmod_
For each node, define what to do in the presence of a missing value: if a value is missing (NaN), use the 'true' or 'false' branch based on the value in this array.<br>This attribute may be left undefined, and the defalt value is false (0) for all nodes.
For each node, define what to do in the presence of a NaN: use the 'true' (if the attribute value is 1) or 'false' (if the attribute value is 0) branch based on the value in this array.<br>This attribute may be left undefined and the defalt value is false (0) for all nodes.
For example, the following tensor shapes are supported (with broadcast=1):
For internal use.
For map type num_values MUST be 2
For ort_value with index: 
For previous (depreciated) non-spatial cases, implementors are suggested
forgot to update the version range in DomainToVersionRange 
FormatMessageA
FormatMessageW
forward
Found '.' not part of an ellipsis in input: 
Found '.' not part of an ellipsis in the output subscript provided
Found a '.' not part of an ellipsis in input: 
Found a '.' not part of an ellipsis in the output subscript provided
found duplicated provider 
Found kernel for Op with name (
Found session/run/environment configuration in the model file to be used while running the model
Four modes: round_prefer_floor (default, as known as round half down), round_prefer_ceil (as known as round half up), floor, ceil. Only used by nearest interpolation. It indicates how to get "nearest" pixel in input tensor from x_original, so this attribute is valid only if "mode" is "nearest".
foward
Fp+Fl
frame != nullptr
FreeDimensionOverrideTransformer
FreeLibrary
ft&9q
Ft+Fl
func info for node: 
function
Function
function not supported
Fused
fused 
fused Add and Gelu
Fused an attention node for GPT.
Fused an attention node.
Fused Attention subgraphs 
fused Conv 
fused EmbedLayerNorm subgraphs 
fused Gelu subgraphs 
fused Gemm 
fused GPT2Gelu subgraphs 
fused LayerNorm subgraphs 
fused Matmul and Add 
Fused MatMul and Scale
fused op (
Fused reshape node: 
fused SkipLayerNorm subgraphs 
fused_
fused_activation
fused_activation_domain
fused_activation_since_version
fused_alpha
fused_beta
fused_function_subgraph
fused_gamma
fused_ratio
FusedActivation
FusedAdd
FusedBatchNormalization
FusedConv
FusedConvTranspose
FusedGemm
FusedInstanceNormalization
FusedMatMul
FusedMeanVarianceNormalization
FusedSum
FuseReluClip
FuseReluClip_
fusion_style == IExecutionProvider::FusionStyle::Function
FVVVQQh
G _[]
G _^[
G$;G(u
G$+G 
G(0";
G(D";
G(P$;
G(P%;
G(PVQ
G(t!;
G,+G(
G,+G(ua
G,+G(um
G;|$4
G;|$L
G;~$r
G@;C@u\
G@+G<
G\+GX
G\t2QQ
G_^[]
G`;Gdu
G`+G\
G|+Gx
G|F+Gx
G<+G8
G<+G8j
G0PQQ
G0PQQQQQh
G4`4r8
G4+G0
G4+G0j
G8_^[]
G88WP
gamma
Gamma
gamma is expected to have 1 dimension, got 
gamma is expected to have 1 dimensions, got 
gamma is expected to have size of 
Gamma should be of shape (hidden_size). 
gates
Gather
gather axis value not expected
gather indices not matched.
gather input 1 value is not expected
Gather node in path 2 is not linked to another subgraph.
Gather Tind type not supported in this build.
GatherElements
GatherElements op: Cannot operate on scalar input
GatherElements op: Data type of input 'data' should match the data type of the output
GatherElements op: 'indices' shape should have values within bounds of 'data' shape. Invalid value in indices shape is: 
GatherElements op: Rank of input 'data' needs to be equal to rank of input 'indices'
GatherElements op: Value in indices must be within bounds [
GatherND
GatherNDBase PrepareForCompute: Input count mismatch
Gaussian Error Linear Unit.
GD;CDuL
Gelu approximation
GeluApproximation
GeluFusion
Gemm bias is not constant
Gemm bias is not constant initializer
Gemm bias shape is not expected
Gemm bias shape not expected
Gemm does not have 3 inputs
Gemm weight is not constant initializer
Gemm weight shape is not expected
GEMM: Dimension mismatch, W: 
Gemm: Invalid bias shape for broadcast
GemmActivationFusion
GENERAL ERROR
generic
generic-type-
Georgian
Get preallocated buffer for initializer '
GetCPInfo
GetCurrentProcess
GetCurrentProcessId
GetCurrentThread
GetCurrentThreadId
GetEnvironmentVariableA
GetFileAttributesA
GetFileAttributesW
GetFileLength: File is too large
GetFileSizeEx
GetFileSizeEx 
GetFinalPathNameByHandle() failed: 
GetFinalPathNameByHandleW
GetFullPathNameA
GetFusedActivationAttr(info, activation_).IsOK()
GetLastError
GetLocaleInfoEx
GetLogicalProcessorInformation
GetModuleFileNameA
GetModuleHandleExW
GetModuleHandleW
GetNativeSystemInfo
GetProcAddress
GetProcessHeap
GetProvider
GetRestrictedErrorInfo
GetStringTypeW
GetSystemInfo
GetSystemTimeAsFileTime
GetSystemTimePreciseAsFileTime
GFQPQ
GH;CHuT
Gh+Gd
GHPRQj
Given `data` tensor of rank r >= 1, and `indices` tensor of rank q >= 1, gather
Given model could not be parsed while creating inference session. Error message: 
GivenTensorFill
GL;CLud
Gl;Gp
GL+GH
Glagolitic
global
global_bias
global_weight
GlobalAveragePool
GlobalLpPool
GlobalMaxPool
Got invalid dimensions for input: 
Got nullptr for input tensor.
Got nullptr for sequence input.
Got nullptr from GetKernel for node: 
Got nullptr input for index tensor
Got weights of size: 
Gothic
GP;FPt
GP+GL
GPT2Gelu
GQPPj
GradientTensor
-grams
Grantha
graph
GRAPH
Graph
Graph attribute inferencing failed: 
Graph attribute inferencing returned type information for 
Graph attribute value was null. Invalid ORT format model.
Graph ctor should have created NodeArg for initializer. Missing:
Graph has 
Graph in 'body' attribute of Loop should have 
Graph is null. Invalid ORT format model.
Graph must be in single static assignment (SSA) form, however '
Graph state to be loaded into must be empty.
Graph to run if condition is false. Has N outputs: values you wish to be live-out to the enclosing scope. The number of outputs must match the number of outputs in the then_branch.
Graph to run if condition is true. Has N outputs: values you wish to be live-out to the enclosing scope. The number of outputs must match the number of outputs in the else_branch.
Graph transformers must be registered before the session is initialized.
graph_->GetNode(idx) != nullptr
graph_index
graph_inputs_excluding_initializers_.empty() && graph_inputs_including_initializers_.empty() && value_info_.empty() && graph_outputs_.empty()
graph_optimization_level
graph_optimization_level <= TransformerLevel::MaxLevel
graph_optimization_level is not valid
graph_optimization_level option in the model file must be an integer
graph_proto != nullptr
graph_proto cannot be null
graph_proto_ is not in sync with name_to_initial_tensor_.
GraphProto attribute inferencing is not enabled in this InferenceContextImpl instance.
GRAPHS
Greater
GreaterOrEqual
Greek
group
group count is <= 0
GroupCount
GrowStack() failed: 
GRU operator does not support double yet
GRUUnit
GRUUnit computes the activations of a standard GRU,
gsl::narrow_cast<int64_t>(input_axes_.size()) == num_scan_inputs_
gsl::narrow_cast<int64_t>(input_shape.Size()) == size
gsl::narrow_cast<int64_t>(output_axes_.size()) == num_scan_outputs
gsl::narrow_cast<int64_t>(tensor_shape.NumDimensions()) >= slice_dimension
gsl::narrow_cast<int64_t>(X_shape.NumDimensions()) >= axis
GSWWSQQh
GT;GX
Gujarati
Gunjala_Gondi
Gurmukhi
GWWRQQh
GWWRSQh
GWWShL
GWWSQQh
GWWWS
GWWWSh 
GX;G\
GxPtO
GYYf;
h != kInvalidChunkHandle
h < chunks_.size()
H$9_<uI
H|+Hx
H|+Hx3
H0d0n0
H0P1X2
H1X1~1
half_pixel
Hangul
Hanifi_Rohingya
Hanunoo
Hardmax
Hardmax inputs N, D and N * D must be < 
hardsigmoid
HardSigmoid
has output size 
has_starts && has_ends && attr_starts_.size() == attr_ends_.size()
HasDataType(dense_proto)
HasExclusiveSum
Hatran
Having memory pattern enabled is not supported while using the DML Execution Provider. 
HeapAlloc
HeapFree
Hebrew
height_scale
helper.HaveTwoTensorInputs()
hidden
hidden_prev
hidden_size
hidden_size != num_heads * head_size
HiddenInitTensor
hipMalloc
Hiragana
Hl;Hp
host unreachable
However, the number of key is 
hResult
https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,
https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html
Hx+Ht
I != nullptr
i < input_shape.NumDimensions()
i < tensors_.size()
I.e. the output shape should be [C][0] or [N][C][0] if input shape was [N][C].
id >= 0 && static_cast<size_t>(id) < ort_value_info_.size()
identifier removed
Identity
IExecutionProvider constructor must be called with true for use_metadef_id_creator
IExecutionProvider::Compile with fused Node and dll path is not implemented by 
IExecutionProvider::Compile with fused Node is not implemented by 
IExecutionProvider::Compile with FusedNodeAndGraph is not implemented by 
If `axes` are omitted, they are set to `[0, ..., ndim-1]`.
If 0, normalize the mean only.  Default is 1.
If 1, mean and variance are computed across channels. Default is 0.
if coordinate_transformation_mode is "align_corners", <br/>
if coordinate_transformation_mode is "asymmetric", <br/>
if coordinate_transformation_mode is "half_pixel", <br/>
if coordinate_transformation_mode is "pytorch_half_pixel", <br/>
if coordinate_transformation_mode is "tf_crop_and_resize", <br/>
if coordinate_transformation_mode is "tf_half_pixel_for_nn", <br/>
If keepdims equal 0, then the resulting tensor have the reduced dimension pruned.
If necessary the right-hand-side argument will be broadcasted to match the
If node has 
'If' node has 
If scale is not provided, crop the borders as provided.
If set to 1 will perform the sums in reverse direction.
If set to 1 will return exclusive sum in which the top element is not included. In other terms, if set to 1, the j-th output element would be the sum of the first (j-1) elements. Otherwise, it would be the sum of the first j elements.
If set to 1, the weight of sampling locations outside the tensor will be set to 0 and the weight will be renormalized so that their sum is 1.0. The default value is 0.
If set to nonzero, run spatial batch normalization in test mode, default is 0.
If set, defines the broadcast dimensions.
If set, defines the broadcast dimensions. See doc for details.
If shape was concrete we shouldn't be using a custom allocator
If the tokenizer receives empty input of [0] then the output is [0] if empty input
If the value of map_form is 'SPARSE,' this attribute indicates the total length of the output tensor.
If tokenizer removes the entire content of [C]-input, it will produce [[]].
If true and category is not present, will return all zeros; if false and a category if not found, the operator will fail.
If true, compute the mean and variance across all spatial elements If false, compute the mean and variance across per feature.Default is 1.
If true, compute the mean and variance across per activation. If false, compute the mean and variance across per feature over each mini-batch.
If value is 1, output type is uint32_t, else int32_t. Default value is 1.
ignore_index
Ignoring unsupported session option in ORT config: 
illegal byte sequence
illegal input path:
ImageScaler
Imperial_Aramaic
impl_->max_gram_length_ >= impl_->min_gram_length_
impl_->max_skip_count_ >= 0
impl_->min_gram_length_ > 0
impl_->weighting_criteria_ != kNone
impl_->weights_.size() == impl_->ngram_indexes_.size()
imputed_value_floats
imputed_value_int64s
imputed_values_float_.empty() ^ imputed_values_int64_.empty()
Imputer
in a sequence-length aware fashion.
in onnx/defs/schema.h).
in the inclusive range [
in[idx]->IsTensor()
inappropriate io control operation
IncludePadding
Incompatible dimensions
Incompatible dimensions for matrix multiplication
Incompatible matrix dimensions for matMul
Inconsistent shape in loop output for output. 
Incorrect arena extend strategy.
Incorrect or missing attribute value for starts and ends
Incorrect or missing input value for starts and ends
index < data_.size()
index >= 0 && static_cast<size_t>(index) < inputs.size()
index >= 0 && static_cast<size_t>(index) < outputs.size()
index out of range
Index tensor shape should be same as that of the input data tensor to unpool.
IndexDimensions
IndexedSubGraph contains values not present in the Graph
Indicate up to which input dimensions (exclusive) should be flattened to the outer dimension of the output. The value for axis must be in the range [0, R], where R is the rank of the input tensor. When axis = 0, the shape of the output tensor is (1, (d_0 X d_1 ... d_n), where the shape of the input tensor is (d_0, d_1, ... d_n). 
Indicate up to which input dimensions (exclusive) should be flattened to the outer dimension of the output. The value for axis must be in the range [-r, r], where r is the rank of the input tensor. Negative value means counting dimensions from the back. When axis = 0, the shape of the output tensor is (1, (d_0 X d_1 ... d_n), where the shape of the input tensor is (d_0, d_1, ... d_n). 
Indicates the transform to apply to the regression output vector.<br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
Indicates the transform to apply to the score. <br> One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT.'
Indicates the transform to apply to the score. <br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
Indicates the transform to apply to the score. <br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT.'
Indicates the transform to apply to the scores vector.<br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
Indicates whether to do OvR or multinomial (0=OvR is the default).
Indicates whether to only output as many values as are in the input (dense), or position the input based on using the key of the map as the index of the output (sparse).<br>One of 'DENSE', 'SPARSE'.
Indices
indices
Indices and updates must have the same rank
Indices dim=
indices element out of data bounds, idx=
Indices must have the same rank as Input. Indices rank=
indices tensor data type not supported
indices tensor must has rank larger than 0
Indices tensor must have rank >= 1
Indices vs updates dimensions differs at position=
IndicesDimensionCount
IndicesTensor
Indicies expected to be INT64
ineIu
InferenceSession is null. Invalid ORT format model.
Inferred elem type differs from existing elem type: (
Inferred shape and existing shape differ in dimension 
Inferred shape and existing shape differ in rank: (
InfinityMode
info == nullptr
info.GetAttr("alpha", &alpha_).IsOK()
info.GetAttr("beta", &beta_).IsOK()
info.GetAttr("blocksize", &blocksize_).IsOK()
info.GetAttr("direction", &direction).IsOK()
info.GetAttr("direction", &direction_).IsOK()
info.GetAttr("hidden_size", &hidden_size_).IsOK()
info.GetAttr("hidden_size", &int64_value).IsOK() && int64_value > 0
info.GetAttr("keepdims", &keepdims).IsOK()
info.GetAttr("linear_before_reset", &int64_value).IsOK()
info.GetAttr("num_heads", &num_heads).IsOK() && num_heads > 0
info.GetAttr("scale", &scale_).IsOK()
info.GetAttr("storage_order", &storage_order).IsOK()
info.GetAttr<float>("alpha", &alpha_).IsOK()
info.GetAttr<float>("beta", &beta_).IsOK()
info.GetAttr<float>("high", &high_).IsOK()
info.GetAttr<float>("low", &low_).IsOK()
info.GetAttr<float>("mean", &mean_).IsOK()
info.GetAttr<float>("scale", &scale_).IsOK()
info.GetAttr<float>("spatial_scale", &spatial_scale_).IsOK()
info.GetAttr<int64_t>("across_channels", &across_channels_).IsOK()
info.GetAttr<int64_t>("axis", &axis_).IsOK()
info.GetAttr<int64_t>("batch_axis", &batch_axis).IsOK()
info.GetAttr<int64_t>("count_include_pad", &temp).IsOK()
info.GetAttr<int64_t>("default_int64", &default_int_).IsOK()
info.GetAttr<int64_t>("dtype", &dtype).IsOK()
info.GetAttr<int64_t>("max_map", &max_map_).IsOK()
info.GetAttr<int64_t>("normalize_variance", &normalize_variance_).IsOK()
info.GetAttr<int64_t>("num_scan_inputs", &num_scan_inputs_).IsOK()
info.GetAttr<int64_t>("p", &p_).IsOK()
info.GetAttr<int64_t>("sample_size", &num_samples_).IsOK()
info.GetAttr<int64_t>("size", &size).IsOK()
info.GetAttr<int64_t>("targets", &num_targets_).IsOK()
info.GetAttr<int64_t>("time_axis", &time_axis).IsOK()
info.GetAttr<int64_t>("transA", &temp).IsOK()
info.GetAttr<int64_t>("transB", &temp).IsOK()
info.GetAttr<int64_t>("upper", &temp).IsOK()
info.GetAttr<ONNX_NAMESPACE::GraphProto>("body", &proto).IsOK()
info.GetAttr<ONNX_NAMESPACE::GraphProto>("else_branch", &proto).IsOK()
info.GetAttr<ONNX_NAMESPACE::GraphProto>("then_branch", &proto).IsOK()
info.GetAttr<std::string>("auto_pad", &auto_padding).IsOK()
info.GetAttr<std::string>("cast_to", &attr).IsOK()
info.GetAttr<std::string>("default_string", &default_string_).IsOK()
info.GetAttr<std::string>("equation", &equation_).IsOK()
info.GetAttr<std::string>("map_form", &attr).IsOK()
info.GetAttr<std::string>("metric", &metric).IsOK()
info.GetAttr<std::string>("mode", &mode).IsOK()
info.GetAttr<std::string>("norm", &norm).IsOK()
info.GetAttrs("activations", activations_).IsOK()
info.GetAttrs("axes", axes_).IsOK()
info.GetAttrs(std::is_same<AttrType, std::string>::value ? "string_vocabulary" : "int64_vocabulary", vocabulary_).IsOK()
info.GetAttrs<float>("bias", bias_).IsOK()
info.GetAttrs<float>("coefficients", coefficients_).IsOK()
info.GetAttrs<float>("kernel_params", kernel_params).IsOK()
info.GetAttrs<float>("rho", rho_).IsOK()
info.GetAttrs<float>("scales", scales_).IsOK()
info.GetAttrs<int64_t>("cats_int64s", int_categories).IsOK()
info.GetAttrs<int64_t>("kernel_shape", kernel_shape).IsOK()
info.GetAttrs<int64_t>("kernel_shape", kernel_shape_).IsOK()
info.GetAttrs<int64_t>("pooled_shape", pooled_shape).IsOK()
info.GetAttrs<int64_t>("shape", shape).IsOK()
info.GetAttrs<std::string>("cats_strings", string_categories).IsOK()
info.GetAttrs<std::string>("classes_strings", string_classes).IsOK()
info.GetAttrs<std::string>("classlabels_strings", classlabels_strings_).IsOK() || info.GetAttrs<int64_t>("classlabels_ints", classlabels_ints_).IsOK()
info.GetAttrs<TKey>(_key_field_name, keys).IsOK()
info.GetAttrs<TValue>(_value_field_name, values).IsOK()
info_ == nullptr
Inherited
initial_c
initial_h
initial_state_and_scan_inputs
Initial_ZeroPoint_FP
initialize preallocated buffer failed
InitializeCriticalSectionAndSpinCount
InitializeCriticalSectionEx
Initialized tensor with unexpected type: 
Initializer 
Initializer tensor is missing. Invalid ORT format model.
Initializer with same name exists. Name:
InitializeSListHead
InitializeSRWLock
Initializing session.
InitOnceExecuteOnce
Input
input
input 
Input 
Input 0 and 1 shall have same shape
Input 0 and 7 (mask) shall have same shape
Input 0 dimension 2 should be divisiable by value of the num_heads attribute.
Input 0 is expected to have 1 or more dimensions, got 
Input 1 dimension 0 should have same length as dimension 2 of input 0
Input 1 dimension 0 should have same length as the last dimension of input 0
Input 1 is expected to have 1 dimensions, got 
Input and output types can be of any tensor type.
Input and target dimension value mismatch.
input and zero_point pair is expected to have be same type.
input and zero_point pair is expected to have same type.
input array doesn't equal tensor size
input array is too short
Input axes has incorrect length
Input axes has invalid data
Input axis is invalid: 
Input B must have shape {
Input B should not be null.
Input 'bias' dimension 0 should have same length as dimension 1 of input 'weights'
Input 'bias' is expected to have 1 dimension, got 
Input can be of any tensor type.
Input cannot be split evenly on selected axis. Input shape=
Input channels C is not equal to kernel channels * group.
Input channels is not divisible by group.
Input contains invalid utf8 chars at: 
input count mismatch
input count mismatch, expected 1 input - the tensor to be processed
input count mismatch, expected 2 inputs - the tensor to be processed and a tensor containing k value
Input count of Tile OP mismatch, the first one is empty
Input count of Tile OP mismatch, the second one is empty
Input data tensor from the previous layer.
Input data tensor from the previous operator; 4-D feature map of shape (N, C, H, W), where N is the batch size, C is the number of channels, and H and W are the height and the width of the data.
Input data to be scaled
Input data type does not match the expected data type
Input data type does not match the expected data type. Current data type is 
Input data type is not int32 or int64
Input data with index: 
Input 'depth' must be a scalar or rank 1 tensor.
Input 'depth' must have exactly one element.
Input dim is zero but required output dim is non-zero. 
Input dimension cannot be less than 3.
Input dimensions are either [C] or [N][C] allowed
Input dimensions are either[C > 0] or [1][C > 0] allowed
input edges
Input element type of 
Input features_per_batch[
Input id is not valid. 
input index out of range
input index: 
Input initial_c must have shape {
Input initial_h must have shape {
Input 'input' is expected to have 3 dimensions, got 
Input is ether string UTF-8 or int32/int64
input is expected to have 3 dimensions, got 
Input is expected to have dim value in all dimensions.
Input is expected to have four dimensions corresponding to [N,C,H,W]
Input is expected to have four dimensions corresponding to [N,C,H,W], got 
Input is not of one of the supported map types.
Input is not of one of the supported sequence types.
Input is not of type sequence or map.
Input 'mask_index' is expected to have 1, 2 or 3 dimensions, got 
'input' must have rank >= 2
input name cannot be empty
Input of int64 must have output of string 
Input of reshape_before_gemm is not the input of subgraph
Input of string must have output of int64
Input of tensor(int64) must have output of tensor(string)
Input of tensor(string) must have output of tensor(int64)
Input P must have shape {
Input 'past' is expected to have 5 dimension, got 
Input R must have shape {
Input rank must be >= 2.
input scale must be a scalar or 1D tensor of size 1
Input 'scales' must have float element type.
Input Sequence and Tensor are expected to have the same elem type. Sequence=
Input Sequence and Tensor are expected to have type info. Current type is null.
Input sequence_lens must have shape {
Input shape dimensions mismatch:
Input shape had more than 2 dimension. Dims=
Input shape is unknown or not 2D, or data type unknown
Input shape must have either [C] or [1,C] dimensions where C > 0
Input shape must have either [C] or [B,C] dimensions with B > 0.
Input shape needs to be at least a single dimension.
input shape: 
Input 'sizes' must have int64 element type.
Input 'split' can not be empty.
Input steps has incorrect length
Input string contains invalid utf8 chars: 
Input tensor
input tensor
Input tensor A. The shape of A should be (M, K) if transA is 0, or (K, M) if transA is non-zero.
input tensor and indices tensor must has rank larger than 0. 
Input tensor B. The shape of B should be (K, N) if transB is 0, or (N, K) if transB is non-zero.
Input tensor C. The shape of C should be unidirectional broadcastable to (M, N).
Input tensor can be of arbitrary type.
Input tensor has no dimensions
Input tensor must be 2-dimensional
Input tensor must be 4-dimensional
Input tensor must have at least 2 dimensions
Input tensor must have atleast 2 dimensions
Input tensor must have rank 1 or 2
Input tensor must have rank 2
Input tensor of rank 2 or higher.
Input tensor of shape [N,C,H,W]
Input tensor should have a rank of at least 2
Input tensor to Unique op should be 1D
Input tensor X must have atleast 2 dimensions.
Input tensor.
Input tensor. Every matrix in the batch must be invertible.
Input tensors of wrong rank (0).
Input to be reduced is null
Input to 'Range' op should be scalars (Tensor with only one element and shape empty)
Input to set must exist.
Input type for input at index 
Input type for input at index 0 is null. Type info is expected.
Input type is not float tensor but keys_floats is set
Input type is not int64 tensor but keys_int64s is set
Input type is not string tensor but key_strings is set
Input type was null
Input 'values' must be rank 1 tensor.
Input 'values' must have exactly two elements.
Input W must have shape {
Input was expected to have either tensor or sequence type. Got 
Input was expected to have sequence type. Got 
Input was expected to have tensor type. Got 
Input 'weights' dimension 1 should be 3 times of dimension 0
Input 'weights' is expected to have 2 dimensions, got 
Input with name: 
Input X must have 3 dimensions only. Actual:
Input x_scale must be a scalar or 1D tensor of size 1
input x_zero_point must be a scalar or 1D tensor of size 1 if given
input y_scale must be a scalar or 1D tensor of size 1
input y_zero_point must be a scalar or 1D tensor of size 1 if given
input zero point must be a scalar or 1D tensor of size 1.
input.Shape().NumDimensions() == 4
Input/Output is a string tensor
input_0
input_1
input_1.DataType() == input_2.DataType()
input_arg->Type() != nullptr
input_as_shape
input_copy_needed != DeviceCopyCheck::Unknown && output_copy_needed != DeviceCopyCheck::Unknown
input_count >= 0 && static_cast<size_t>(input_count) == input_dimensions_.size()
input_count >= 1
input_depth % (blocksize_ * blocksize_) == 0
input_dims.size() >= 2
input_dims[rank - 2] == input_dims[rank - 1]
input_forget
input_gather_element
input_gather_element_transform
input_height % this->blocksize_ == 0
input_ids
Input_ids and segment id should have the same shape. 
input_ids is expected to have 2 dimensions, got 
input_indices.size() == expected_values.size() && input_indices.size() > 0
input_node.InputDefs().size() == 2 && scale_and_index.value().second < 2
input_num_bytes % 4 == 0
input_rank == permutation.size()
input_scale
input_sequence
input_shape.NumDimensions() == 4
input_shape.Size() > 0 || input_shape[0] == 0
input_shape.Size() > 0 || N == 0
input_shape[i] == 1
input_shape_1_override.size() == 3 && input_shape_2_override.size() == 3
input_shape_1_override[0] == input_shape_2_override[0]
input_shape_1_override[2] == input_shape_2_override[1]
input_size < std::numeric_limits<std::ptrdiff_t>::max()
input_tensor != nullptr && indices_tensor != nullptr
input_tensor_ptr != nullptr
input_tensor_ptr->Shape().Size() == input_shape_override->Size()
input_width % this->blocksize_ == 0
input_zero_point
input->Exists()
InputBroadcaster can only start at span boundary!
InputCount
inputCount >= 1
InputDimensionCount
inputdimensions
inputdimensions attribute must be provided
InputFirstMomentTensor
InputGradientTensor
InputParametersTensor
InputPixelOffsets
inputs
Inputs
Inputs 0 shall be 2 dimensions
Inputs 0 shall be 3 dimensions
Inputs 4 shall be 5 dimensions
inputs are expected to have tensor type and output type should not be null.
inputs are expected to have tensor type.
inputs by their magnitude, rather than gates inputs by their sign as in ReLUs.
Inputs have ellipses in them but the provided output subscript does not contain an ellipsis
Input's height (
Inputs 'mask_index' dimension 0 shall have length of batch_size or 2 * batch_size
Inputs 'mask_index' of 3d shall have shape batch_size x sequence_length x (past_sequence_length + sequence_length)
Inputs 'mask_index' with raw attention mask shall have shape batch_size x (past_sequence_length + sequence_length)
Inputs 'past' dimension 0 shall have length of 2
Inputs 'past' dimension 1 shall have same length as dimension 0 of input 0
Inputs 'past' dimension 2 shall have length of 
Inputs 'past' dimension 2 shall have length of num_heads
Input's shape must be 4-D
Input's shape should be 1D or 2D
Input's width (
inputs_n_rank == inputs_0_rank
InputScaleTensor
InputSecondMomentTensor
InputStateTensor
InputTensor
InputTensors
InputWindowOffsets
InputWindowSizes
InputWindowStrides
InputZeroPointTensor
Inscriptional_Pahlavi
Inscriptional_Parthian
Insert and concatenate on a new axis or not, default 0 means do not insert new axis.
InsertCastTransformer works on the assumption that `dtype` attribute holds an integer.
Inserted_Cast
InstanceNormalization
Insufficient dimensions to slice on 
int16
int32
int32_data
int64
Int64 tensor
int64_data
int64_vocabulary
Integer indicate the format of the box data. The default is 0. 0 - the box data is supplied as [y1, x1, y2, x2] where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Mostly used for TF models. 1 - the box data is supplied as [x_center, y_center, width, height]. Mostly used for Pytorch models.
Integer overflow
Integer representing the embedding vector size for each char.If not provide, use the char embedding size of embedding vector.
Integer representing the embedding vector size for each word.If not provide, use the fileter size of conv weight
inter_op_num_threads
inter_op_num_threads option in the model file must be an integer
intercepts
InterlockedFlushSList
InterlockedPushEntrySList
internal error
Internal error in BatchNormalizationMulFusion. BatchNormalization_B_tensor_proto is NULL
Internal error.
Internal error. The preallocated buffer is too small. Requires 
InternalName
InternalTestingExecutionProvider
inter-op
InterpolationMode
interrupted
intra_op_num_threads
intra_op_num_threads option in the model file must be an integer
intra-op
inv_std_var
Invalid activation function of 
Invalid allocation kind: 
invalid allocator
Invalid arg_num of 
invalid argument
Invalid argument for depth; it's not a scalar.
Invalid argument for values; either it's rank is more than 1 or it has more than 2 elements
Invalid argument: input has empty dimensions.
Invalid argument: X input has empty dimensions.
Invalid assumption of output element size
Invalid attribute perm {
Invalid axes attribute, axes attribute (if present) should have the same size as starts/ends attributes
Invalid batch_axis of 
invalid BOM; must be 0xEF 0xBB 0xBF if given
Invalid CAST_TO value of 
invalid character class
invalid character class range
Invalid data type for GRU operator of 
Invalid data type for LSTM operator of 
Invalid data type for split tensor 
Invalid data type of 
Invalid DataTypeImpl TypeProto definition
Invalid destination node arg slot specified when adding edge.
Invalid destination node arg slot specified when removing edge.
Invalid dim0_offset of 
Invalid dimension of 
Invalid dimension value: 
Invalid 'direction' argument of '
Invalid direction value of '
Invalid dtype of 
Invalid 'end'. Value is larger than 'start'.
Invalid entries in sequence_lens. Max sequence length was 
invalid escape sequence
Invalid ExecutionOrder
invalid expand shape
Invalid fd was supplied: 
Invalid Feed Input Name:
Invalid free dimension override.
Invalid GRU hidden gate activation function: 
Invalid GRU reset gate activation function: 
invalid hash bucket count
invalid index 
invalid index found, index = 
Invalid index requested for map type.
invalid indice found, indice = 
Invalid input B: 
Invalid input B: 0th dimension != 
Invalid input B: number of dimensions is not 1: 
Invalid input B: NumDimensions() != 
Invalid input data: number of dimensions is less than 3: 
Invalid input index for node 
Invalid input mean: 
Invalid input mean: 0th dimension != 
Invalid input mean: NumDimensions() != 
Invalid input scale: 
Invalid input scale: 0th dimension != 
Invalid input scale: number of dimensions is not 1: 
Invalid input scale: NumDimensions() != 
Invalid input shape. Only N can be zero. Got:
Invalid input shape: 
Invalid input type of value: 
Invalid input type:
Invalid input var: 
Invalid input var: 0th dimension != 
Invalid input var: NumDimensions() != 
Invalid input X: The rank of input X must be atleast 2. Got rank: 
invalid literal
invalid location range
Invalid LSTM merge activation function of 
invalid map<K, T> key
Invalid 'mode' attribute value
Invalid mode of value 
invalid named capture group
Invalid node indexes specified when adding edge.
Invalid node indexes specified when removing edge.
Invalid normalize value of 
invalid number; expected '+', '-', or digit after exponent
invalid number; expected digit after '-'
invalid number; expected digit after '.'
invalid number; expected digit after exponent sign
Invalid ORT format model.
invalid ort_value_index:
Invalid Output Name:
Invalid PACK_MAP value of 
Invalid 'pads' attribute value
invalid perl operator
Invalid position of 0
Invalid program_counter entries at index 
Invalid rank for input: 
Invalid RE2: 
invalid repetition size
Invalid roi input index.
Invalid run log severity level. Not a valid onnxruntime::logging::Severity value: 
Invalid scan input:
invalid seek
Invalid sequence index (
Invalid session log severity level. Not a valid onnxruntime::logging::Severity value: 
Invalid shape value: 
Invalid source node arg slot specified when adding edge.
Invalid source node arg slot specified when removing edge.
Invalid SparseTensor indices. Should be rank 0 or 1. Got:
Invalid SparseTensor indices. Should either have raw or int64 data
Invalid 'start'. Value is smaller than previous 'end'.
Invalid start/ending offset [
invalid stod argument
invalid stof argument
invalid stoll argument
invalid stoull argument
invalid string position
invalid string: '\u' must be followed by 4 hex digits
invalid string: control character U+0000 (NUL) must be escaped to \u0000
invalid string: control character U+0001 (SOH) must be escaped to \u0001
invalid string: control character U+0002 (STX) must be escaped to \u0002
invalid string: control character U+0003 (ETX) must be escaped to \u0003
invalid string: control character U+0004 (EOT) must be escaped to \u0004
invalid string: control character U+0005 (ENQ) must be escaped to \u0005
invalid string: control character U+0006 (ACK) must be escaped to \u0006
invalid string: control character U+0007 (BEL) must be escaped to \u0007
invalid string: control character U+0008 (BS) must be escaped to \u0008 or \b
invalid string: control character U+0009 (HT) must be escaped to \u0009 or \t
invalid string: control character U+000A (LF) must be escaped to \u000A or \n
invalid string: control character U+000B (VT) must be escaped to \u000B
invalid string: control character U+000C (FF) must be escaped to \u000C or \f
invalid string: control character U+000D (CR) must be escaped to \u000D or \r
invalid string: control character U+000E (SO) must be escaped to \u000E
invalid string: control character U+000F (SI) must be escaped to \u000F
invalid string: control character U+0010 (DLE) must be escaped to \u0010
invalid string: control character U+0011 (DC1) must be escaped to \u0011
invalid string: control character U+0012 (DC2) must be escaped to \u0012
invalid string: control character U+0013 (DC3) must be escaped to \u0013
invalid string: control character U+0014 (DC4) must be escaped to \u0014
invalid string: control character U+0015 (NAK) must be escaped to \u0015
invalid string: control character U+0016 (SYN) must be escaped to \u0016
invalid string: control character U+0017 (ETB) must be escaped to \u0017
invalid string: control character U+0018 (CAN) must be escaped to \u0018
invalid string: control character U+0019 (EM) must be escaped to \u0019
invalid string: control character U+001A (SUB) must be escaped to \u001A
invalid string: control character U+001B (ESC) must be escaped to \u001B
invalid string: control character U+001C (FS) must be escaped to \u001C
invalid string: control character U+001D (GS) must be escaped to \u001D
invalid string: control character U+001E (RS) must be escaped to \u001E
invalid string: control character U+001F (US) must be escaped to \u001F
invalid string: forbidden character after backslash
invalid string: ill-formed UTF-8 byte
invalid string: missing closing quote
invalid string: surrogate U+DC00..U+DFFF must be followed by U+DC00..U+DFFF
invalid string: surrogate U+DC00..U+DFFF must follow U+D800..U+DBFF
Invalid Target shape product of 0
Invalid tensor shape slice argument.
Invalid TensorProto
Invalid time_axis of 
Invalid type
invalid unordered_map<K, T> key
Invalid usage. Input 1 is a shape with no data.
invalid UTF-8
Invalid value for attribute axis
Invalid value for attribute k
Invalid value in scan_input_axes for input 
Invalid value in scan_output_axes for output 
Invalid value in 'split' attribute. All values must be > 0
Invalid value in 'split' input. All values must be >= 0
Invalid value of attribute 'axis'. Accepted range=[
Invalid value of attribute 'axis'. Rank=
Invalid value(
Invalid value/s in sequence_lens. All values must be > 0 and < seq_length. seq_length=
Invalid values in '
invalid vector<T> subscript
Invalid Y argument: index is out of range: Y[
Invalid Y argument: num_indices = 0
INVALID_ARGUMENT
INVALID_GRAPH
invalid_iterator
INVALID_PROTOBUF
Inverse
inverse_indices
io error
ios_base::badbit set
ios_base::eofbit set
ios_base::failbit set
iostream
iostream stream error
iou_threshold
iou_threshold must be in range [0, 1].
Irfft
irVersion
is a directory
is applied to the data tensor elementwise.
is applied to the tensor elementwise.
'is defined.
is not supported.
is_case_sensitive
is_concrete_shape_
is_model_proto_parsed
is_test
IsDebuggerPresent
IsInf
ISink must be provided.
IsNaN
IsNan
IsProcessorFeaturePresent
isRedist
IsScalarOr1ElementVector(a_offset)
IsScalarOr1ElementVector(a_scale)
IsScalarOr1ElementVector(a_scale_tensor)
IsScalarOr1ElementVector(a_zero_point)
IsScalarOr1ElementVector(a_zero_point_tensor)
IsScalarOr1ElementVector(b_offset)
IsScalarOr1ElementVector(b_scale)
IsScalarOr1ElementVector(b_scale_tensor)
IsScalarOr1ElementVector(b_zero_point)
IsScalarOr1ElementVector(b_zero_point_tensor)
IsScalarOr1ElementVector(k)
IsScalarOr1ElementVector(tensor_a_scale)
IsScalarOr1ElementVector(tensor_b_scale)
IsScalarOr1ElementVector(tensor_c_scale)
IsScalarOr1ElementVector(tensor_x_scale)
IsScalarOr1ElementVector(tensor_x_zero_point)
IsScalarOr1ElementVector(tensor_y_scale)
IsScalarOr1ElementVector(tensor_y_zero_point)
IsScalarOr1ElementVector(W_Zero_Point)
IsScalarOr1ElementVector(X_scale)
IsScalarOr1ElementVector(X_zero_point)
IsScalarOr1ElementVector(X_Zero_Point)
IsScalarOr1ElementVector(y_offset)
IsScalarOr1ElementVector(y_scale)
IsScalarOr1ElementVector(Y_scale)
IsScalarOr1ElementVector(Y_zero_point)
IsSparseTensor()
IsTensor()
IsTensorSequence()
it->i < (int64_t)predictions.size()
iteration_num_ < sequence_len_
iterator does not fit current value
iterator out of range
itr != node.InputDefs().end()
It's an extension of Gelu. It takes the sum of input A and bias input B as the input of Gelu activation. 
j h j=
j h@K>
j hh\>
j WPj W
j Z_^
j$h(j>
j$hPe=
j$Z_^
j&hlT>
J(+J$
j(hHyL
j(Y!>
j(Z_^
j)h0-=
j)hPe=
j,h0G>
j,hXn=
j,Zj(X
j.h8'>
j.hPQ>
j/hLH>
j/hXv>
j:h`]@
j?h`]@
j?h8'>
j\^f91u
j\Xf9
j^hp{=
j_h8==
j`h0z>
j+hP&=
j=h0Y>
j>hp{=
J0_0q0y0
j0h`]@
j0h0-=
j1hXv>
j2h@+=
j3h0-=
j3h8_=
j3hXv>
j4hP&=
j5h`]@
j7hh\>
j8hp{=
j9hPi=
jA^f;1
ja^f;1w
jAh0Y>
jAhPe=
Javanese
jaZf;Q
jAZf;Q
jBh01>
jbh0Y>
jChp{=
jDh0Y>
jdh8'>
jDhx1>
jEh0Y>
jEh8'>
jEhp{=
j'h8_=
j'hh\>
jHh8_=
jHhp{=
j-hP&=
j-hX#=
j-hXv>
jJh@+=
jJhp{=
jjjjj
jkhp{=
jLh@+=
jlhx/=
jmh0Y>
jMh0Y>
jNh0Y>
job_.size() = 
joZjpY
joZRY
jpZjoY
jqh 5>
jQh(#?
jqh(;<
jQh0Y>
jRh(2>
jrh8g>
jRhXf>
jSh8'>
Json stored in the `ort_config` key cannot be parsed. Error message: 
jTh0z>
jtYjt
jUh8'>
jVh0-=
jVh0z>
jwh0z>
jWh8_=
jXh0Y>
jXh0z>
jZh0z>
K +K$
k and v are not from same Split node
k argument [
K input must be a one-dimensional tensor of size 1.
K input must be of type int64.
k root is not layer norm
k should be a 1-D or 0-D tensor.
k tensor should be a 1D tensor of size 1
K,;K,u
K,;K0t
k_matmul and k_add shape not matched
k_reshape const not matched
k_temp > 0
k_transpose perm attribute not matched
K89K@
Kaithi
Kannada
Katakana
Kayah_Li
Keep the reduced dimension or not, default 1 mean keep reduced dimension.
Keep the split dimension or not. Default 1, which means we keep split dimension. If input 'split' is specified, this attribute is ignored.
keep_dims
keepdims
keepdims_
kernel != nullptr
Kernel create info hashes are null. Invalid ORT format model.
Kernel create info is null. Invalid ORT format model.
Kernel create info node indices are null. Invalid ORT format model.
kernel def can't be NULL
Kernel not found
kernel_params
kernel_shape
kernel_shape is not compatible with W shape.
kernel_shape num_dims is not compatible with W num_dims.
kernel_shape num_dims is not compatible with X num_dims.
kernel_shape[dim] > 0
kernel_shape_[dim] > 0
kernel_type
kernel32.dll
kernelbase.dll
key '
Key and value tensors have unequal number of elements.
Key type is not supported yet.
key_type
keys_floats
keys_int64s
keys_strings
Kharoshthi
KhH5=
Khmer
Khojki
Khudawadi
Klh`#>
KlhH#>
KlhT#>
known by the checker.
KpQhL}<
kRegexpCapture cap() == 0
KX;K\t
L$ ;L$,|
L$ ;N
L$ ;t$$
L$ ^[3
L$ PV
L$ PWQ
L$ RP
L$$;C,u
L$$;G,u
L$$;O
L$$^3
L$$_^[3
L$$_^3
L$$9M
L$(;K
L$(SQ
L$(SV
L$(Wj
L$, D$
L$,^3
L$,_^[3
L$,_^3
L$,RQ
L$,xO
L$@;G
L$@;H
L$@;J
L$\^3
L$\_^[3
L$\_^3
L$`WR
L$|_^[3
L$|_^3
L$<_^[3
L$<_^3
L$=QP
L$0;K
L$09y
L$0C;
L$4;H
L$4^3
L$4_^
L$4_^[3
L$4QVRPQVRP
L$4RP
L$8;M0
L$D_^[3
L$d_^[3
L$D_^[3
L$d_^[3
L$D_^[3
L$D_^3
L$h;J
L$H;L$0
L$H;M$
L$H9|$L
L$H9u
L$HxK
L$L;T$
L$L_^[3
L$l_^[3
L$L_^[3
L$l_^[3
L$L9|$P
L$lWVPu
L$PxK
L$t;M(
L$t_^[3
L$T_^[3
L$t_^[3
L$T_^[3
l?4?~?
l?4?~?4?~?4?~?4?~?
l<p<t<x<|<
L>2?~>2?~>2?~>2?~>
L>2?~>p
L1f1q2&3
l7G8X8M9a9=:M:
Label encoder has only one input.
Label encoder has only one output.
LabelEncoder
labels
lambd
largest
largest <= 1
Last dimension of `indices` input tensor in GatherND op must not be larger than the rank of `data` tensor
Last dimension of beta and input does not match
Last dimension of bias and input does not match
Last dimension of gamma and input does not match
last dimension of indices must not be larger and rank of data tensor
last dimension of indices must not be larger than rank of input tensor
last_outputs[j + 1].IsTensor()
last_results.last_loop_red_size > 0
last_results.last_loop_size > 0
last_results.projected_index.size() > 0
Latin
LayerNormalization
LayerNormFusion
LCMapStringEx
lda >= K && ldb >= K && ldc >= N
LeakyRelu
leakyrelu
LearningRate
LeaveCriticalSection
left operand cannot broadcast on dim 
Left shape: 
left.NumDimensions() == 2 || left.NumDimensions() == 1
left.Shape().Size() == left_shape_override.Size()
left_dim == right_dim
left_num_dims and right_num_dims must be >= 1
left_rank == right_rank
legacy optimization attribute.
LegalCopyright
len <= op_schema.inputs().size()
len >= 0 && static_cast<uint64_t>(len) < std::numeric_limits<size_t>::max()
length
length > buffer.size()
length of each output
length of each output. Values should be >= 0.
Length of permutation must match the rank of the input to be permutated
length overflow
lengths allocation failed
Lepcha
LessOrEqual
Level
Limbu
limit
limit in Range operator should be scalar like tensor, yet got shape:
Linear
linear
LINEAR
'Linear' mode only support 2-D inputs or 3-D inputs ('Bilinear', 'Trilinear') or 4-D inputs or 5-D inputs with the corresponding outermost 2 scale values being 1 in the 
Linear_A
Linear_B
linear_before_reset
LinearBeforeReset
LinearClassifier
LinearRegressor
list count 
List of 3 elements containing gamma, coef0, and degree, in that order. Zero if unused for the kernel.
List of categories, ints.<br>One and only one of the 'cats_*' attributes must be defined.
List of categories, strings.<br>One and only one of the 'cats_*' attributes must be defined.
list of floats. This attribute stores the weight of each n-gram in pool. The i-th element in weights is the weight of the i-th n-gram in pool. Its length equals to the size of ngram_indexes. By default, weights is an all-one tensor.This attribute is used when mode is "IDF" or "TFIDF" to scale the associated word counts.
List of int64 n-grams learned from the training set. Either this or pool_strings attributes must be present but not both. It's an 1-D tensor starting with the collections of all 1-grams and ending with the collections of n-grams. The i-th element in pool stores the n-gram that should be mapped to coordinate ngram_indexes[i] in the output vector.
list of int64s (type: AttributeProto::INTS). This list is parallel to the specified 'pool_*' attribute. The i-th element in ngram_indexes indicate the coordinate of the i-th n-gram in the output tensor.
List of integers indicate the padding element count at the beginning and end of each axis, for 2D it is the number of pixel. `paddings` rank should be double of the input's rank. `paddings` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded).
List of integers indicating the dimensions to squeeze. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
List of integers indicating the number of padding elements to add or remove (if negative) at the beginning and end of each axis. For 2D it is the number of pixels. `pads` rank should be double of the input's rank. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
List of non-negative integers, indicate the dimensions to be inserted
List of non-negative integers, indicate the dimensions to squeeze.
List of stop words. If not set, no word would be removed from X.
List of strings n-grams learned from the training set. Either this or pool_int64s attributes must be present but not both. It's an 1-D tensor starting with the collections of all 1-grams and ending with the collections of n-grams. The i-th element in pool stores the n-gram that should be mapped to coordinate ngram_indexes[i] in the output vector.
List of tensors for 
list<T> too long
Load model 
Load model from 
loadedFrom
LoadLibraryExA
LoadNodeArgsFromOrtFormat: Node [
Local\SM0:%d:%d:%hs
locale
LocalSize
localtime_s(&local_tm, &in_time_t) == 0
location
location dimensions do not match shape size
log_prob
LogHr
LOGISTIC
LogSoftmax
long 
LongformerAttention
Loop 'body' subgraph outputs should all be tensors but output 
Loop 'body' subgraph outputs should all be tensors or sequences but output 
Loop 'body' subgraph scan outputs should all be tensors but output 
Loop had zero iterations and the shape of subgraph output 
'Loop' input 'cond' should be a scalar tensor. Got shape of 
'Loop' input 'M' should be a scalar tensor. Got shape of 
'Loop' node has 
loop_body_attribute
loss_N1dd
loss_NCdd
loss_Ndd
loss_sum
loss_unweighted
LOWER
Lower boundary of the output values.
LpNormalization
LpPool
LSTM operator does not support double yet
Lycian
Lydian
M PRV
M$@Pj
M$SV3
M(PSW
M_ == 1 && N_ == 1 was false
M_ >= 0 && K_ > 0 && N_ >= 0
M0;M,|:
M0+M(
M0e0o0
M0f0y0
m0r0A1
Mahajani
Main Graph instance should have populated all subgraphs when being resolved.
Makasar
Malayalam
Malformed repeat 
Mandaic
Manichaean
Map is missing type entry for its value
map(int64, double)
map(int64, float)
map(int64, string)
map(string, double)
map(string, float)
map(string, int64)
map/set too long
map_form
map_form_ != PACK_MAP::SPARSE || max_map_ > 0
map_type
MapFileIntoMemory is not implemented on Windows.
Marchen
Masaram_Gondi
Mask data type is not int32 or int64 or float32
Mask is neither unidirectional nor all ones
Mask shape is unknown or not 2D, or data type unknown
mask_index
Mask_Int32
mask_mul const input not matched
mask_sub const input not matched
mask_unsqueeze_1 axes not matched. Expect: 1
mask_unsqueeze_2 axes not matched. Expect: 2
MaskCast
Match contains invalid utf8 chars: 
MatchInputMaskSubgraph returns false
MatchPastSubgraph returns false
MatchUnidirMaskSubgraph returns NULL
MatMul
MatMul dimension mismatch
MatMulAddFusion
MatMulInteger
MatmulInteger : input1 A_scale must be a scalar or 1D tensor of size 1
MatmulInteger : input1 A_zero_point must be a scalar or 1D tensor of size 1 if given
MatmulInteger : input1 B_scale must be a scalar or 1D tensor of size 1
MatmulInteger : input1 B_zero_point must be a scalar or 1D tensor of size 1 if given
MatmulInteger : input1 C_scale must be a scalar or 1D tensor of size 1
MatmulInteger : input1 C_zero_point must be a scalar or 1D tensor of size 1 if given
MatmulInteger : input1 zero point must be a scalar or 1D tensor of size 1
MatmulInteger : input2 zero point must be a scalar or 1D tensor of size 1
MatMulInteger16
MatMulIntegerToFloat
MatMulIntegerToFloat : input A scale must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
MatMulIntegerToFloat : input A zero point must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
MatMulIntegerToFloat : input B scale must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
MatMulIntegerToFloat : input B zero point must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
MatMulIntegerToFloatFusion
MatMulScaleFusion
Matrix dimensions are not equal. Square matrix is expected
Matrix multiply results
Matrix multiply results from A * B
Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html
Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html.
max should be a scalar.
max_gram_length
max_gram_length must be inbounds of ngram_counts: 
max_map
max_map must be > 0 if map_form is SPARSE
max_output_boxes_per_class
max_skip_count
max_skip_count is required
max_skip_count must be non-negative: 
max->Shape().NumDimensions() == 0
Maximum n-gram length. If this value is 3, 3-grams will be used to generate the output.
Maximum number of events reached, could not record profile event.
Maximum number of items (integers/strings) to be skipped when constructing an n-gram from X. If max_skip_count=1, min_gram_length=2, max_gram_length=3, this operator may generate 2-grams with skip_count=0 and skip_count=1, and 3-grams with skip_count=0 and skip_count=1
Maximum value, above which element is replaced by max
MaximumSamplesPerOutput
MaxPool
MaxpoolWithMask
MaxRoiPool
MaxUnpool
MaxUnpool op must have either two or three inputs.
MeanTensor
MeanVarianceNormalization
Medefaidrin
Meetei_Mayek
Mem pattern for initializer 
Mem pattern should be disabled when using DML execution provider.
mem_steps <= max_memory_steps_ && mem_steps > 0
Memcpy
MemcpyFromHost
MemcpyToHost
MemcpyTransformer
Memory pattern planner is not enabled on this execution framework.
Memory type 
memory_seq_lens
Mende_Kikakui
Meroitic_Cursive
Meroitic_Hieroglyphs
message size
metadef_id_generator_
Method IncrementIndexAndComputeOffset assumes this value is strictly positive.
metric
Microsoft
Microsoft Corporation
Microsoft.ML.ONNXRuntime
MIGraphXExecutionProvider
min should be a scalar.
min_ <= max_
min_gram_length
min_gram_length >= max_gram_length required: 
min_gram_length is required
min_gram_length must be inbounds of ngram_counts: 
Min_Scaled
min->Shape().NumDimensions() == 0
mincharnum
mincharnum is too big for char level tokenezation
mincharnum_ > 0
Minimum n-gram length. If this value is 2 and max_gram_length is 3, output may contain counts of 2-grams and 3-grams.
Minimum number of characters allowed in the output. For example, if mincharnum is 2, tokens such as "A" and "B" would be ignored
Minimum value, under which element is replaced by min
MinimumSamplesPerOutput
Mismatch between expected shape and shape from first output
Mismatch between Graph and IndexedSubGraph. Input not found:
Mismatch between Graph and IndexedSubGraph. Node not found: 
Mismatch between Graph and IndexedSubGraph. Output not found:
Mismatch between input data and B: size of B != input channel count 
Mismatch between input data and scale: size of scale != input channel count 
Mismatch between number of source and target dimensions. Source=
Mismatch between number of splits (
Mismatch between the sum of 'split' (
Mismatched attribute type in '
Mismatched data types between input and output Tensors. 
Mismatched tensor element type for output 
Mismatched tensor element type:
Mismatched type for output 
Mismatched type:
missing )
missing ]
Missing case in Compiler: 
Missing dimensions for initializer. Invalid ORT format model.
Missing dims for sparse initializer: 
Missing 'equation' attribute
Missing indicies for sparse initializer: 
Missing Input: 
Missing Model. Invalid ORT format model.
Missing name for SparseTensor initializer. Invalid ORT format model.
Missing opset in the model. All ModelProtos MUST have at least one entry that specifies which version of the ONNX OperatorSet is being imported.
Missing or invalid starts and ends attribute
Missing raw data for initializer. Invalid ORT format model.
Missing session state for subgraph. Node:'
Missing string data for initializer. Invalid ORT format model.
Missing values for sparse initializer. Invalid ORT format model.
Missing/Invalid 'axes' attribute value
Missing/Invalid 'axis' attribute value
Misuse of LoopStateVariable. Attempt to move beyond end of sequence
ml_type != nullptr
ML9uH
MLDataType for: 
mlvalue.Fence() == nullptr
mode attribute is 
mode is required
mode: 
Model file not found!
model format error!
model format error! Missing 'location'
model format error! Need a key for the external data info
model format error! Need a value for the external data info
Model must have opset imports. Invalid ORT format model.
Model was not loaded
Model was not loaded.
MODEL_LOADED
model_loading_array
model_loading_from_saved_proto
model_loading_proto
model_loading_uri
model_path must not be empty. Ensure that a path is provided when the model is created or loaded.
model_run
modelDomain
modelGraphName
modelMetaData
modelProducerName
modelProducerVersion
ModelProto corresponding to the model to be loaded has already been parsed. Invoke Load().
ModelProto corresponding to the model to be loaded has not been parsed yet. This API should be called in conjunction with a ctor that takes a model abstraction.
ModelProto does not have a graph.
ModelProto needs to be parsed to check for ORT config within it
momentum
Mongolian
Move it out of graph inputs if there is no need to override it, 
Msg:[%ws] 
mul_B_tensor_proto
mul_inputs.size() == 2
MulInteger
Multani
multi_class
MultiByteToWideChar
Multinomial
Multiple errors were found.
multiplication
Multiplicative spatial scale factor to translate ROI coordinates from their input scale to the scale used when pooling.
Multiplicative spatial scale factor to translate ROI coordinates from their input spatial scale to the scale used when pooling, i.e., spatial scale of the input feature map X relative to the input image. E.g.; default is 1.0f. 
MurmurHash3
Must be a scalar or 1D tensor or size 1.
Must have 1 or more inputs
Must have a single dimension
Must have a single dimension of 1
Must have a valid data type
Must have a valid input shape.
Must have valid 'axis' attribute
Must provide classlabels_strings or classlabels_int64s but not both.
Must provide imputed_values_float_ or imputed_values_int64_ but not both.
Must use Function based fusion when exporting compiled nodes to dll.
mutually equal shape is specified by the argument "axis", and if it is not set,
Myanmar
N ;O t
n >= 0
n >= 0 && static_cast<size_t>(n) < ort_value_info_.size()
n >= 0 && static_cast<size_t>(n) < plan_.allocation_plan.size()
N 9H 
N$9H 
N,;N,tm
N@;O@t
n_supports
n_targets
n_targets_or_classes > 0
N`;O`t
N|+Nx
N0X0m0
Nabataean
name:
naxes > 0
Nd;Nht
Nd;Odt
ND;ODt
N-dimensional matrix A
N-dimensional matrix B
nearest
NEAREST
nearest_mode
nearest_mode:[
Negative index values are not permitted. First entry in map has index value of 
Negative ngram_indexes values are not allowed
Negative values are not allowed in a shape specification
NegativeLogLikelihoodLoss
Nested parallelism not supported
network down
network reset
network unreachable
New shape
new_axis
new_axis must be either 0 or 1
New_Tai_Lue
n-gram counts out of bounds for 
ngram_counts
ngram_indexes
ngram_indexes must be non-empty with no negative values
NH;OHt
Nh;Oht
NH;OHt
NH;VD
nhwc_permutated_pads
NhwcMaxPool
NhwcTransformer
njob_ = 
NL;NPt
NL;OLt
Nl;Olt
NL;OLt
Nlh$9<
NLjt[
NnapiExecutionProvider
no argument for repetition operator
No attribute with name:'
No attribute with name: 
No attribute with this name is defined.
no buffer space
no child process
no error
No Graph instance was found for attribute 
No graph was found in the protobuf.
No kernel shape is set.
no link
no lock available
No matching 'start' entry.
no message
no message available
No Op registered for 
No opset import for domain '
no protocol option
No provider specified.
No ranges in char class
No requested allocator available
no space on device
no stream resources
no such device
no such device or address
no such file or directory
no such process
NO_MODEL
NO_SUCHFILE
Node 
Node (
Node [
Node id for each node. Ids may restart at zero for each tree, but it not required to.
Node id for each node. Node ids must restart at zero for each tree and increase sequentially.
node id that this weight is for.
Node index is out of range
Node is missing. Invalid ORT format model.
Node must only have one used output
Node placements
node.GetAttributeNameToMutableSubgraphMap().empty()
node.MutableOutputDefs().size() == subgraph.GetOutputs().size()
Node:
Node::LoadEdgesFromOrtFormat, edge is missing for 
Node::LoadFromOrtFormat, input_arg_counts is missing
node_arg
node_arg_name cannot be null
node_index < nodes_.size()
node_index_info and ort_value_idx_map are out of sync and cannot be used
node_index_info_
node_index_info_.GetMaxMLValueIdx() == ort_value_idx_map.MaxIdx()
node_offsets_index < node_offsets_size_
node->GetOutputEdgesCount() == 0
nodearg
NodeArg is missing. Invalid ORT format model.
NodeArg Name is missing. Invalid ORT format model.
NodeEdge is missing. Invalid ORT format model.
NodeProto (name: 
Nodes in a graph must be topologically sorted, however input '
nodes_.size() < static_cast<unsigned int>(std::numeric_limits<int>::max())
nodes_falsenodeids
nodes_falsenodeids.size() == nodes_featureids.size()
nodes_falsenodeids.size() == nodes_modes.size()
nodes_falsenodeids.size() == nodes_nodeids.size()
nodes_falsenodeids.size() == nodes_treeids.size()
nodes_falsenodeids.size() == nodes_truenodeids.size()
nodes_falsenodeids.size() == nodes_values.size()
nodes_featureids
nodes_hitrates
nodes_missing_value_tracks_true
nodes_modes
nodes_nodeids
nodes_treeids
nodes_truenodeids
nodes_values
Non concat axis dimensions must match: Axis 
Non per-tensor quantization is not supported now.
non_tensor_base != nullptr
Non-empty ngram_counts is required
Non-empty ngram_indexes is required
non-empty pool_int64s is required if pool_strings not provided
NonMaxSuppression
NonZero
Non-zero status code returned while running 
noop_with_empty_axes
normalize_variance
normalized
Normalizer
NormalizeVariance
not a directory
not a socket
not a stream
Not all dimensions to be reduced have been reduced in the candidate output. Candidate output dims: 
not connected
Not eliminating output 
Not enough elements in dilations. Expected: 
Not enough elements in kernel shape. Expected: 
Not enough elements in pads. Expected: 
Not enough elements in strides. Expected: 
not enough memory
not enough space: expected 
Not implemented
not implemented
not support normalize yet.
not supported
Not supported
Not supported with filtered graph.
NOT_IMPLEMENTED
NOT_SET
NOTSET
Np;Opt
Nt;Ott
nteltF=Authu
Null batch_indices_ptr
Null crop_size_ptr
Null entry in dimensions. Invalid ORT format model.
Null floats attribute. Invalid ORT format model.
Null graph attribute. Invalid ORT format model.
Null input ptr
Null input X ptr
Null ints attribute. Invalid ORT format model.
null literal
Null map type info. Invalid ORT format model.
Null rois_ptr
Null sequence type info. Invalid ORT format model.
NULL state in RunStateOnByte
Null string attribute. Invalid ORT format model.
Null string in strings attribute. Invalid ORT format model.
Null strings attribute. Invalid ORT format model.
Null tensor attribute. Invalid ORT format model.
Null tensor in tensors attribute. Invalid ORT format model.
Null tensor type info. Invalid ORT format model.
Null tensors attribute. Invalid ORT format model.
Null type info for 
Null value type info in fbs::MapType. Invalid ORT format model.
Null value type info in fbs::SequenceType. Invalid ORT format model.
nullptr != func_meta_def
nullptr != p.output_tensor
nullptr != tensor_type_base
nullptr != type_proto
nullptr == p_data
num_axes > 0
num_broadcasted_indices < num_of_ellipsis_dims_
num_categories_ > 0
num_classes is < 1
num_dims_with_pad - 1 != num_output_dims
num_dims_with_pad - 2 != num_output_dims
num_dims_with_pad != num_output_dims
num_entries == int_categories.size()
num_explicit_inputs == static_cast<size_t>(target_input_idx)
num_features == feature_count_
num_heads
num_inputs >= 1
num_keys == num_values
num_samples is < 1
num_scan_inputs
num_subgraph_outputs - 1 == num_outputs
num_subgraph_outputs == static_cast<size_t>(num_outputs)
num_variadic_inputs == num_subgraph_inputs
number
number literal
Number of attention heads
Number of dimensions for batch indices should be exactly 1
Number of dimensions for crop size should be exactly 1
Number of dimensions for rois should be exactly 
number of elements in this dimension), it represents `n`. For slicing to the
Number of elements of attribute 'scales' must be same as rank of input 'X'
Number of elements of input 'scales' must be same as rank of input 'X'
Number of elements of input 'sizes' must be same as rank of input 'X'
Number of entries in '
Number of entries in 'scan_input_axes' was 
Number of entries in 'scan_output_axes' was 
number of groups input channels and output channels are divided into.
number of groups input channels and output channels are divided into. default is 1.
Number of input tensors does not match the operands in the equation.
Number of inputs (
Number of items must compose whole 
Number of neurons in the hidden layer
Number of neurons in the hidden layer.
Number of sampling points in the interpolation grid used to compute the output value of each pooled output bin. If > 0, then exactly sampling_ratio x sampling_ratio grid points are used. If == 0, then an adaptive number of grid points are used (computed as ceil(roi_width / output_width), and likewise for height). Default is 0.
Number of scan input axes specified (
Number of scan output axes specified (
Number of subscripts in the input equation does not match number of input tensors
Number of times to sample.
Number of top elements to retrieve
Number of values should be at least 1.
number overflow parsing '
NumCapturesWalker::ShortVisit called
NupharExecutionProvider
Nushu
O ;O t
O(9O$t'+O$
O,;O,u
O8_^]
O8+O4
object
object key
object separator
of [N, 0] then [N, 0].
offset
Offset
offset % span_size_ == 0
offset + size <= size_t(span.size())
offset < 0
offset >= 0 && static_cast<size_t>(offset) < node_values_size_
Offsets
offsets buffer is not equal to tensor size
Ogham
Oh$9<
Ol_Chiki
Old_Hungarian
Old_Italic
Old_North_Arabian
Old_Permic
Old_Persian
Old_Sogdian
Old_South_Arabian
Old_Turkic
OLEAUT32.dll
One (or two if bidirectional) activation function for input gate. The activation function must be one of the activation functions specified above. Optional: Default `Tanh` if not specified.
One and only one of the attributes 'value', 'value_*' or 'sparse_value' must be specified for a Constant node.
One and only one of the 'cats_*' attributes must be defined
One falsenode is pointing either to itself, either to another tree.
One float, indicates the value to be filled, default is 0
One float, indicates the value to be filled.
One of 'MAX,' 'L1,' 'L2'
One of the attributes 'value' or 'sparse_value' must be specified for a Constant node.
One sided attention windows length W, or half of total window length
one_class
OneHot
OneHot node must have three inputs.
OneHotEncoder
onesided
Only 1 batch dimension is allowed for MatMul
Only bool
Only CPU devices are supported for now.
Only one instance of LoggingManager created with InstanceType::Default can exist at any point in time.
Only one node should produce an output. Existing entry for 
Only one of keys_*'s can be set in label encoder.
Only one of scales or sizes must be provided as input.
Only one of the attributes 'value' or 'sparse_value' must be specified for a Constant node.
Only one of values_*'s can be set in label encoder.
Only one thread was configured for parallel execution. Hence will use sequential execution.
Only ONNX MLDataType can be registered
Only supports `int32_t` or `int64_t` inputs for split
Only supports `int32_t` or `int64_t` inputs for starts/ends/axes/steps
Only tensors or sequence of tensors are suppported
ONNX model does not support multiple opset versions for a domain. Model imports opset version 
ONNX models don't support multiple opset version imports for a domain. Function 
ONNX Runtime
ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 
ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain 'ai.onnx'. Please upgrade your model to opset 7 or higher. For now, this opset 
ONNX Schema 
onnx.AttributeProto
onnx.FunctionProto
onnx.GraphProto
onnx.ModelProto
onnx.NodeProto
onnx.OperatorSetIdProto
onnx.SparseTensorProto
onnx.StringStringEntryProto
onnx.TensorAnnotation
onnx.TensorProto
onnx.TensorProto.Segment
onnx.TensorShapeProto
onnx.TensorShapeProto.Dimension
onnx.TrainingInfoProto
onnx.TypeProto
onnx.TypeProto.Map
onnx.TypeProto.Opaque
onnx.TypeProto.Sequence
onnx.TypeProto.SparseTensor
onnx.TypeProto.Tensor
onnx.ValueInfoProto
ONNX_NAMESPACE::TensorProto::DataType_IsValid(dtype_) && dtype_ != ONNX_NAMESPACE::TensorProto::UNDEFINED
ONNX_NAMESPACE::TensorProto::DataType_IsValid(output_dtype_) && output_dtype_ != ONNX_NAMESPACE::TensorProto::UNDEFINED
ONNX_NAMESPACE::TensorProto::DataType_IsValid(t_proto.data_type())
onnxruntime
onnxruntime.dll
onnxruntime.pdb
onnxruntime::`anonymous-namespace'::Cast::Cast
onnxruntime::`anonymous-namespace'::CastToString
onnxruntime::`anonymous-namespace'::ConstantOfShape::Compute
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<__int64>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<double>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<float>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<int>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<struct onnxruntime::BFloat16>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<struct onnxruntime::MLFloat16>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<unsigned __int64>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<unsigned int>::operator ()
onnxruntime::`anonymous-namespace'::GetClipConstantMinMax::<lambda_936f40e242e3130b40c035d3ac1b5c8b>::operator ()
onnxruntime::`anonymous-namespace'::GetCurrentTimeString
onnxruntime::`anonymous-namespace'::GetInputNodeMerges
onnxruntime::`anonymous-namespace'::GetOutputNodeMerges
onnxruntime::`anonymous-namespace'::GetRatioOrDefault
onnxruntime::`anonymous-namespace'::GetScalarConstantInitializer
onnxruntime::`anonymous-namespace'::GetScaleFromNode
onnxruntime::`anonymous-namespace'::ParsePathRoot
onnxruntime::`anonymous-namespace'::TraverseFormalParametersWithTypeProto
onnxruntime::`anonymous-namespace'::WindowsEnv::DeleteFolder
onnxruntime::`anonymous-namespace'::WindowsEnv::FormatLibraryFileName
onnxruntime::`anonymous-namespace'::WindowsEnv::GetCanonicalPath
onnxruntime::`anonymous-namespace'::WindowsEnv::GetNumCpuCores
onnxruntime::`anonymous-namespace'::WindowsEnv::ReadFileIntoBuffer
onnxruntime::AllocateSparseTensor
onnxruntime::AllocatorManager::InsertAllocator
onnxruntime::AllocPlanPerValue::ProgramCounter::AddEnd
onnxruntime::AllocPlanPerValue::ProgramCounter::AddStart
onnxruntime::AttentionFusion::ApplyImpl
onnxruntime::AttentionFusion::FuseSubGraph
onnxruntime::AttentionFusionHelper::CheckDistilBertReshapeShape
onnxruntime::AttentionFusionHelper::CheckNodesInPathK
onnxruntime::AttentionFusionHelper::CheckNodesInPathQ
onnxruntime::AttentionFusionHelper::CheckNodesInPathV
onnxruntime::AttentionFusionHelper::CheckSliceParameters
onnxruntime::AttentionFusionHelper::FuseGptAttention
onnxruntime::AttentionFusionHelper::MatchGemmSubgraph
onnxruntime::AttentionFusionHelper::MatchInputMaskSubgraph
onnxruntime::AttentionFusionHelper::MatchPastSubgraph
onnxruntime::AttentionFusionHelper::MatchUnidirMaskSubgraph
onnxruntime::AttentionFusionHelper::ValidateGemmInitializer
onnxruntime::AttentionFusionHelper::ValidateUnidirMask
onnxruntime::BatchNorm<double>::BatchNorm
onnxruntime::BatchNorm<double>::Compute
onnxruntime::BatchNorm<float>::BatchNorm
onnxruntime::BatchNorm<float>::Compute
onnxruntime::BFCArena::AllocateRawInternal
onnxruntime::BFCArena::AllocationRegion::AllocationRegion
onnxruntime::BFCArena::AllocationRegion::IndexFor
onnxruntime::BFCArena::BFCArena
onnxruntime::BFCArena::ChunkFromHandle
onnxruntime::BFCArena::DeallocateRawInternal
onnxruntime::BFCArena::Extend
onnxruntime::BFCArena::Extend::<lambda_778708ec45b25e707f8c9627b4dc2fb5>::operator ()
onnxruntime::BFCArena::FindChunkPtr
onnxruntime::BFCArena::FreeAndMaybeCoalesce
onnxruntime::BFCArena::InsertFreeChunkIntoBin
onnxruntime::BFCArena::Merge
onnxruntime::BFCArena::RegionManager::RegionFor
onnxruntime::BFCArena::RemoveFreeChunkFromBin
onnxruntime::BFCArena::RemoveFreeChunkIterFromBin
onnxruntime::BFCArena::Reserve
onnxruntime::BFCArena::SplitChunk
onnxruntime::BiasGeluFusion::ApplyImpl
onnxruntime::BiasSoftmaxFusion::ApplyImpl
onnxruntime::BitShift<unsigned __int64>::BitShift
onnxruntime::BitShift<unsigned __int64>::Compute::<lambda_81a36806fa76bdbba7f1d45138e39648>::operator ()
onnxruntime::BitShift<unsigned char>::BitShift
onnxruntime::BitShift<unsigned char>::Compute::<lambda_2e358302575606c2f5458b318947da41>::operator ()
onnxruntime::BitShift<unsigned int>::BitShift
onnxruntime::BitShift<unsigned int>::Compute::<lambda_4afa3a30dd6ede71f6fcc097c88b26ba>::operator ()
onnxruntime::Broadcaster::Broadcaster
onnxruntime::BroadcastIterator::Append
onnxruntime::BroadcastIterator::Init
onnxruntime::BroadcastLooper
onnxruntime::CheckInput
onnxruntime::Clip::ComputeImpl<__int64>::operator ()
onnxruntime::Clip::ComputeImpl<double>::operator ()
onnxruntime::Clip::ComputeImpl<float>::operator ()
onnxruntime::Clip::ComputeImpl<signed char>::operator ()
onnxruntime::Clip::ComputeImpl<unsigned __int64>::operator ()
onnxruntime::Clip::ComputeImpl<unsigned char>::operator ()
onnxruntime::clip_internal::Clip_6Base<float>::Clip_6Base
onnxruntime::common::Status::Status
onnxruntime::CommonReduce
onnxruntime::CommonSubexpressionElimination::ApplyImpl
onnxruntime::Compress::Compute
onnxruntime::ComputePadAndOutputShape
onnxruntime::ConcatBase::ConcatBase
onnxruntime::ConcatBase::PrepareForCompute
onnxruntime::ConcatFromSequence::Compute
onnxruntime::concurrency::ThreadPool::ParallelFor
onnxruntime::concurrency::ThreadPool::ParallelSection::ParallelSection
onnxruntime::ConstantFolding::ApplyImpl
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<struct onnxruntime::MLFloat16,float,double,signed char,short,int,__int64,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::ConstantOfShapeBase
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<struct onnxruntime::MLFloat16,float,double,signed char,short,int,__int64,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::PrepareCompute
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<struct onnxruntime::MLFloat16,float,double,signed char,short,int,__int64,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::SetValueFromTensorProto
onnxruntime::ConstPointerContainer<class std::vector<class onnxruntime::NodeArg *,class std::allocator<class onnxruntime::NodeArg *> > >::at
onnxruntime::contrib::`anonymous-namespace'::QLinearImpl
onnxruntime::contrib::Affine<float>::Affine
onnxruntime::contrib::Attention<float>::Compute
onnxruntime::contrib::AttentionBase::AttentionBase
onnxruntime::contrib::AttentionBase::GetPresent
onnxruntime::contrib::AttentionCPUBase::ApplyAttention
onnxruntime::contrib::BahdanauAttention<float>::BahdanauAttention
onnxruntime::contrib::BahdanauAttention<float>::PrepareMemory
onnxruntime::contrib::BiasGelu<float,0>::Compute
onnxruntime::contrib::BiasGelu<float,1>::Compute
onnxruntime::contrib::CDist<double>::CDist
onnxruntime::contrib::CDist<float>::CDist
onnxruntime::contrib::Crop<float>::Compute
onnxruntime::contrib::CropAndResize<float>::CropAndResize
onnxruntime::contrib::DeepCpuAttnLstmOp::Compute
onnxruntime::contrib::DeepCpuAttnLstmOp::ComputeImpl
onnxruntime::contrib::DeepCpuAttnLstmOp::DeepCpuAttnLstmOp
onnxruntime::contrib::DeepCpuAttnLstmOp::ValidateInputs
onnxruntime::contrib::DynamicQuantizeMatMul::Compute
onnxruntime::contrib::EmbedLayerNorm<float>::Compute
onnxruntime::contrib::EmbedLayerNorm<float>::EmbedLayerNorm
onnxruntime::contrib::ExpandDims::Compute
onnxruntime::contrib::FusedConvFloat::FusedConvFloat
onnxruntime::contrib::FusedGemm<float>::FusedGemm
onnxruntime::contrib::ImageScaler<float>::ImageScaler
onnxruntime::contrib::LayerNorm<double,0>::Compute
onnxruntime::contrib::LayerNorm<double,0>::LayerNorm
onnxruntime::contrib::LayerNorm<double,1>::Compute
onnxruntime::contrib::LayerNorm<double,1>::LayerNorm
onnxruntime::contrib::LayerNorm<float,0>::Compute
onnxruntime::contrib::LayerNorm<float,0>::LayerNorm
onnxruntime::contrib::LayerNorm<float,1>::Compute
onnxruntime::contrib::LayerNorm<float,1>::LayerNorm
onnxruntime::contrib::MatMulInteger16<short,short,int>::Compute
onnxruntime::contrib::MatMulIntegerToFloat::Compute
onnxruntime::contrib::MatMulIntegerToFloatBase::ComputeCommon
onnxruntime::contrib::MaxpoolWithMask::Compute
onnxruntime::contrib::MurmurHash3::Compute
onnxruntime::contrib::NhwcMaxPool::Compute
onnxruntime::contrib::QAttention<float>::Compute
onnxruntime::contrib::QlinearBuildLookupTable
onnxruntime::contrib::QLinearGlobalAveragePool::Compute
onnxruntime::contrib::RegisterCpuContribKernels
onnxruntime::contrib::RegisterQuantizationKernels
onnxruntime::contrib::Scale<float>::Scale
onnxruntime::contrib::SkipLayerNorm<double>::SkipLayerNorm
onnxruntime::contrib::SkipLayerNorm<float>::SkipLayerNorm
onnxruntime::contrib::Tokenizer::Tokenizer
onnxruntime::contrib::Trilu::Compute
onnxruntime::contrib::Trilu::Trilu
onnxruntime::contrib::WordConvEmbedding::Compute
onnxruntime::Conv<float>::Compute
onnxruntime::ConvActivationFusion::ApplyImpl
onnxruntime::ConvAddFusion::Apply
onnxruntime::ConvAttributes::ConvAttributes
onnxruntime::ConvAttributes::InferOutputShape
onnxruntime::ConvBNFusion::Apply
onnxruntime::ConvertMaskToInt32
onnxruntime::ConvInteger::Compute
onnxruntime::ConvMulFusion::Apply
onnxruntime::ConvTranspose<float>::DoConvTranspose
onnxruntime::ConvTransposeAttributes::ComputePadsAndOutputShape
onnxruntime::ConvTransposeAttributes::ComputeTransposePadAndOutputShape
onnxruntime::ConvTransposeAttributes::PrepareForCompute
onnxruntime::core_impl::<lambda_0aec3d6cfc480180f5600d203ab8e460>::operator ()
onnxruntime::core_impl::<lambda_7fc90bb3cfd409083e1364aa2138cba1>::operator ()
onnxruntime::core_impl::<lambda_86d0400a70b4f351bdb6bcc632885fd0>::operator ()
onnxruntime::core_impl::<lambda_e1fda93c4ec01e4b409b356848ca7866>::operator ()
onnxruntime::CPUDataTransfer::CopyTensor
onnxruntime::CPUExecutionProvider::GetKernelRegistry
onnxruntime::CreateAllocator
onnxruntime::CreateCopyAndAppendCpuTensor
onnxruntime::CreateCustomRegistry
onnxruntime::CreateOpsetImportsForFunction
onnxruntime::CreateSchema
onnxruntime::CumSum<__int64>::Compute
onnxruntime::CumSum<double>::Compute
onnxruntime::CumSum<float>::Compute
onnxruntime::CumSum<int>::Compute
onnxruntime::CustomOpKernel::CustomOpKernel
onnxruntime::data_types_internal::DataTypeRegistry::RegisterDataType
onnxruntime::data_types_internal::IsCompatible
onnxruntime::data_types_internal::SetMapTypes<__int64,__int64>::Set
onnxruntime::data_types_internal::SetMapTypes<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::Set
onnxruntime::data_types_internal::SetMapTypes<__int64,double>::Set
onnxruntime::data_types_internal::SetMapTypes<__int64,float>::Set
onnxruntime::data_types_internal::SetMapTypes<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::Set
onnxruntime::data_types_internal::SetMapTypes<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::Set
onnxruntime::data_types_internal::SetMapTypes<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,double>::Set
onnxruntime::data_types_internal::SetMapTypes<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float>::Set
onnxruntime::data_types_internal::SetSequenceType<__int64>::Set
onnxruntime::data_types_internal::SetSequenceType<bool>::Set
onnxruntime::data_types_internal::SetSequenceType<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::Set
onnxruntime::data_types_internal::SetSequenceType<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > > >::Set
onnxruntime::data_types_internal::SetSequenceType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > > >::Set
onnxruntime::data_types_internal::SetSequenceType<double>::Set
onnxruntime::data_types_internal::SetSequenceType<float>::Set
onnxruntime::data_types_internal::SetSequenceType<int>::Set
onnxruntime::data_types_internal::SetSequenceType<short>::Set
onnxruntime::data_types_internal::SetSequenceType<signed char>::Set
onnxruntime::data_types_internal::SetSequenceType<struct onnxruntime::BFloat16>::Set
onnxruntime::data_types_internal::SetSequenceType<struct onnxruntime::MLFloat16>::Set
onnxruntime::data_types_internal::SetSequenceType<unsigned __int64>::Set
onnxruntime::data_types_internal::SetSequenceType<unsigned char>::Set
onnxruntime::data_types_internal::SetSequenceType<unsigned int>::Set
onnxruntime::data_types_internal::SetSequenceType<unsigned short>::Set
onnxruntime::DataTransferManager::CopyTensors
onnxruntime::DataTypeImpl::GetType<T>() == type_
onnxruntime::DeepCpuGruOp::Compute
onnxruntime::DeepCpuGruOp::ComputeImpl
onnxruntime::DeepCpuGruOp::DeepCpuGruOp
onnxruntime::DeepCpuLstmOp::Compute
onnxruntime::DepthToSpace<float>::Compute
onnxruntime::DepthToSpace<float>::DepthToSpace
onnxruntime::DequantizeLinear<int>::Compute
onnxruntime::Det<float>::Compute
onnxruntime::DoTransposeEltWise
onnxruntime::DoTransposeImpl
onnxruntime::Dropout<double,double>::Compute
onnxruntime::Dropout<double,float>::Compute
onnxruntime::Dropout<float,double>::Compute
onnxruntime::Dropout<float,float>::Compute
onnxruntime::DynamicQuantizeLinear<unsigned char>::Compute
onnxruntime::Einsum::DeviceCompute
onnxruntime::Einsum::Einsum
onnxruntime::EinsumComputePreprocessor::PostProcessBroadcastedDims
onnxruntime::EinsumComputePreprocessor::Run
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::DataCopy
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::Diagonal
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::DiagonalInnermostDims
onnxruntime::EinsumOp::IsTransposeRequired
onnxruntime::EinsumOp::MatMul
onnxruntime::EinsumOp::Transpose
onnxruntime::EinsumTypedComputeProcessor<__int64>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<__int64>::PairwiseOperandProcess
onnxruntime::EinsumTypedComputeProcessor<double>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<double>::PairwiseOperandProcess
onnxruntime::EinsumTypedComputeProcessor<float>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<float>::PairwiseOperandProcess
onnxruntime::EinsumTypedComputeProcessor<int>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<int>::PairwiseOperandProcess
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<__int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<__int64> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<int> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<short> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<short> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<signed char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<signed char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned __int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned __int64> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned int> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned short> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned short> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Ceil<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Ceil<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Elu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Elu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Floor<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Floor<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::HardSigmoid<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::HardSigmoid<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::LeakyRelu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::LeakyRelu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<__int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<__int64> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<int> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<signed char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<signed char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ParametricSoftplus<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ParametricSoftplus<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ScaledTanh<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ScaledTanh<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Selu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Selu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sigmoid<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sigmoid<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Softplus<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Softsign<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Tanh<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Tanh<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ThresholdedRelu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ThresholdedRelu<float> >::ElementWiseKernel
onnxruntime::EmbedLayerNormFusion::ApplyImpl
onnxruntime::ExecutionFrame::{ctor}::<lambda_a5265a1b5936252e894f3e3980cc61b4>::operator ()
onnxruntime::ExecutionFrame::AllocateAsPerAllocationPlan
onnxruntime::ExecutionFrame::AllocateMLValueTensorPreAllocateBuffer
onnxruntime::ExecutionFrame::AllocateMLValueTensorSelfOwnBufferHelper
onnxruntime::ExecutionFrame::ExecutionFrame
onnxruntime::ExecutionFrame::GetAllocationPlan
onnxruntime::ExecutionFrame::ReleaseMLValueImpl
onnxruntime::ExecutionFrame::TraceAllocate
onnxruntime::ExecutionFrame::TraceFree
onnxruntime::ExecutionProviders::Add
onnxruntime::ExLibLoader::{dtor}::<lambda_c307c48f1ec95d0e0c61485494323d73>::operator ()
onnxruntime::ExLibLoader::~ExLibLoader
onnxruntime::ExLibLoader::LoadExternalLib
onnxruntime::Expand_8<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::Compute::<lambda_74275d975e317ce49df88cad4c2e9024>::operator ()
onnxruntime::ExpandBroadcastLooper
onnxruntime::experimental::utils::LoadAttributeOrtFormat
onnxruntime::experimental::utils::LoadInitializerOrtFormat
onnxruntime::experimental::utils::LoadMapTypeOrtFormat
onnxruntime::experimental::utils::LoadOpsetImportOrtFormat
onnxruntime::experimental::utils::LoadSequenceTypeOrtFormat
onnxruntime::experimental::utils::LoadSparseInitializerOrtFormat
onnxruntime::experimental::utils::LoadTensorDimensionOrtFormat
onnxruntime::experimental::utils::LoadTensorShapeOrtFormat
onnxruntime::experimental::utils::LoadTensorTypeAndShapeOrtFormat
onnxruntime::experimental::utils::LoadTypeInfoOrtFormat
onnxruntime::experimental::utils::LoadValueInfoOrtFormat
onnxruntime::experimental::utils::SaveAttributeOrtFormat
onnxruntime::experimental::utils::SaveInitializerOrtFormat
onnxruntime::experimental::utils::SaveMapTypeOrtFormat
onnxruntime::experimental::utils::SaveSequenceTypeOrtFormat
onnxruntime::experimental::utils::SaveSparseInitializerOrtFormat
onnxruntime::experimental::utils::SaveTensorTypeAndShapeOrtFormat
onnxruntime::experimental::utils::SaveTypeInfoOrtFormat
onnxruntime::experimental::utils::SaveValueInfoOrtFormat
onnxruntime::EyeLike::Compute
onnxruntime::FastGeluFusion::ApplyImpl
onnxruntime::FeedsFetchesInfo::FeedsFetchesInfo
onnxruntime::FeedsFetchesInfo::MapNamesToMLValueIdxs
onnxruntime::FeedsFetchesManager::SetDeviceCopyChecks
onnxruntime::FinalizeSessionOptions
onnxruntime::Flatten::Compute
onnxruntime::Flatten::Flatten
onnxruntime::FreeDimensionOverrideTransformer::ApplyImpl
onnxruntime::FreeDimensionOverrideTransformer::FreeDimensionOverrideTransformer
onnxruntime::FuncManager::GetFuncs
onnxruntime::FunctionImpl::FunctionImpl
onnxruntime::FunctionKernel::FunctionKernel
onnxruntime::functors::HardSigmoid<float>::Init
onnxruntime::functors::ParametricSoftplus<float>::Init
onnxruntime::functors::ScaledTanh<float>::Init
onnxruntime::functors::Selu<float>::Init
onnxruntime::FuseReluClip::Apply
onnxruntime::FuseSubGraph
onnxruntime::FuseSubGraphDistilBert
onnxruntime::FuseSubGraphQK
onnxruntime::FuseSubGraphQKDistilBert
onnxruntime::FuseSubGraphQKImpl
onnxruntime::Gather::Compute
onnxruntime::GatherBase::GatherBase
onnxruntime::GatherElements::GatherElements
onnxruntime::GatherND::Compute
onnxruntime::GeluApproximation::ApplyImpl
onnxruntime::GeluFusion::ApplyImpl
onnxruntime::Gemm<double>::Gemm
onnxruntime::Gemm<float>::Gemm
onnxruntime::GemmActivationFusion::ApplyImpl
onnxruntime::GemmBroadcastBias
onnxruntime::GemmHelper::GemmHelper
onnxruntime::GetKernelCreateInfo
onnxruntime::GetScalarSplitInput
onnxruntime::GetSeqIdx
onnxruntime::GetSplitSizesInput
onnxruntime::GetSubGraphSessionStatesOrtFormat
onnxruntime::Graph::AddEdge
onnxruntime::Graph::AddInitializedTensor
onnxruntime::Graph::AllocateNode
onnxruntime::Graph::BuildConnections
onnxruntime::Graph::CleanUnusedInitializers
onnxruntime::Graph::CreateFusedSubGraphNode
onnxruntime::Graph::FinalizeFuseSubGraph
onnxruntime::Graph::ForThisAndAllSubgraphs
onnxruntime::Graph::Graph
onnxruntime::Graph::InferAndVerifySubgraphTypes
onnxruntime::Graph::InferAndVerifyTypeMatch
onnxruntime::Graph::InitializeStateFromModelFileGraphProto
onnxruntime::Graph::InitInputsInitializersOutputs
onnxruntime::Graph::InlineFunction
onnxruntime::Graph::KahnsTopologicalSort
onnxruntime::Graph::LoadFromOrtFormat
onnxruntime::Graph::LoadFromOrtFormat::<lambda_b83ad9e7313f9e7d01d4ef449180f4a4>::operator ()
onnxruntime::Graph::NodeAtIndexImpl
onnxruntime::Graph::PerformTypeAndShapeInferencing
onnxruntime::Graph::RemoveEdge
onnxruntime::Graph::RemoveInitializedTensor
onnxruntime::Graph::RemoveNode
onnxruntime::Graph::Resolve
onnxruntime::Graph::SaveToOrtFormat
onnxruntime::Graph::SetInputs
onnxruntime::Graph::SetOuterScopeNodeArgs
onnxruntime::Graph::ToGraphProto
onnxruntime::Graph::ToGraphProtoInternal
onnxruntime::Graph::UpdateShapeInference
onnxruntime::Graph::VerifyNodeAndOpMatch
onnxruntime::graph_utils::AddInitializer
onnxruntime::graph_utils::AddNodeInput
onnxruntime::graph_utils::CanUpdateImplicitInputNameInSubgraphs
onnxruntime::graph_utils::FindPath
onnxruntime::graph_utils::GetNodeInputIndexFromInputName
onnxruntime::graph_utils::GetNodeInputName
onnxruntime::graph_utils::GetNodeOutputName
onnxruntime::graph_utils::RemoveNode
onnxruntime::graph_utils::RemoveNodeWithSingleNodeInSingleUsedOutput
onnxruntime::graph_utils::ReplaceNodeInput
onnxruntime::graph_utils::UpdateImplicitInputNameInSubgraph
onnxruntime::GraphPartitioner::Partition
onnxruntime::GraphPartitioner::PartitionOnnxFormatModel
onnxruntime::GraphPartitioner::PartitionOrtFormatModel
onnxruntime::GraphTransformer::Apply
onnxruntime::GraphTransformer::Recurse
onnxruntime::GraphTransformerManager::ApplyTransformers
onnxruntime::GraphViewer::GetNodesInTopologicalOrder
onnxruntime::GraphViewer::GetRootNodes
onnxruntime::GraphViewer::GraphViewer
onnxruntime::HandleNegativeAxis
onnxruntime::Hardmax<float>::Compute
onnxruntime::IAllocator::CalcMemSizeForArrayWithAlignment::<lambda_9d11340812e7a423c90c42486d2ca1f6>::operator ()
onnxruntime::IDataTransfer::CopyTensors
onnxruntime::IdentityOp<0>::Compute
onnxruntime::IdentityOp<1>::Compute
onnxruntime::IExecutionFrame::GetMLValue
onnxruntime::IExecutionFrame::GetOrCreateNodeOutputMLValue
onnxruntime::IExecutionFrame::IExecutionFrame
onnxruntime::IExecutionFrame::Init
onnxruntime::IExecutionProvider::GenerateMetaDefId
onnxruntime::IExecutionProvider::InsertAllocator
onnxruntime::IExecutionProvider::TryInsertAllocator
onnxruntime::If::Compute
onnxruntime::If::If
onnxruntime::If::Info::Info
onnxruntime::If::SetupSubgraphExecutionInfo
onnxruntime::IfImpl::Execute
onnxruntime::IfImpl::Initialize
onnxruntime::IncrementIndexAndComputeOffsetSetup
onnxruntime::inference_session_utils::JsonConfigParser::ParseOrtConfigJsonInModelProto
onnxruntime::inference_session_utils::JsonConfigParser::ParseOrtConfigJsonInModelProto::<lambda_2239dc5493f1d6b7cff1e7f61a9bac4f>::operator ()
onnxruntime::inference_session_utils::JsonConfigParser::ParseSessionOptionsFromModelProto
onnxruntime::InferenceSession::{dtor}::<lambda_c4084d08bcc4f395bcba2e0aace14aa5>::operator ()
onnxruntime::InferenceSession::~InferenceSession
onnxruntime::InferenceSession::AddCustomOpDomains
onnxruntime::InferenceSession::AddPredefinedTransformers
onnxruntime::InferenceSession::ConstructorCommon
onnxruntime::InferenceSession::ConstructorCommon::<lambda_82945435a9d262261a78bf069eb7ee53>::operator ()
onnxruntime::InferenceSession::CreateLoggerForRun
onnxruntime::InferenceSession::EndProfiling
onnxruntime::InferenceSession::GetModelInputs
onnxruntime::InferenceSession::GetModelMetadata
onnxruntime::InferenceSession::GetModelOutputs
onnxruntime::InferenceSession::GetOverridableInitializers
onnxruntime::InferenceSession::GetSessionState
onnxruntime::InferenceSession::InferenceSession
onnxruntime::InferenceSession::Initialize
onnxruntime::InferenceSession::Initialize::<lambda_0ed0acc2fb71c29aa1d43489f2750509>::operator ()
onnxruntime::InferenceSession::Initialize::<lambda_d415c6adffc2525a7a99eade8e34a274>::operator ()
onnxruntime::InferenceSession::InitLogger
onnxruntime::InferenceSession::Load
onnxruntime::InferenceSession::LoadOrtModel
onnxruntime::InferenceSession::LoadOrtModel::<lambda_8032534fac3fd5dda5243b4509fad456>::operator ()
onnxruntime::InferenceSession::NewIOBinding
onnxruntime::InferenceSession::PartitionOrtFormatModel
onnxruntime::InferenceSession::RegisterExecutionProvider
onnxruntime::InferenceSession::RegisterGraphTransformer
onnxruntime::InferenceSession::Run
onnxruntime::InferenceSession::SaveToOrtFormat
onnxruntime::InferenceSession::TransformGraph
onnxruntime::InferenceSession::ValidateInputs
onnxruntime::Initializer::Initializer
onnxruntime::Initializer::ReadExternalRawData
onnxruntime::Initializer::ToProto
onnxruntime::InlineNodes
onnxruntime::InputBroadcaster::AdvanceBy
onnxruntime::InsertCastTransformer::ApplyImpl
onnxruntime::InstanceNorm<float>::Compute
onnxruntime::InstanceNorm<float>::InstanceNorm
onnxruntime::IOBinding::BindInput
onnxruntime::IsInf::IsInf
onnxruntime::KernelRegistry::TryCreateKernel
onnxruntime::LayerNormFusion::ApplyImpl
onnxruntime::LoadOrtModelBytes
onnxruntime::logging::LoggingManager::CreateDefaultLogger
onnxruntime::logging::LoggingManager::DefaultLogger
onnxruntime::logging::LoggingManager::LoggingManager
onnxruntime::Loop::Compute
onnxruntime::Loop::Info::Info
onnxruntime::Loop::Loop
onnxruntime::Loop::SetupSubgraphExecutionInfo
onnxruntime::LoopDir
onnxruntime::LoopImpl::ConcatenateLoopOutput
onnxruntime::LoopImpl::Execute
onnxruntime::LoopImpl::Execute::<lambda_1a5e1a4ed6c5357aaa194050a27576b8>::operator ()
onnxruntime::LoopImpl::SaveOutputsAndUpdateFeeds
onnxruntime::LpNorm<double>::LpNorm
onnxruntime::LpNorm<float>::LpNorm
onnxruntime::LRN<float>::Compute
onnxruntime::LRN<float>::LRN
onnxruntime::LSTMBase::ComputeImpl
onnxruntime::LSTMBase::LSTMBase
onnxruntime::MatchInputToConcatSubgraph
onnxruntime::MatchPositionEmbeddingSubgraphsFromGather
onnxruntime::math::Gemm
onnxruntime::math::NextPosition
onnxruntime::MatMul<__int64>::Compute
onnxruntime::MatMul<double>::Compute
onnxruntime::MatMul<float>::Compute
onnxruntime::MatMul<int>::Compute
onnxruntime::MatMulAddFusion::ApplyImpl
onnxruntime::MatMulComputeHelper::Compute
onnxruntime::MatMulInteger::Compute
onnxruntime::MatMulIntegerToFloatFusion::ApplyImpl
onnxruntime::MatMulScaleFusion::ApplyImpl
onnxruntime::Max_6<float>::Compute
onnxruntime::MaxPoolV8::ComputeImpl
onnxruntime::MaxUnpool::Compute
onnxruntime::MaxUnpool::MaxUnpool
onnxruntime::Mean_6<float>::Compute
onnxruntime::MeanVarianceNormalization_0<float>::MeanVarianceNormalization_0
onnxruntime::MemcpyTransformer::ApplyImpl
onnxruntime::MemPatternPlanner::TraceAllocation
onnxruntime::MergeShapeInfo
onnxruntime::Min_6<float>::Compute
onnxruntime::ml::batched_update_scores_inplace
onnxruntime::ml::CastInputToFloat
onnxruntime::ml::CastMap::CastMap
onnxruntime::ml::CastMap::ComputeImpl
onnxruntime::ml::CategoryMapper::CategoryMapper
onnxruntime::ml::detail::TreeAggregator<double,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregator<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorAverage<double,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorAverage<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<__int64,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<__int64,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<double,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<double,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<float,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<int,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<int,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorMax<double,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorMax<float,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorMin<double,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorMin<float,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<__int64,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<__int64,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorSum<double,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<double,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorSum<float,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<float,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorSum<int,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<int,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeEnsembleCommon<__int64,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommon<double,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommon<double,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommon<float,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommon<float,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommon<int,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<__int64,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<double,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<float,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<int,float>::compute
onnxruntime::ml::DictVectorizerOp<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<__int64,double>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<__int64,float>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,double>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float>::DictVectorizerOp
onnxruntime::ml::FeatureVectorizer::Compute
onnxruntime::ml::FeatureVectorizer::FeatureVectorizer
onnxruntime::ml::ImputerOp::Compute
onnxruntime::ml::ImputerOp::ImputerOp
onnxruntime::ml::LabelEncoder::LabelEncoder
onnxruntime::ml::LabelEncoder_2<__int64,__int64>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<__int64,float>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<float,__int64>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<float,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::LabelEncoder_2
onnxruntime::ml::LinearClassifier::ComputeImpl
onnxruntime::ml::LinearClassifier::LinearClassifier
onnxruntime::ml::LinearRegressor::LinearRegressor
onnxruntime::ml::MakeCast
onnxruntime::ml::MakeNormalize
onnxruntime::ml::MakePack
onnxruntime::ml::Normalizer::Normalizer
onnxruntime::ml::OneHotEncoderOp<__int64>::OneHotEncoderOp
onnxruntime::ml::OneHotEncoderOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::OneHotEncoderOp
onnxruntime::ml::OneHotEncoderOp<double>::OneHotEncoderOp
onnxruntime::ml::OneHotEncoderOp<float>::OneHotEncoderOp
onnxruntime::ml::RegisterOnnxMLOperatorKernels
onnxruntime::ml::ScalerOp<__int64>::ScalerOp
onnxruntime::ml::ScalerOp<double>::ScalerOp
onnxruntime::ml::ScalerOp<float>::ScalerOp
onnxruntime::ml::ScalerOp<int>::ScalerOp
onnxruntime::ml::SVMClassifier::Compute
onnxruntime::ml::SVMClassifier::SVMClassifier
onnxruntime::ml::SVMCommon::SVMCommon
onnxruntime::ml::SVMRegressor<float>::Compute
onnxruntime::ml::SVMRegressor<float>::SVMRegressor
onnxruntime::ml::ZipMapOp::ZipMapOp
onnxruntime::Mod::Compute
onnxruntime::Mod::Mod
onnxruntime::Model::Load
onnxruntime::Model::LoadFromOrtFormat
onnxruntime::Model::Model
onnxruntime::Model::Save
onnxruntime::Model::SaveToOrtFormat
onnxruntime::model_load_utils::IsAllowReleasedONNXOpsetsOnlySet
onnxruntime::model_load_utils::ValidateOpsetForDomain
onnxruntime::Multinomial::Multinomial
onnxruntime::MultinomialCompute
onnxruntime::ngram_details::PopulateGrams
onnxruntime::NhwcTransformer::ApplyImpl
onnxruntime::Node::ForEachWithIndex
onnxruntime::Node::LoadEdgesFromOrtFormat
onnxruntime::Node::LoadEdgesFromOrtFormat::<lambda_d0449cbf0086f9694a813cc2f46b6e2a>::operator ()
onnxruntime::Node::LoadFromOrtFormat
onnxruntime::Node::LoadFromOrtFormat::<lambda_56c170021cf476babf8838dcd7e48b57>::operator ()
onnxruntime::Node::SaveToOrtFormat
onnxruntime::NodeArg::UpdateTypeAndShape
onnxruntime::NodeIndexInfo::GetMLValueIndex
onnxruntime::NodeIndexInfo::GetNodeOffset
onnxruntime::NodeIndexInfo::Init::<lambda_24d5fcc8102cdc77a3db148fe9344e7b>::operator ()
onnxruntime::NodeIndexInfo::Init::<lambda_95a8f80a3b96610e783f7b34d80ac7bd>::operator ()
onnxruntime::NonMaxSuppression::Compute
onnxruntime::NonMaxSuppressionBase::GetThresholdsFromInputs
onnxruntime::NonMaxSuppressionBase::NonMaxSuppressionBase
onnxruntime::NonMaxSuppressionBase::PrepareCompute
onnxruntime::NonTensorTypeBase::FromDataContainer
onnxruntime::NonTensorTypeBase::IsMapCompatible
onnxruntime::NonTensorTypeBase::IsSequenceCompatible
onnxruntime::NonTensorTypeBase::ToDataContainer
onnxruntime::NonZero<__int64>::Compute
onnxruntime::NonZero<bool>::Compute
onnxruntime::NonZero<float>::Compute
onnxruntime::NonZero<int>::Compute
onnxruntime::NonZero<unsigned char>::Compute
onnxruntime::NoTransposeReduce
onnxruntime::OneHotOp<__int64,__int64,__int64>::Compute
onnxruntime::OneHotOp<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::Compute
onnxruntime::OneHotOp<__int64,float,__int64>::Compute
onnxruntime::OneHotOp<__int64,float,float>::Compute
onnxruntime::OneHotOp<__int64,float,int>::Compute
onnxruntime::OneHotOp<__int64,int,float>::Compute
onnxruntime::OneHotOp<float,__int64,__int64>::Compute
onnxruntime::OneHotOp<float,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::Compute
onnxruntime::OneHotOp<float,float,float>::Compute
onnxruntime::OneHotOp<int,float,float>::Compute
onnxruntime::OneHotOp<int,float,int>::Compute
onnxruntime::OnnxRuntimeOpSchemaRegistry::RegisterOpSchemaInternal
onnxruntime::OnnxRuntimeOpSchemaRegistry::RegisterOpSet
onnxruntime::OpKernel::ComputeAsync
onnxruntime::OpKernelContext::GetOrCreateOutputMLValue
onnxruntime::OpKernelContext::Input
onnxruntime::OpKernelContext::NumVariadicInputs
onnxruntime::OpKernelContext::OpKernelContext
onnxruntime::OpKernelContext::Output
onnxruntime::OpKernelContext::OutputMLValue
onnxruntime::OpKernelContextInternal::OpKernelContextInternal
onnxruntime::OpKernelInfo::GetMemoryInfo
onnxruntime::OpNodeProtoHelper<class onnxruntime::ProtoHelperNodeContext>::GetAttrs
onnxruntime::OpNodeProtoHelper<struct onnx::InferenceContext>::GetAttrs
onnxruntime::optimizer_utils::GenerateRewriteRules
onnxruntime::optimizer_utils::GenerateTransformers
onnxruntime::OptimizerExecutionFrame::Info::{ctor}::<lambda_2242169c388de54aeef0fbc82d712874>::operator ()
onnxruntime::OptimizerExecutionFrame::Info::Info
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Create
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Iterator::Iterator
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Iterator::operator *
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Create
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Iterator::Iterator
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Iterator::operator *
onnxruntime::OutputBroadcaster::OutputBroadcaster
onnxruntime::Pad::Compute
onnxruntime::PadBase::PadBase
onnxruntime::PadImpl
onnxruntime::PadInputWithDimValueOfZero
onnxruntime::PadValueFromFloat
onnxruntime::ParallelExecutor::Execute
onnxruntime::ParallelExecutor::RunNodeAsync
onnxruntime::PartitionOnnxFormatModelImpl
onnxruntime::PartitionOrtFormatModelImpl
onnxruntime::Path::Parse
onnxruntime::PlaceNode
onnxruntime::PlannerImpl::AllocPlan
onnxruntime::PlannerImpl::Buffer
onnxruntime::PlannerImpl::ComputeReusePlan
onnxruntime::PlannerImpl::ComputeUseCounts
onnxruntime::PlannerImpl::CreatePlan
onnxruntime::PlannerImpl::GenerateDeallocationPlan
onnxruntime::PlannerImpl::GeneratePlanForWeights
onnxruntime::PlannerImpl::GetElementSize
onnxruntime::PlannerImpl::GetLocationForNodeInput
onnxruntime::PlannerImpl::Index
onnxruntime::PlannerImpl::ProcessDef
onnxruntime::PlannerImpl::Reuse
onnxruntime::PlannerImpl::UseCount
onnxruntime::PlannerImpl::VerifyMemoryTimeSchedule
onnxruntime::Pool<float,class onnxruntime::LpPool>::Compute
onnxruntime::PoolAttributes::ComputeSizePadDilations
onnxruntime::PoolAttributes::InferOutputSize
onnxruntime::PoolAttributes::PoolAttributes
onnxruntime::PoolAttributes::SetOutputSize
onnxruntime::PoolBase::Compute
onnxruntime::PoolProcessContext::init
onnxruntime::PrepareForQDQ
onnxruntime::profiling::Profiler::EndProfiling
onnxruntime::profiling::Profiler::EndTimeAndRecordEvent
onnxruntime::profiling::Profiler::Initialize
onnxruntime::profiling::Profiler::StartProfiling
onnxruntime::profiling::Profiler::StartTime
onnxruntime::ProviderLibrary::Get
onnxruntime::ProviderSharedLibrary::Ensure
onnxruntime::QLinearConv::Compute
onnxruntime::QLinearMatMul::Compute
onnxruntime::RandomNormal::RandomNormal
onnxruntime::RandomNormalCompute
onnxruntime::RandomNormalLike::Compute
onnxruntime::RandomNormalLike::RandomNormalLike
onnxruntime::RandomUniform::RandomUniform
onnxruntime::RandomUniformCompute
onnxruntime::RandomUniformLike::Compute
onnxruntime::RandomUniformLike::RandomUniformLike
onnxruntime::ReduceKernelBase<0>::ReduceKernelBase
onnxruntime::ReduceKernelBase<1>::ReduceKernelBase
onnxruntime::ReduceSum<__int64>::Impl
onnxruntime::ReduceSum<double>::Impl
onnxruntime::ReduceSum<float>::Impl
onnxruntime::ReduceSum<int>::Impl
onnxruntime::RegisterCPUKernels
onnxruntime::RegisterOnnxOperatorKernels
onnxruntime::ReleaseNodeMLValues
onnxruntime::RemoveDuplicateCastTransformer::ApplyImpl
onnxruntime::Reshape::Compute
onnxruntime::Reshape_1::Reshape_1
onnxruntime::ReshapeFusion::ApplyImpl
onnxruntime::ReshapeFusion::Fuse_Subgraph
onnxruntime::ReshapeHelper::ReshapeHelper
onnxruntime::ReverseSequenceOp::Compute
onnxruntime::ReverseSequenceOp::ReverseSequenceOp
onnxruntime::rnn::detail::ComputeGemm
onnxruntime::rnn::detail::deepcpu::ActivationFuncByName
onnxruntime::rnn::detail::deepcpu::GruOutputGateFuncByName
onnxruntime::rnn::detail::deepcpu::GruResetGateFuncByName
onnxruntime::rnn::detail::deepcpu::LstmMergeGatesFuncByName
onnxruntime::rnn::detail::MakeDirection
onnxruntime::rnn::detail::NormalizeActivationArgumentAndGetAlphaBetaCount
onnxruntime::rnn::detail::SafeRawConstPointer
onnxruntime::rnn::detail::SafeRawPointer
onnxruntime::RNN<float>::Compute
onnxruntime::RNN<float>::RNN
onnxruntime::RoiAlignBase::RoiAlignBase
onnxruntime::RoiPool<float>::Compute
onnxruntime::RoiPool<float>::RoiPool
onnxruntime::RuleBasedGraphTransformer::ApplyImpl
onnxruntime::RuleBasedGraphTransformer::ApplyRulesOnNode
onnxruntime::SaveModel
onnxruntime::scan::detail::CreateFeedsFetchesManager
onnxruntime::scan::detail::Info::Info
onnxruntime::scan::detail::IterateSequence
onnxruntime::scan::detail::IterateSequence::<lambda_f112d6225128e4a2bde6b3763bdc14bc>::operator ()
onnxruntime::scan::detail::LoopStateVariable::Next
onnxruntime::scan::detail::OutputIterator::AllocateFinalBuffer
onnxruntime::scan::detail::OutputIterator::AllocateFinalOutput
onnxruntime::scan::detail::OutputIterator::GetOutput
onnxruntime::scan::detail::OutputIterator::Initialize
onnxruntime::scan::detail::OutputIterator::operator *
onnxruntime::scan::detail::OutputIterator::operator ++
onnxruntime::scan::detail::ReadDirections
onnxruntime::Scan<8>::Compute
onnxruntime::Scan<8>::Scan
onnxruntime::Scan<8>::SetupSubgraphExecutionInfo
onnxruntime::Scan<9>::Compute
onnxruntime::Scan<9>::Scan
onnxruntime::Scan<9>::SetupSubgraphExecutionInfo
onnxruntime::Scan8Impl::AllocateOutputTensors
onnxruntime::Scan8Impl::CreateLoopStateVariables
onnxruntime::Scan8Impl::Execute
onnxruntime::Scan8Impl::Initialize
onnxruntime::Scan8Impl::ValidateInput
onnxruntime::ScanImpl::AllocateOutputTensors
onnxruntime::ScanImpl::CreateLoopStateVariables
onnxruntime::ScanImpl::Execute
onnxruntime::ScanImpl::Initialize
onnxruntime::ScanImpl::SetupInputs
onnxruntime::ScanImpl::TransposeOutput
onnxruntime::ScanImpl::ValidateInput
onnxruntime::Scatter::Compute
onnxruntime::Scatter::Scatter
onnxruntime::ScatterND::Compute
onnxruntime::ScatterNDBase::PrepareForCompute
onnxruntime::SequenceAt::Compute
onnxruntime::SequenceConstruct::Compute
onnxruntime::SequenceEmpty::Compute
onnxruntime::SequenceErase::Compute
onnxruntime::SequenceInsert::Compute
onnxruntime::SequenceLength::Compute
onnxruntime::SequenceTensorTypeBase::GetElementType
onnxruntime::SequenceTensorTypeBase::IsCompatible
onnxruntime::SequentialExecutor::Execute
onnxruntime::session_state_utils::DeserializeTensorProto
onnxruntime::session_state_utils::SaveInitializedTensors
onnxruntime::session_state_utils::SaveInitializedTensors::<lambda_0f3baafaadb6d956732a1772e571d27a>::operator ()
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping::<lambda_613905a3c12d2353d146a5d251b5caad>::operator ()
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping::<lambda_83eca28d6349860dc86100acaa252e03>::operator ()
onnxruntime::SessionOptions::AddConfigEntry
onnxruntime::SessionState::AddOutputNameToNodeInfoMapping
onnxruntime::SessionState::AddSubgraphSessionState
onnxruntime::SessionState::CreateGraphInfo
onnxruntime::SessionState::CreateSubgraphSessionState
onnxruntime::SessionState::FinalizeSessionState
onnxruntime::SessionState::FinalizeSessionStateImpl
onnxruntime::SessionState::GetNodeIndexInfo
onnxruntime::SessionState::GetNodeKernelCreateInfo
onnxruntime::SessionState::LoadFromOrtFormat
onnxruntime::SessionState::LoadFromOrtFormat::<lambda_e5149623d2be3341d6cd63f43fb8aa4e>::operator ()
onnxruntime::SessionState::PopulateKernelCreateInfo
onnxruntime::SessionState::PrepackConstantInitializedTensors
onnxruntime::SessionState::SaveToOrtFormat
onnxruntime::SessionState::SetupAllocators
onnxruntime::SessionState::UpdateToBeExecutedNodes
onnxruntime::SetEnableProfiling
onnxruntime::SetExecutionMode
onnxruntime::SetGraphOptimizationLevel
onnxruntime::SetInterOpNumThreads
onnxruntime::SetIntraOpNumThreads
onnxruntime::SetupForReduce
onnxruntime::Shrink::Shrink
onnxruntime::SimpleTensorAllocator::GetPreallocatedBuffer
onnxruntime::SimplifiedLayerNormFusion::ApplyImpl
onnxruntime::SkipLayerNormFusion::ApplyImpl
onnxruntime::SliceBase::Compute
onnxruntime::SliceBase::FillVectorsFromInput
onnxruntime::SliceBase::SliceBase
onnxruntime::SliceImpl::<lambda_0eaa0a0fc928d4f0bec531de7bda1edb>::operator ()
onnxruntime::SliceImpl::<lambda_3f2606452e09d9a3394f78e248d1f350>::operator ()
onnxruntime::SliceImpl::<lambda_5b57937f5c481dce74849e89fba461d3>::operator ()
onnxruntime::SliceImpl::<lambda_7ba9c5a2833b2b4f43ecc78b0cfd639d>::operator ()
onnxruntime::SliceImpl::<lambda_dae8352f63c2d3b4a7fe471396e5ee55>::operator ()
onnxruntime::SliceIteratorBase::CopyInnermostAxisNonSolitaryInnerStep
onnxruntime::SliceIteratorBase::Init
onnxruntime::SliceSkips::SliceSkips
onnxruntime::Softmax<double>::ComputeImplOpset13
onnxruntime::Softmax<float>::ComputeImplOpset13
onnxruntime::SpaceDepthBase::SpaceDepthBase
onnxruntime::SpaceToDepth<float>::Compute
onnxruntime::SparseTensorTypeBase::GetElementType
onnxruntime::SparseTensorTypeBase::IsCompatible
onnxruntime::Split::Compute
onnxruntime::Split::ComputeImpl
onnxruntime::SplitBase::SplitBase
onnxruntime::SplitToSequence::Compute
onnxruntime::SplitToSequence::ComputeImpl
onnxruntime::Squeeze::Compute
onnxruntime::SqueezeBase::ComputeOutputShape
onnxruntime::string_normalizer::Locale::Locale
onnxruntime::StringNormalizer::StringNormalizer
onnxruntime::StringToAutoPadType
onnxruntime::Sum_6<double>::Compute
onnxruntime::Sum_6<float>::Compute
onnxruntime::Tensor::Data
onnxruntime::Tensor::DataAsSpan
onnxruntime::Tensor::DataRaw
onnxruntime::Tensor::Init
onnxruntime::Tensor::MutableData
onnxruntime::Tensor::MutableDataAsSpan
onnxruntime::Tensor::MutableDataRaw
onnxruntime::Tensor::Reshape
onnxruntime::Tensor::SizeInBytes
onnxruntime::Tensor::Tensor
onnxruntime::TensorAllocator::TensorAllocator
onnxruntime::TensorAllocatorWithMemPattern::FinalizePlan
onnxruntime::TensorAllocatorWithMemPattern::Trace
onnxruntime::TensorSeq::Get
onnxruntime::TensorSeq::SetType
onnxruntime::TensorShape::SizeFromDimension
onnxruntime::TensorShape::SizeToDimension
onnxruntime::TensorShape::Slice
onnxruntime::TensorTypeBase::GetElementType
onnxruntime::TensorTypeBase::IsCompatible
onnxruntime::TfIdfVectorizer::TfIdfVectorizer
onnxruntime::Tile::Compute
onnxruntime::ToMBString
onnxruntime::TopkOpset10ConstructorCommon
onnxruntime::TopkOpset11ConstructorCommon
onnxruntime::TopkOpset9ConstructorCommon
onnxruntime::ToWideString
onnxruntime::TransformerMemcpyImpl::ProcessDefs
onnxruntime::TransformerMemcpyImpl::ProcessInitializers
onnxruntime::TransformerMemcpyImpl::ProcessInitializers::<lambda_b769cb65454a05d6b4a26fb57be3e5e8>::operator ()
onnxruntime::Transpose::Compute
onnxruntime::TransposeBase::TransposeBase
onnxruntime::TypedDoTransposeEltWise
onnxruntime::Unsqueeze::Compute
onnxruntime::UnsqueezeBase::PrepareCompute
onnxruntime::UnsqueezeBase::UnsqueezeBase
onnxruntime::UnsqueezeElimination::Apply
onnxruntime::UntypedExpand
onnxruntime::update_subgraphs_within_function_body
onnxruntime::Upsample<float>::BaseCompute
onnxruntime::Upsample<float>::Compute
onnxruntime::Upsample<int>::BaseCompute
onnxruntime::Upsample<int>::Compute
onnxruntime::Upsample<unsigned char>::BaseCompute
onnxruntime::Upsample<unsigned char>::Compute
onnxruntime::UpsampleBase::ParseScalesData
onnxruntime::UpsampleBase::ParseScalesDataFromOutputSize
onnxruntime::UpsampleBase::ScalesValidation
onnxruntime::UpsampleBase::StringToCoordinateTransformationMode
onnxruntime::UpsampleBase::StringToNearestMode
onnxruntime::UpsampleBase::StringToUpsampleMode
onnxruntime::UpsampleBase::UpsampleBase
onnxruntime::utils::BatchOrCopyMLValue
onnxruntime::utils::CalculateStaticCopyInfoForFeed
onnxruntime::utils::CalculateStaticCopyInfoForFeeds
onnxruntime::utils::ConstantNodeProtoToTensorProto
onnxruntime::utils::ContainerChecker::ContainerChecker
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,__int64,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,__int64> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,double,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,double> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,__int64> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,double,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,double> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::vector<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > >,class std::allocator<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > > > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::vector<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > >,class std::allocator<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > > > > >::check
onnxruntime::utils::CopyInputsAcrossDevices
onnxruntime::utils::CopyOneInputAcrossDevices
onnxruntime::utils::CopyOutputsAcrossDevices
onnxruntime::utils::CopySparseData
onnxruntime::utils::DenseTensorToSparseTensorProto
onnxruntime::utils::detail::CopyLittleEndian
onnxruntime::utils::ExecuteGraph
onnxruntime::utils::ExecuteGraphImpl
onnxruntime::utils::FinalizeCopyInfoForFeeds
onnxruntime::utils::FinalizeCopyInfoForFetches
onnxruntime::utils::FindMemoryInfoForValue
onnxruntime::utils::GetFileContent
onnxruntime::utils::GetMLDataType
onnxruntime::utils::InitializeFeedFetchCopyInfo
onnxruntime::utils::mltype_dispatcher_internal::CallableDispatchableHelper::CheckCalledOnce
onnxruntime::utils::mltype_dispatcher_internal::UnsupportedTypeDefaultPolicy<class onnxruntime::common::Status>::operator ()
onnxruntime::utils::SparseTensorProtoToDenseTensorProto
onnxruntime::utils::TensorProtoToMLValue
onnxruntime::utils::UnpackInitializerData
onnxruntime::utils::UnpackTensorWithExternalDataImpl
onnxruntime::ViewerFunctionImpl::Body
onnxruntime::WritableSliceIterator<__int64>::Init
onnxruntime::WritableSliceIterator<double>::Init
onnxruntime::WritableSliceIterator<float>::Init
onnxruntime::WritableSliceIterator<int>::Init
onnxruntime_profile_
onnxruntime_providers_dnnl.dll
onnxruntime_providers_openvino.dll
onnxruntime_providers_shared.dll
onnxruntime_providers_tensorrt.dll
Op registered for 
Op with name (
op_kernel_info.GetAttr("axis", &axis_).IsOK()
op_kernel_info.GetAttr<float>("bias", &bias_temp).IsOK()
op_kernel_info.GetAttr<float>("epsilon", &epsilon_).IsOK()
op_kernel_info.GetAttr<float>("lambd", &lambd_temp).IsOK()
op_kernel_info.GetAttr<int64_t>("axis", &axis_).IsOK()
op_kernel_info.GetAttr<int64_t>("axis", &axis_temp).IsOK()
op_kernel_info.GetAttr<int64_t>("k", &k_temp).IsOK()
op_kernel_info.GetAttr<int64_t>("largest", &largest_temp).IsOK()
op_kernel_info.GetAttr<int64_t>("p", &p_).IsOK()
op_kernel_info.GetAttr<int64_t>("sorted", &sorted_temp).IsOK()
op_name
op_type
opaque
Opaque type is not a non_tensor type!!!
opaque(
opaque_type
open file 
OpenSemaphoreW
OpenVINOExecutionProvider
operation canceled
operation in progress
operation not permitted
operation not supported
operation would block
operator
Operator '
operator "" 
operator co_await
operator<=>
OpKernel was null
opset id is null. Invalid ORT format model.
opset import domain is null. Invalid ORT format model.
Optional position subgraph nodes number of outputs unexpected.
Optional position subgraph nodes Where node is expected to be the parent of Reshape.
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM.
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM. Default values are the same as of corresponding ONNX operators.
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM. Default values are the same as of corresponding ONNX operators.For example with LeakyRelu, the default alpha is 0.01.
or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
Order
OriginalFilename
Oriya
ORT config json from the model: 
ORT model verification failed.
ort_config
ORT_LOAD_CONFIG_FROM_MODEL
ort_value.Fence() == nullptr
ort_value.IsAllocated()
ort_value.IsTensor()
ort_value_idx >= 0 && static_cast<size_t>(ort_value_idx) < alloc_plan.size()
ort_value_index >= 0 && static_cast<size_t>(ort_value_index) < all_values_size_
ort_value_index >= 0 && static_cast<size_t>(ort_value_index) < alloc_plan.size()
ort_value_name_idx_map.MaxIdx() > -1
OrtApis::CreateOpaqueValue
OrtApis::GetOpaqueValue
OrtApis::GetTensorTypeAndShape
OrtCreateMapMLValue
OrtCreateValueImplMapHelper
OrtCreateValueImplSeqHelper
OrtEnv::Release
OrtGetApiBase
OrtGetValueImplMapHelper
OrtGetWinMLAdapter
OrtMemoryInfo is null
OrtMemoryInfo:[
OrtProgrammingProjection
OrtSessionOptionsAppendExecutionProvider_CPU
OrtSessionOptionsAppendExecutionProvider_DML
OrtSessionOptionsAppendExecutionProviderEx_DML
OrtValue has not been allocated so can't be sliced.
OrtValue indexes should have been populated.
OrtValue is not a Tensor
OrtValue is TensorSequence type but has no element Tensor DataType.
OrtValue shape verification failed. Current shape:
OrtValue::Get
OrtValue::GetMutable
Osage
Osmanya
other_error
out of index
Out of memory
out_of_range
Outer scope node arg name '
outer_scope_node_args_consumed.empty()
OutOfBoundsInputValue
output
Output
output 
Output 
output != nullptr
output == output_end
Output buffer allocation failed
output buffer is too small
Output case #1: Y, mean, var, saved_mean, saved_var (training mode)
Output case #2: Y (test mode)
Output channels M is not divisible by group.
output count mismatch, expected 2 outputs to be present for TopK operator
Output data after scaling
Output data tensor.
Output edge count not expected for Add or MatMul in path v
Output edge count not expected for mask nodes
Output edge count not expected for nodes in gemm gather path
Output edge count not expected for nodes in gemm path
Output edge count not expected for nodes in past subgraph
Output edge count not expected for nodes in path 1 of position shape.
Output edge count not expected for nodes in path 1 of unidirectional mask
Output edge count not expected for nodes in path 2 of position shape.
Output edge count not expected for nodes in path v
Output edge count not expected for nodes in path1.
Output edge count not expected for Softmax
Output edge count not expected for squeeze_2/slices2/shape2 of unidirectional mask
Output edge count not expected for unsqueeze2 of unidirectional mask
Output edge count not expected for unsqueeze3 of unidirectional mask
output edges
output name cannot be empty
Output OrtValue has not been created for loop state variable output 
Output subscript contains letters not seen in the inputs
Output subscript contains repeated letters
Output tensor
output tensor
Output tensor must have at least 2 dimensions
Output tensor of shape (M, N).
Output tensor of the same type and shape as the input tensor.
Output type is determined by the specified 'values_*' attribute.
Output type must be int32 or int64
Output vector incorrectly sized: output_names.size(): 
Output vector pointer is NULL
Output was expected to have tensor type. Got 
output.SizeInBytes() == input.SizeInBytes()
Output:
output_dims.size() == dims.size()
output_dims[i] == 0
output_height
output_mlvalue
output_names_to_nodeinfo.empty()
output_node.OutputDefs().size() == 1
output_padding
output_sequence
output_shape
output_shape is smaller than minimum required. output_shape:
'output_shape' must be rank 1 tensor.
'output_shape' must have same number of elements as the shape of input tensor X.
output_size
output_width
OutputCellSingleTensor
OutputCoordinatesTensor
OutputCount
OutputCountTensor
OutputDebugStringW
OutputFirstMomentTensor
OutputGradientTensor
OutputIndexTensor
OutputIndicesTensor
OutputPadding
OutputParametersTensor
OutputPixelOffsets
outputs
Outputs from Scan are not optional and should never be null.
outputs...
OutputScaleTensor
OutputSecondMomentTensor
OutputSequenceTensor
OutputSingleTensor
OutputStateTensor
OutputTensor
OutputTensors
OutputValueTensor
OutputZeroPointTensor
owner dead
p p t y 
p value of the Lp norm used to pool over the input data, default is 2.0.
p value of the Lp norm used to pool over the input data.
p.first->second->id_ == 0
p.second
P;K0t
p;S u
P;V,u&
p_ == 1 || p_ == 2
p_fetches->size(): 
p_int < base_int + memory_size_
p_int >= base_int
p_kernel_def
p_ml_value
p_mlvalue
p_op_kernel
p_provider
p_type != nullptr
p|+px
P|+Px
p|+px
P|+Px
p|+px
P|+Px
P|+Px3
p|+pxXf
P0~011O1c1
P002v2
P0T0X0\0`0p0t0
p7M}6p7M
Pad should be smaller than kernel.
pad_value
Padding for the beginning and ending along each axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute.
Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaults to 0 along start and end of each spatial axis.
Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0.The value represent the number of pixels added to the beginning and end part of the corresponding axis.`pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number ofpixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaultsto 0 along start and end of each spatial axis.
PaddingMode
paddings
PaddingValue
Pads has incorrect number of values
'pads' has wrong number of values
'pads' input must be a 1D (shape: [2 * input_rank]) tensor of type int64
'pads' input must be a 1D (shape: [2 * n_input_dims]) tensor of type int64
'pads' input must be a 1D (shape: [input_rank]) or 2D tensor (shape: [1, input_rank]) of type int64
Pads tensor should be a 1D tensor of shape [2 * input_rank] or a 2D tensor of shape [1, 2 * input_rank]
Pads tensor should be an INT64 tensor
Pads tensor size should be equal to twice the input dimension count 
pads[dim] < kernel_shape[dim] && pads[dim + kernel_shape.size()] < kernel_shape[dim]
pads_[dim] < kernel_shape_[dim] && pads_[dim + kernel_shape_.size()] < kernel_shape_[dim]
pads_size == 2 * data_rank
pads_tensor.IsDataType<int64_t>()
pads_tensor_dims.size() == 1 || (pads_tensor_dims.size() == 2 && pads_tensor_dims[0] == 1)
Pahawh_Hmong
Palmyrene
Parallel execution mode does not support the CUDA Execution Provider. 
Parallel execution mode does not support the DML Execution Provider. 
Parallel mode
ParallelExecutor::Execute
parameter_size
ParametricSoftplus
ParametricSoftplus takes one input data (Tensor<T>) and produces one output data
parse
parse error
parse_error
parsing 
PartA_PrivTags
Pass 1 to enable broadcasting
Pass CheckNodesInPathK
Pass CheckNodesInPathQ
Pass CheckNodesInPathV
Pass MatchGemmSubgraph
Pass MatchInputMaskSubgraph
Pass MatchInputMaskSubgraphDistilBert
Pass MatchPastSubgraph
Pass MatchUnidirMaskSubgraph
Pass ValidateGemmInitializer
past_k_gather indices != 0
past_k_transpose perm attribute not matched
past_v_gather and past_k_gather does not have same past input
past_v_gather indices != 1
PathCchRemoveBackslash
PathCchRemoveFileSpec
pattern too large - compile failed
pattern too large - reverse compile failed
Pau_Cin_Hau
pD;pDt0
PeepholeTensor
Perform mean variance normalization.
Performs element-wise binary {name} on 8 bit data types (with Numpy-style broadcasting support).
perm: 
permission denied
Ph _<
Ph |=
Ph +>
Ph =<
Ph 3=
Ph 6=
Ph A<
Ph a<
Ph B<
Ph B=
Ph d<
Ph D<
Ph E<
Ph E=
Ph e=
Ph G<
Ph H=
Ph K<
Ph K=
Ph l<
Ph L<
Ph m<
Ph m>
Ph o<
Ph p<
Ph q<
Ph R<
Ph S<
Ph t<
Ph T<
Ph t<
Ph u=
Ph W<
Ph X<
Ph Z=
Ph$&=
Ph$@<
Ph$e<
Ph$E>
Ph$f<
Ph$h<
Ph$l=
Ph$M<
Ph(?<
Ph([<
Ph(\<
Ph(]<
Ph(^<
Ph(`<
Ph({>
Ph(><
Ph(b<
Ph(C<
Ph(F<
Ph(g<
Ph(H<
Ph(J<
Ph(k<
Ph(n<
Ph(O<
Ph(P<
Ph(Q<
Ph(S=
Ph(U<
Ph(V<
Ph(Y<
Ph(Z<
Ph,:=
Ph,\>
Ph,}=
Ph,d=
Ph,q=
Ph@_<
Ph@{=
Ph@=<
Ph@4<
Ph@B<
Ph@D<
Ph@E<
Ph@G<
Ph@H<
Ph@I<
Ph@j<
Ph@l<
Ph@L<
Ph@L>
Ph@m<
Ph@o<
Ph@p<
Ph@Q4
Ph@R<
Ph@T<
Ph@V=
Ph\/=
Ph\>>
Ph\d=
Ph\F<
Ph\F>
Ph\h>
Ph\J<
Ph\K<
Ph\N<
Ph\O<
Ph\S<
Ph\U<
Ph`@<
Ph`[<
Ph`_<
Ph`8=
Ph`A<
Ph`B<
Ph`D<
Ph`E<
Ph`f<
Ph`H<
Ph`I<
Ph`l<
Ph`L<
Ph`l<
Ph`n<
Ph`o<
Ph`O4
Ph`Q<
Ph`R<
Ph`T<
Ph||=
Ph|}?
Ph|><
Ph|b<
Ph|e<
Ph|J<
Ph|O<
Ph|S>
Ph<^<
Ph<b>
Ph<n<
Ph0!>
Ph0_<
Ph0=<
Ph07<
Ph09<
Ph0a<
Ph0A<
Ph0a>
Ph0B<
Ph0D<
Ph0E<
Ph0G<
Ph0I<
Ph0j<
Ph0l<
Ph0L<
Ph0m<
Ph0p<
Ph0R<
Ph0T<
Ph0W<
Ph4(=
Ph4d<
Ph4e<
Ph4h<
Ph4K<
Ph4M<
Ph4S<
Ph4X<
Ph8;>
Ph8?<
Ph8@<
Ph8[<
Ph8\<
Ph8]<
Ph8`<
Ph8~<
Ph8><
Ph81<
Ph87=
Ph8b<
Ph8C<
Ph8c<
Ph8F<
Ph8f<
Ph8g<
Ph8J<
Ph8k<
Ph8O<
Ph8P<
Ph8P=
Ph8Q<
Ph8q<
Ph8U<
Ph8V<
Ph8Y<
Ph8Z<
Phags_Pa
Phd)=
Phd^<
PhD|?
PhD9=
PhDa<
PhDe<
PhdG<
PhDh<
Phdm<
Phdp<
PhDW<
PhdW<
PhH*=
PhH?<
Phh?<
Phh\<
PhH\<
PhH]<
Phh`<
PhH`<
PhH`=
PhH|?
Phh+>
PhH><
Phh><
Phha<
Phha=
PhhA=
Phhb<
PhHb<
PhHc<
PhhC<
PhHC<
Phhc<
Phhd<
PhHd<
PhhD=
PhHd>
Phhe<
PhHg<
Phhg<
Phhh<
Phhi<
PhHi<
PhhJ=
Phhk<
PhHk<
PhHK<
PhHM<
PhhM<
PhHM=
PhHN<
PhHO<
PhHP<
PhhP<
Phhr<
PhHS<
PhHU<
PhHV<
PhhV<
Phhv=
PhHW=
PhHX<
PhhY<
PhHY<
PhhZ<
PhHZ<
Phl;=
PhL@<
PhL[<
Phl}<
PhL}<
PhL1=
PhL4>
PhLF<
PhlF<
PhlJ<
PhlK<
PhlO<
Phlt=
Phlw=
PhlX<
Phoenician
PhP'?
Php@<
PhP^<
PhP_<
Php=<
PhP=<
Php6=
PhPA<
PhPB<
PhpB<
PhPD<
PhpD<
PhpE<
PhPE<
Phpf<
PhPf<
PhpH<
PhPH<
PhpI<
PhPI<
Phpj<
PhPj<
PhpJ>
PhpL<
Phpl<
PhPl<
PhPL<
PhPm<
Phpn<
PhpN<
PhPn<
PhPo<
Phpo<
Phpo=
PhPP4
PhPQ<
PhpQ<
PhPR<
PhpR<
PhPR=
PhpS<
PhPs<
PhpT<
PhPT<
PhpU<
Pht[<
Pht_<
Pht9=
PhtA<
PhTe<
PhtG<
PhTG<
PhTh<
Phtj=
PhTp<
Phtp<
PhTu=
PhTW<
PhtW<
Phx?<
PhX?<
Phx\<
PhX\<
Phx]<
Phx]=
Phx^<
Phx`<
PhX`<
PhX}>
PhX><
Phx9>
Phxa<
PhXb<
PhxC<
PhXc<
PhXC<
Phxd<
PhXd<
PhXg<
Phxg<
PhXG=
Phxh<
Phxi<
PhXi<
Phxk<
PhXk<
PhxM<
PhXM<
Phxm<
PhXP<
PhxP<
PhxT=
PhxV<
PhXV<
PhXY<
PhXy=
PhxZ<
Please fetch output tensor with specified shape.
pool_int64s
pool_strings
pool_strings must not be empty if specified
pooled_height_ > 0
pooled_shape
pooled_shape.size() == 2
pooled_width_ > 0
PooledSize
Popularity of each node, used for performance and may be omitted.
position
Position embedding data type shall be float or float16.
Position embedding shape is not expected.
Position embedding shape not matched.
position_ >= 0 && position_ < sequence_length_
position_embedding
position_embedding is expected to have 2 dimensions, got 
position_embeddings
positive
post_transform
Pow takes input data (Tensor<T>) and exponent Tensor, and
pp+pl
PPhP5<
PPPPPPPPPPP
PPPQQh
PPPQQh 
PPPQQhP
PPQQPVQh
PPSQh
PPSQh\[?
PPSSPRQh
PPSSSQQh
PPSSSWQh
PPWQPh
PPWWPVQh
PQh(B?
PQh8y@
PQhh_?
PQhL(?
PQhP'?
PQhTO?
PQhxI?
PQQQR
PQQQRQ
PQQQRQh
PQQQV
PQQSV
PQQSVW
PQRWS3
PQSVW
PQVVh
predictions.size() == (size_t)n_targets_or_classes_
predictions.size() == 2
predictions.size() == predictions2.size()
PRelu
present
present_k_transpose perm attribute not matched
present_k_unsqueeze axes value not expected
present_v_unsqueeze axes value not expected
Previous entry was not terminated.
PRhPz?
private: 
prob_a
prob_b
proba_.size() == probb_.size()
Processed_STD
ProcessInfo
Produces a slice of the input tensor along multiple axes. Similar to numpy:
produces one output data (Tensor<T>) where the function `f(x) = x^exponent`,
ProductName
ProductVersion
Profiler is disabled.
protected: 
proto != nullptr
Protobuf parsing failed.
Protobuf serialization failed.
protocol error
protocol not supported
provided axis. The resulting tensor has the same rank as the input if keepdims equal 1.
provider
Provider 
Provider_SetHost
PRSRRSh 
PRVhxs<
PRVhXs<
PRWQf
Psalter_Pahlavi
PSh w@
PShpw@
PSPPS
PSPPSQh
PSPPSQh \?
PSPPSQh4r?
PSPPSQhpP?
PSPPSQht{?
PSPPSQQh
PSPPSQQh|#?
PSPPSQQhtt?
PSPPSQRh
PSPPSWQh
PSQQQVh
PSQQSPh
PSQQSPh<@?
PSQQSPhh{?
PSRRRPQh
PSRRS
PSRRSh
PSRRSPhhN?
PSRRSPQh
PSSPh
PSSPh,
PSSPVQh
PSSPVQhl
PSSPWQh
PSSPWQh,)?
PSSPWQh|&?
PSSPWQhD(?
PSSPWQhxs<
PSSSQQh<)?
PSSSRQh
PSSSRQhh9?
PSSSWQh
PSSSWQh0(?
PSSSWQhxs<
PSWhp
public: 
PVWRSQ
PVWWRh
PWj#S
PWj?S
PWj@S
PWj{S
PWj|S
PWj}S
PWj~S
PWj3S
PWjoY
PWjsS
PWjTS
PWjtS
PWjTS
PWjuS
PWjvS
PWjwS
PWjxS
PWjyS
PWjzS
PWWPSQh
PWWPSQhl
PWWPVQh
PWWWWSRQh
pytorch_half_pixel
q and v are not from same Split node
q root should be layer normalization
Q!<$j
Q;CPt
Q;Ftt
Q;G,t>
q_matmul and q_add shape not matched
Q_Max
Q_Min
q_reshape const not matched
q_transpose perm attribute not matched
Q|+Qx
Q0+Q,
Q0h0m0r0
Q8+Q4
QAttention
QBC;]
QBRRQh
QBRRQh 
QBRRQSQh
QFVVQh
QFVVQSQh
Qh(8?
Qh,&@
Qh,}<
Qh|#?
QhL}<
Qhl}<
QhLf@
Qhp(<
QhP'?
QhPD@
QhPs?
QhPs<
QhT[?
Qhx&?
Qhx9@
QhXa<
Qhxs<
qk_div const not matched.
qkv_bias
qkv_weights
QLinearAdd
QLinearAveragePool
QLinearConv
QLinearConv : filter scale shape invalid
QLinearConv : filter zero point must be constant
QLinearConv : filter zero point shape invalid
QLinearConv : input scale must be a scalar or 1D tensor of size 1
QLinearConv : input zero point must be a scalar or 1D tensor of size 1
QLinearConv : result scale must be a scalar or 1D tensor of size 1
QLinearConv : result zero point must be a scalar or 1D tensor of size 1
QLinearGlobalAveragePool
QLinearGlobalAveragePool parameter out of computation range!
QLinearLeakyRelu
QLinearLeakyRelu : input X_scale must be a scalar or 1D tensor of size 1
QLinearLeakyRelu : input X_zero_point must be a scalar or 1D tensor of size 1
QLinearLeakyRelu : input Y_scale must be a scalar or 1D tensor of size 1
QLinearLeakyRelu : input Y_zero_point must be a scalar or 1D tensor of size 1
QLinearMatMul
QLinearMatmul : input scale must be a scalar or 1D tensor of size 1
QLinearMatmul : input zero point must be a scalar or 1D tensor of size 1
QLinearMatmul : result scale must be a scalar or 1D tensor of size 1
QLinearMatmul : result zero point must be a scalar or 1D tensor of size 1
QLinearMatmul : weight scale must be a scalar or 1D tensor of size 1
QLinearMatmul : weight zero point must be a scalar or 1D tensor of size 1
QLinearMul
QLinearReduceMean
QLinearSigmoid
QPh O?
QPSSPQQh
QQh|#?
QQhHz@
QQhx&?
QQPVQh
QQQh,
QQQQ3
QQQQj
QQQQPQh 
QQQQQPQh
QQQQRSQh
QQQQRSQh|#?
QQQQRSQhtt?
QQQQSPQQ
QQQQSRQh
QQQQSWQh
QQQQVSQh
QQQRQh
QQQRSQh
QQQRSQhl
QQQSQh
QQQSRQh
QQQSRQPj
QQRh,
QQRh`
QQRQj
QQRRQh
QQRRQSQh
QQRRRh,
QQRSQh
QQSPQh
QQSQQ
QQSSQh
QQSSSRQhd
QQSSSSRQh
QQSVW
QQSVW3
QQSVWd
QQV;U
QQVSQh
QQVVQSQh
QQVWj
QQVWQ
QQVWQQ
QQWSSSRQh
QRCSSRhL
QRGWWRSQh
QRh(g@
QRh(J@
QRh,`@
QRh,h@
QRh`h@
QRh|v@
QRh4F@
QRhLf@
QRhP'?
QRhpF@
QRhX}@
QRhXp@
QRPh(
QRQh<
QRQQQh
QRQQRSQh
QRQQRSQh|#?
QRRQh
QRRQSQh
QRRQSQhd
QRRQWQh\
QRRRh
QRRRj
QRRRQSQh
QRSSRQQh
QRSSRQQh$
QS@PPSh
QS@PPSQh
QS@PPSQh`c?
QS@PPSQh|l?
QS@PPSQhh{?
QS@PPSQhHL?
QS@PPSQQh
QS@PPSQRh
QS@PPSRh
QS@PPSVh(`?
QSBRRSQQh
QSh(B?
QSh(g@
QSh,h@
QSh`h@
QSh8y@
QShH[@
QShl[@
QShL_@
QShTO?
QShXp@
QSPPSQQh
QSPPSRQh
QSQQSPh
QSQQSPQh
QSQQSRhpP?
QSQQSRQh
QSQQSWQh
QSQQSWQhtt?
QSRhp
QSRhP'?
QSRhT
QSRRS
QSSQRQh
QSSSh
QSSSRQQh
QSSSWQh
QSSSWQQh
QSVPP
QSVWh
QSVWj
QSVWQ
QT;G$t-9
Quantized GEMM only support alpha equal to 1.0f and beta equal to 0.0f or 1.0f
QuantizeLinear
QueryPerformanceCounter
QueryPerformanceFrequency
QVPSRRSPQh
QVQQQPQ
QVQQVSQh
QVQVQh
QVQWQ
QVQWQS
QVRRVPQh
QVVPP
QVVQh
QVWj(
QWBRRWQQh
QWCSSSQQh
QWCSSWQQh
QWCSSWQQhl
QWQQWRQh
QWQQWRQh|
QWQSQ
QWQVQVQh
QWSh@J@
QWSh0
QWShP'?
QWSSSQQh
QWSSWQQh
QWSSWQQh$
r&;N4
R_scale
R_zero_point
R->Shape()[1] == 5
R0s0x0
raB3G
RaiseException
RaiseFailFastException
RandomNormal
RandomNormalLike
RandomUniform
RandomUniformLike
Range
range
rank == dims.size() && rank > 0
rank >= 2 && dim_1 != dim_2 && input_dims[dim_1] == input_dims[dim_2]
rank must be greater than axis
Rank of input 
Rank of input and output tensor should be same.
Rank of input to Normalized must be less than 2. Got 
Rank of the input must match number of subscript labels corresponding to the input
Ranks inferred (
Ranks of input data are different, cannot concatenate them. expected rank: 
Ranks of pair-wise operands must be equal. 
RAQQj
RAQQQSQh
RAQQRh
RAQQRh 
RAQQRh,
RAQQRSQh
RAQQRSQhl
RAQQRSQhL
RAQQRVQSRRQQRVQh
ratio
ratio input should have a single value.
ratio must be in the range [0, 1)
Ratio of Dropout must be a scalar.
ratio_tensor->Shape().Size() == 1
raw_data
RCSSRWQh
RE2: invalid startpos, endpos pair. [
RE2: unexpected op: 
read only file system
ReadExternalRawData() failed: 
ReadFile
ReadFile 
Reading the provided model for the ORT config
Real memory steps 
Received invalid value for allow_spinning. Valid values are 0 or 1
Received invalid value of arena_extend_strategy 
Received negative size from stat call
Received null OrtThreadingOptions
Received nullptr for custom registry
Received nullptr for exec provider
Received nullptr for graph transformer
Received nullptr for name.
Received nullptr for OrtValue.
Received OrtValue is not a tensor. Only tensors are supported.
Reciprocal
RecurrenceTensor
Recurrent
reduced
reduced_scale
reduced_zero_point
ReduceL1
ReduceL2
ReduceLogSum
ReduceLogSumExp
ReduceMax
ReduceMean
ReduceMin
ReduceProd
ReduceSum
ReduceSumInteger
ReduceSumSquare
reduction
Reduction on all axes, output size should be 1.
ReductionFunction
reflect
Regexp not destroyed.
RegisterCustomOps
RegisterCustomOpsLibrary: Entry point RegisterCustomOps not found in library
RegisterCustomOpsLibrary: Failed to load library
Rejang
Release_State_
ReleaseMutex
ReleaseSemaphore
ReleaseSRWLockExclusive
RemoveDirectory() failed - path: 
RemoveDirectoryW
RemoveDuplicateCastTransformer
Removing initializer '
ReorderInput
ReorderOutput
'repeat' input tensor must be 1 dimensional
'repeat' input tensor must have the same length as the 'input' tensor
Repeats
repeats
'Repeats' input has incorrect number of values. The number of values in 'repeats' must be equal to the number of input dimensions.
'Repeats' input must be 1D tensor of type int64
RepeatsCount
RepetitionWalker::ShortVisit called
replaced_value_float
replaced_value_int64
representative.output_index != kInvalidOutputIndex
Requested attribute: 
Requested size is too large to fit into size_t.
requested_shape[i] >= -1
Required attribute '
Required attribute axis is missing
Required min_gram_length must be positive: 
reserved_chunks_.find(ptr) == reserved_chunks_.end()
ResetEvent
Reshape
reshape initializer value is not expected
reshaped
ReshapeFusion
Resize
Resize operator
Resize: input shape needs to be at least a single dimension
Resize: input tensor's dimension does not match the scales.
Resize: input tensor's rank does not match the output tensor's rank.
Resize: input/output value is nullptr
Resize: input/output value's dimension mismatch
Resize: size of roi array should be 2 * N where N is the rank of input tensor X.
Resize: unexpected mode
Resolve subgraph failed:
ResolveDelayLoadedAPI
resource deadlock would occur
resource unavailable try again
result
Result buffer is not large enough
result out of range
Result, has same shape and type as input
Result, has same type as input, with H and W dimensions reduced.
result.second
ReturnHr
reused != reused_for
reverse
ReverseSequence
Rh`U?
RhLf@
RhP'?
RhpA?
RhX}@
RIGHT
right operand cannot broadcast on dim 
Right shape: 
right.NumDimensions() == 2
right.Shape().Size() == right_shape_override.Size()
RknpuExecutionProvider
RNN op: Invalid activation attribute - 
ROCMExecutionProvider
ROI pool output shape (height, width).
RoI pooled output, 4-D tensor of shape (num_rois, C, crop_height, crop_width). The r-th batch element Y[r-1] is a pooled feature map corresponding to the r-th RoI X[r-1].
roi_batch_id < batch_size
roi_batch_id >= 0
roi_input_idx_ > 0
RoiAlign
RoIs (Regions of Interest) to pool over; rois is 2-D input of shape (num_rois, 4) given as [[y1, x1, y2, x2], ...]. The RoIs' coordinates are normalized in the coordinate system of the input image. Each coordinate set has a 1:1 correspondence with the 'batch_indices' input.
rois input tensor has wrong dimension
RoIs tensor must have 2 dimensions
ROITensor
RoOriginateLanguageException
Round
round_prefer_ceil
round_prefer_floor
Rounded_ZeroPoint_FP
RoundingMode
RQjoY
RQQQh
RQQQj
RQQQPQhd&?
RQQQRSQh
RQQQSQh
RQQQWQh
RQQRh
RQQRh,
RQQRSQh
RQRRQSQh
RQRRQSQhT[?
RQRRRh
RQSQP
RRQQj
RRQQQh
RRQQQSQh
RRQQRh
RRQQRh,
RRQQRSQh
RRQRj
RRQSQh
RRQSQhT[?
RRRQQh
RRRQQhh&@
RRRQSQh
RRRR3
RRRRj
RRRRPQh
RRRRQQh
RRRRSQQh
RRRVSQh
RRRWQQh
RRSh,
RRSSj
RRSSRh
RRSSRQQh
RRSSRQQhl
RRSSRQQhtt?
RRSSSQQhl
RRSSSQQWSRSSRQQh
RRVQQh
RRVQQhL
RRVVVQQhT[?
RRWWRQQh
RRWWRQQhl
RSGWWShL
RSQQRWQh
RSQQSPhxw?
RSQQSRh
RSRRj
RSRRSPQh
RSRRSPQh|#?
RSRRSPQhtt?
RSRRSQQh
RSRRSWQhtt?
RSSRQQh
RSSSRQQh
RSSSSQQhl
RSSSWQh
RtlUnwind
run_options.run_log_severity_level >= 0 && run_options.run_log_severity_level <= static_cast<int>(logging::Severity::kFATAL)
Runic
Running with tag: 
RunStateOnByteUnlocked failed after Reset
RunStateOnByteUnlocked failed after ResetCache
RUNTIME_EXCEPTION
RuntimeError
RuntimePerf
runtimeVersion
RVRRRQQh
RVVVQQh
RWCSSWQQh
RWhL:@
RWhP'?
RWRRj
RWSh 
RWShh
RWSSSRQh
RWSSWQQh
RWWRQQh
S != nullptr
's number of inputs is different from function body graph's number of input.
's number of outputs is different from function body graph's number of outputs.
S WVj
s#QQQ
s&j1h
s,WPV
s,WRP
s/QPQ
S|WVj
S2g2t2{2
S3]3v3
s4+s0;
s4WQS
S4WVj
s9QSQ
SafeIntExceptionHandler<class onnxruntime::OnnxRuntimeException>::SafeIntOnDivZero
SafeIntExceptionHandler<class onnxruntime::OnnxRuntimeException>::SafeIntOnOverflow
Samaritan
SAME_LOWER
SAME_UPPER
Sample echo operator.
sample_size
SampleOp
Sampling ratio should be >=0, but it was 
sampling_ratio
sampling_ratio_ >= 0
Saurashtra
SaveAttributeOrtFormat: Unsupported attribute type: 
Saved inverse standard variance used during training to speed up gradient computation.
Saved mean used during training to speed up gradient computation
saved_mean
saved_var
SaveMLValueNameIndexMapping
SaveValueInfoOrtFormat: value_info_proto for 
Saving initialized tensors.
Sbad variant access
sbetu
Scalar multiplier for input tensor C, the default value is 1.0.
Scalar multiplier for input tensor C.
Scalar multiplier for the product of input tensors A * B, the default value is 1.0.
Scalar multiplier for the product of input tensors A * B.
Scalar multiplier for the product of the input tensors.
scale
Scale
scale > 0
scale >= 1
Scale and bias the input image. Bias values are stored in
Scale and Zero-point must be a scalar
scale must be 1D tensor with size 
'Scale' must contain exactly 2 values - (height, width)
Scale size: (
Scale tensor.
Scale value should be greater than 0.
Scale value should be greater than or equal to 1.
scale.Shape().NumDimensions() == 1 && scale.Shape()[0] == broadcast_dim
scale_.size() == offset_.size()
ScaleBias
ScaleCount
scaledtanh
ScaledTanh
Scaler
Scales
scales
scales size should be greater than 0.
scales.size() == 2 || (scales.size() == 4 && scales[0] == 1 && scales[1] == 1)
scales.size() == 2 || (scales.size() == 4 && scales[0] == 1 && scales[1] == 1) || scales.size() == 3 || (scales.size() == 5 && scales[0] == 1 && scales[1] == 1)
scales_size > 0
ScaleSize
ScaleTensor
Scaling parameter.
Scaling value
Scan 'body' subgraph outputs should all be tensors but output 
Scan input 
Scan inputs have inconsistent batch size. Previous value was 
Scan inputs have inconsistent sequence lengths. Previous value was 
scan_input_axes
scan_input_directions
scan_output_axes
scan_output_directions
Scan<8> spec does not support transpose of output. This should never be called.
Scatter
ScatterElements
ScatterND
Schema error: 
schemaVersion
score_threshold
scores
scores must be a 3D tensor.
Scores output is incorrect size. Expected:
scores.size() == static_cast<size_t>(expected_num_scores)
scores_output_data.length() >= scores_output_size
scores_tensor
SearchBitState inconsistency
SearchDFA inconsistency
SearchNFA inconsistency
SearchOnePass inconsistency
Second dimension for rois should be exactly 
Second input does not have rank 2
Second input of Gather in path 1 of position shape should be a constant with value 0.
Second input of Gather in path 2 of position shape should be a constant with value 1.
Second input of Gather should be a constant with value 1. 
Second input tensor has wrong dimension
Second set of probability coefficients. This array must be same size as prob_a.<br>If these are provided then output Z are probability estimates, otherwise they are raw scores.
Second, multiply by this.<br>Can be length of features in an [N,F] tensor or length 1, in which case it applies to all features, regardless of dimension count.<br>Must be same length as 'offset'
Seed for the hashing algorithm, unsigned 32-bit integer, default to 0.
Segment id is not valid. 
segment_embedding
segment_embedding is expected to have 2 dimensions, got 
segment_ids
select_last_index
selected_indices
separators
separators must not be empty
seq(map(int64, float))
seq(map(string, float))
seq(tensor(bool))
seq(tensor(complex128))
seq(tensor(complex64))
seq(tensor(double))
seq(tensor(float))
seq(tensor(float16))
seq(tensor(int16))
seq(tensor(int32))
seq(tensor(int64))
seq(tensor(int8))
seq(tensor(string))
seq(tensor(uint16))
seq(tensor(uint32))
seq(tensor(uint64))
seq(tensor(uint8))
seq_lengths
Sequence
Sequence is missing type entry for its element
sequence_lens
sequence_lens length of 
'sequence_lens' must have rank of 1
sequence_lens shape must be {batch_size}. Got:
sequence_type
SequenceAt
SequenceAt: Got nullptr for output tensor
SequenceConstruct
SequenceConstruct is expected to have at least 1 input.
SequenceConstruct: Got nullptr for output sequence
SequenceEmpty
SequenceEmpty: Got nullptr for output sequence
SequenceErase
SequenceErase: Got nullptr for output sequence
SequenceInsert
SequenceInsert: Got nullptr for output sequence
SequenceLength
SequenceLength: Got nullptr for output tensor
SequenceLengthsTensor
Sequences must have tensors of the same data type. There was at least one tensor in the input that was different.
Sequential execution should be enabled when using DML execution provider.
Sequential mode
SequentialExecutor::Execute
Serialization error. Graph attribute was serialized without Graph instance
Serialization of fused function body is not currently supported, 
Serialized version info is null. Invalid ORT format model.
Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED. The generated model may contain hardware and execution provider specific optimizations, and should only be used in the same environment the model was optimized for.
Session
Session Config with key [
Session has already been initialized.
Session not initialized.
Session successfully initialized.
Session was not initialized
session.disable_prepacking
session.enable_quant_qdq
session.load_model_format
session.save_model_format
session.set_denormal_as_zero
session.use_env_allocators
session_env.EnvCreatedWithGlobalThreadPools()
session_initialization
session_logger != nullptr
session_options
session_options_.session_log_severity_level >= 0 && session_options_.session_log_severity_level <= static_cast<int>(logging::Severity::kFATAL)
session_state
session_state_ != nullptr
SessionCreation
SessionCreationStart
sessionId
SessionOptionsAppendExecutionProvider_OpenVINO: Failed to load shared library
SessionOptionsAppendExecutionProvider_Tensorrt: Failed to load shared library
SessionState for subgraphs is null. Invalid ORT format model.
SessionState is null. Invalid ORT format model.
SessionState should have saved the KernelCreateInfo prior to this running. NodeIndex:
SetEvent
SetFilePointerEx
SetFilePointerEx 
SetGraphAndCreateKernels must be called prior to GetExecutionInfo.
SetLastError
SetRestrictedErrorInfo
SetThreadAffinityMask
SetThreadDescription
setting data_type field (tensor name: 
Setting enable_profiling to 
Setting execution_mode to 
Setting graph_optimization_level to ORT_DISABLE_ALL
Setting graph_optimization_level to ORT_ENABLE_ALL
Setting graph_optimization_level to ORT_ENABLE_BASIC
Setting graph_optimization_level to ORT_ENABLE_EXTENDED
Setting inter_op_num_threads to 
Setting intra_op_num_threads to 
SetUnhandledExceptionFilter
SetupSubgraphExecutionInfo should only be called once for each subgraph.
SGWWShL
SGWWSQQh
Sh,1>
Sh<}<
Sh06<
shape
Shape
shape && tensor.Shape() == *shape
shape as a contiguous subset of the first tensor's shape. The starting of the
'shape' input must be 1D tensor of type INT64
Shape input must be a one-dimensional tensor.
shape is invalid
Shape mismatch attempting to re-use buffer. 
Shape must be 1 dimensional as it's tensor data of a shape
shape of layer norm bias tensor not expected
shape of left-hand-side argument. When broadcasting is specified, the second
shape.Size() must >=0
shape_.Size() == new_shape.Size()
shape_data_tensor.Shape().GetDims().size() == 1
shape_size == out.length()
shapeTensor->Shape().NumDimensions() == 1
ShapeToInitializer
Sharada
Shavian
ShL}<
short
short 
Should be unreachable if CanRemoveNodeAndMergeEdges is in sync with the logic here.
should never happen
Should not have entry in kernel create info with nullptr for kernel_def
Shouldn't be possible to have NodeArgs that haven't been handled already.
Shrink
ShtIA
ShTy=
shWSRV
ShX`@
ShXB?
Siddham
sigmoid
Sigmoid
signal_ndim
signed 
SignWriting
SimplifiedLayerNormFusion
Simplify case not handled: 
SimplifyWalker::ShortVisit called
Single dimension value must be greater than 0
single_node_compute_func should have 1 elements
Sinhala
siWSRV
size != 0 && (input_shape.Size() % size) == 0
size is different
Size mismatch for kernel create info node indexes and hashes. Invalid ORT format model.
Size mismatch validating subgraph inputs. Got 
Size mismatch: feed_names has 
size overflow
size_ % 2 == 1
size_ == size
size_ > 0
size_t(impl_->max_gram_length_) <= impl_->ngram_counts_.size()
size_t(impl_->min_gram_length_) <= impl_->ngram_counts_.size()
sizeof(uint32_t) == output_element_bytes
sizes
Sizes
sizes != nullptr && sizes->Shape().Size() != 0
sizes == nullptr
SjpZjoY
skip is expected to have same shape as input
SkipLayerNormalization
SkipLayerNormFusion
Sleep
SleepConditionVariableCS
SleepConditionVariableSRW
Slice
Slice does not have enough number of inputs
Slice ends is less than INT_MAX
Slice op must have either three, four or five inputs.
Slice parameter is not expected. Input index:
slice the input `data` tensor. If a negative value is passed for any of the
Sliced data tensor.
slices of `data` into an output tensor of rank q - 1 + r - indices[-1].
Slices uses `axes`, `starts` and `ends` inputs to specify the start and end
SLjt_
slope
SlopeTensor
snprintf() failed with return value: 
snprintf_result > 0
snprintf_result > 0 && gsl::narrow_cast<size_t>(snprintf_result) == buffer_span.size() - 1
So disabling it for this session since it uses the DML Execution Provider.
So making the execution mode sequential for this session since it uses the CUDA Execution Provider.
So making the execution mode sequential for this session since it uses the DML Execution Provider.
Softmax
SOFTMAX
Softmax attribute axis is expected to be 3
softmax_axis
SOFTMAX_ZERO
SoftmaxCPU inputs N, D and N * D must be < 
SoftmaxCrossEntropyLoss
Softplus
softplus
softsign
Softsign
Sogdian
Some nodes are not included in the topological sort, graph have a cycle.
Sora_Sompeng
sorted
source and destination buffer size mismatch
source sequence type missing element type.
Soyombo
SpaceToDepth
SPARSE
Sparse Indicies raw data size does not match expected.
Sparse initializer must have a name. This model is invalid
Sparse Initializer tensor is missing. Invalid ORT format model.
Sparse tensor (
Sparse tensor indices (
Sparse tensor initializers must have a non-empty name
Sparse tensor values (
sparse_tensor
SPARSE_TENSOR
sparse_tensor(
sparse_tensor_names_ not in sync with name_to_initial_tensor_
sparse_tensor_names_.count(tensor_name) == 0
sparse_tensor_proto
sparse_tensor_type
SPARSE_TENSORS
sparse_value
SparseTensor element type mismatch. 
Spatial
spatial
spatial_scale
spatial_scale_ > 0
SpatialScale
SpatialScaleX
SpatialScaleY
Specified axis to insert a dimension
Specified device is not supported.
Specified domain and type names combination does not refer to a registered opaque type
Specifies a target value that is ignored and does not contribute to the input gradient. It's an optional value.
Specify batchs of sequence words to embedding
Specify bias of conv
Specify embedding vector of char
Specify if the RNN is forward, reverse, or bidirectional. Must be one of forward (default), reverse, or bidirectional.
Specify weights of conv
SPhxI?
split
Split
Split operator does not support 
Split should be > 0
split_scalar > 0
split_size_sum (
split_tensor->Shape().NumDimensions() == 1
SplitToSequence
SplitToSequence operator does not support 
SPPPh
SPPPh,
SPPPQQh
SPPPRQh
SPPSh
SPPSh,
SPPSQh
SPPSQh`K?
SPPSQhhM?
SPPSQQh
SPPSQRh
SPPSRh
SPPSRh(A?
SPPSRQh
SPPSWQh
SPQhXs<
SPSSPQQhH)?
SPSSPWQh
sqeuclidean
SQh|#?
SQQh0
SQQh8
SQQQPQh
SQQQRQh
SQQQWQh|&?
SQQShL
SQQSPhhk?
SQQSPQh
SQQSRh
SQQSRh0j?
SQQSRQh
SQQSVQh
SQSSQWQhtt?
SQSSSh
Squeeze
squeeze_mask
squeezed
'SQWQ
src.SizeInBytes() == dst.SizeInBytes()
SRh`\?
SRPSQ
SRQh$
SRRRPQ
SRRRPQh
SRRRPQhd
SRRRQQh
SRRRSQQh
SRRRVQhp
SRRRVQhPs<
SRRSh,
SRRSPQh
SRRSPQhL
SRRSQQh
SRRSWQh
SRSSRh
SRSSRQQh
SRSSRQQh|
SRSSRQQh|#?
SSPPP
SSPPPQh
SSPPPQQh
SSPPPQQh<
SSPPPQRh
SSPPPRQh
SSPPSh
SSPPSh<
SSPPSQh
SSPPSQh,{?
SSPPSQh8B?
SSPPSQh8V?
SSPPSQhHt?
SSPPSQQh
SSPPSQRh
SSPPSRh
SSPPSWQh<
SSPPSWQhtt?
SSPVQh|#?
SSPWQh|#?
SSQQ3
SSQQQh,
SSQQQPh
SSQQQPQh
SSQQQRQh<
SSQQQWQh|&?
SSQQSPh
SSQQSPh$E?
SSQQSPh8B?
SSQQSPhxw?
SSQQSPhXw?
SSQQSRh
SSQQSRQh
SSQQSVQh
SSRR3
SSRRj
SSRRRPQh
SSRRRQQhh&@
SSRRSh 
SSRRShL
SSRRSPh
SSRRSPQh
SSRRSPQhl
SSRRSQQh
SSRRSQQh<
SSRRSWQh
SSSh 
SSSh,
SSSPVQh|#?
SSSQQh
SSSRQh
SSSRQQh
SSSS3
SSSSh 
SSSSj
SSSSRQh
SSSSRQQh|#?
SSSSWQQh
SSSSWQQh|#?
SSSSWRQh
SSSSWRQhtt?
SSSWQh
SSSWQQh
SSSWRQh
SSSWRQh$
sstd::exception: %hs
SSVh 
SSVVSPQh
SSWQQh
SSWQQh$
SSWRQh
SSWW3
SSWWj
SSWWSQQh
SSWWSRQh
st.IsOK()
Stack not empty.
Stacktrace:
start
Start CheckNodesInPathK
Start CheckNodesInPathQ
Start CheckNodesInPathV
Start FuseGptAttention
start in Range operator should be scalar like tensor, yet got shape:
Start MatchGemmSubgraph
Start MatchInputMaskSubgraph
Start MatchInputMaskSubgraphDistilBert
Start MatchPastSubgraph
Start MatchUnidirMaskSubgraph
start or end indices, it represent number of elements before the end of that
Start ValidateGemmInitializer
start_offset % span_size == 0 && real_end % span_size == 0
start_offset >= 0 && real_end >= 0 && start_offset <= real_end && real_end <= len
Starting indices of corresponding axis in `axes`
StartPadding
startpos: 
starts
Starts and axes shape mismatch
Starts and ends shape mismatch
Starts and steps shape mismatch
Starts must be a 1-D array
starts.size()=
starts_.empty() || start > ends_.back()
starts_.size() == ends_.size()
starts_.size() == ends_.size() + 1
stash_type
state (NxD), and the sequence lengths (N), computes the GRU
state not recoverable
StateSaver failed to restore state.
static 
static_activation_memory_sizes_in_byte_.find(location.name) == static_activation_memory_sizes_in_byte_.end()
static_cast<int>(activation_func_names.size()) == num_directions_ * 3
static_cast<size_t>(num_subgraph_inputs) == subgraph_inputs.size()
static_cast<uint64_t>(num_kv_pairs) < std::numeric_limits<size_t>::max()
status.IsOK()
status.IsOK() && !impl_->ngram_counts_.empty()
status.IsOK() && !impl_->ngram_indexes_.empty()
status.IsOK() && !input_dimensions_.empty()
status.IsOK() && !pool_int64s.empty()
std::all_of(impl_->ngram_indexes_.cbegin(), impl_->ngram_indexes_.cend(), [](int64_t i) { return i >= 0; })
std::all_of(output_edges.cbegin(), output_edges.cend(), [&src_idx](const GraphEdge& edge) { return edge.src_arg_index == src_idx; })
std::all_of(split_sizes.cbegin(), split_sizes.cend(), [](int64_t value) { return value >= 0; })
std::all_of(split_sizes_.cbegin(), split_sizes_.cend(), [](int64_t value) { return value >= 0; })
std::count_if(subgraph_node.InputEdgesBegin(), subgraph_node.InputEdgesEnd(), [input_slot_index](const Node::EdgeEnd& entry) { return entry.GetDstArgIndex() == input_slot_index; }) == 0
std::nullptr_t
std::nullptr_t 
Steepness
'step' cannot be 0
'step' value cannot be 0
steps
steps.size()=
stod argument out of range
stof argument out of range
stoll argument out of range
Stopword contains invalid utf8 chars
stopwords
storage_order
stoull argument out of range
strcspn
stream timeout
Stride along each axis.
Stride along each spatial axis.
Stride along each spatial axis. If not present, the stride defaults is 1 along each spatial axis.
Stride along each spatial axis. If not present, the stride defaults to 1 along each axis.
Stride along each spatial axis. If not present, the stride defaults to 1 along each spatial axis.
strides
Strides
strides.size() == kernel_shape.size()
strides_.size() == kernel_shape_.size()
STRING
string
string buffer allocation failed
STRING data (tensor name: 
string enum that cases output to be lowercased/uppercases/unchanged. Valid values are "LOWER", "UPPER", "NONE". Default is "NONE"
string literal
string tensor can not have raw data
string tensor is not supported for copying between allocators
string too long
string_data
string_vocabulary
StringFileInfo
StringNormalizer
STRINGS
Strings to tokenize
strncmp
struct 
sub_result
sub_result_casted
subgraph
Subgraph
Subgraph in 'body' produces 
Subgraph input missing type.
Subgraph must have the shape set for all outputs but 
Subgraph SessionState entry for 
Subgraph SessionState for 
Subgraph SessionState was not found for '
Subgraph SessionState was not found for 'body' attribute.
SUCCESS
suffix matching is assumed. 1-dim expansion doesn't work yet.
Sum of split values not equal to 'input' dim size on 'axis'. 'axis' dim size=
Sundanese
Support vector coefficients.
support_vectors
Supported modes: `constant`(default), `reflect`, `edge`
SVjoY
SVMClassifier
SVMRegressor
SVSSVRQh
SVW;B
SVW9A
SVWj$
SVWj@3
SVWj0
SVWj4^
SVWQQ
SVWRQ
SVWUj
SWQQWRQh
SWSSWQQh
SWSSWQQh|#?
SWSSWRQh
SWSSWRQh 
SWSSWRQh8
SWSSWRQhtt?
SWWWSRQQj
SxWVj
Syloti_Nagri
syntax error 
Syriac
system
system error number 
SystemError
t hPs<
t j%X
t!;3r
t!;7r
t!jph@
t!jZh`
t";7r
t#j.X
t#j+X
t$ ;C
t$ ;r
t$ ;s
T$ ;U
T$ +\$
T$ +\$$
t$ PQQ
t$ WV
T$$;\$H|
T$$;OL
T$$9T$0|;
t$$jpY
t$$PQ
t$$RWQSP
t$$SWV
t$(;q
t$(;s
t$(+|$8
t$(jpY
t$(Vj
t$(YY
t$,;C
t$,;t$\
T$,;U
t$,+|$<Q
T$,A;
t$,PV
t$;;r
t$;>r
t$;7r
t$@;\$
T$@;S
t$@;t$\
T$@;U
T$@9s 
t$@QQ
T$@R3
T$@RQ
t$\;u
T$`9}$
T$<;O
t$<;t$$
T$<9s 
T$<QjoY
t$<WP
T$0;t$
t$0+|$<
t$0+|$8
t$09L$(t
T$4;\$P|
T$4;N
T$4;O
T$4;U
t$4+|$@P
t$4+|$<P
T$49U
T$8;t$
t$8WQP
t$D;t$`
T$D;U
T$D9U
T$DA3
t$DjoZRY
t$DPQ
t$DSQP
t$DVW
T$H;S
T$H;T$
t$HVW
T$hVW
t$HWV
t$j#Y
T$L;OL
t$l;u
t$LRP
t$LRQ
t$PjoY
t$PjoZ
t$PQP
t$pRP
t$PRQ
t$pVQ
T$T;U
t$tQP
t$tQR
t$TRP
t$XjoZRY
t$XQP
t$XRP
t$XWQP
t%;A0t 
t%j}h
t&99t
t(jJh
t(PQVQ
t*;7r
t*h`9I
t*j"h
t*j"hl^=
t,j)h
t,j.h
t,jEh
t,jMh
t,j-Y
t,jYh
t.j h
t.j#h,c>
t.j(h
t.j?h`
t.j7h
t.j8h
t.j'h
t.jJh
t.jJh<J>
t.jJhHI>
t.jNh
t/9N,}
t/j!h
t/j!hLi>
t/j"h
t/j"hl^=
t/j%hP
t/j&h(
t/j(hdl>
t/j,h
t/j_hl
t/j|h
t/j2h
t/jBh
t/jdh
t/jEh
t/jihL
t/jIhP
t/jJh
t/jQh
t/jQhhP>
t:j?h
t:jQh
t:jsh
t;hD8@
t?jzh
t@33{@
t@j2h
t^=333
t_=UUU
t_proto.dims()[0] == 1
t_proto.dims_size() == 1
t`hh4=
t~<A|0<P
t+j]h
t+j+h
t+jDh$
t+joh@
t+WQSQ
t=jQ3
t>jbh
T0`0s0
t0j6h
t0jmh
t0jwh
T0l0v0
T1 != nullptr
t1j4h
t1jVh
t2;;r
t2;3r
t2;7r
t2=DDD
T3`4x4
t4;;r
t4;7r
t4<A|)<P
t4j$h
t4j%h
t5;;r
t5jIh
t5QPQ
t6;;r
t6j(X
t6jVh
t6VW+
t7j9h
t7VV9u
t8 9\8$|
t89Xx
t8SVW
t98B t4
t9jEh
t9jnh
Tagalog
Tagbanwa
Tai_Le
Tai_Tham
Tai_Viet
tAj%XP
Takri
Tamil
Tangut
target
Target rank must be 1 less than the input rank.
target sequence type missing element type.
Target shape may not have multiple -1 dimensions
target_class_ids.size() == target_class_nodeids.size()
target_class_ids.size() == target_class_treeids.size()
target_ids
target_nodeids
target_treeids
target_weights
targets
tBVW+
tEj2h
tEjdh
Telugu
template-parameter-
TempSpace allocator not found
TENSOR
tensor
Tensor after padding.
tensor can either be of element size 1 (including a scalar tensor and any
tensor can't contain negative dims
Tensor does not have external data to read from.
Tensor element type mismatch. 
tensor failed memory size calculation
Tensor initializers must have a non-empty name
Tensor is expected to contain one of the primitive data types. Got: 
Tensor must always contain primitive types. Found: 
tensor of bool, which should be a scalar.
Tensor of data to extract slices from.
tensor of int64, which should be a scalar.
Tensor of integers indicating the number of padding elements to add or remove (if negative) at the beginning and end of each axis. For 2D input tensor, it is the number of pixels. `pads` should be a 1D tensor of shape [2 * input_rank] or a 2D tensor of shape [1, 2 * input_rank]. `pads` format (1D example) should be as follow [x1_begin, x2_begin,...,x1_end, x2_end,...], where xi_begin is the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
Tensor of rank q >= 1.
Tensor of rank q-1+r-indices[-1].
Tensor of rank r >= 1.
Tensor proto with external data for value attribute is not supported.
Tensor sequence must contain only primitive types
Tensor shape cannot contain any negative value
Tensor shape is too large
Tensor size (
Tensor size mismatch
tensor size overflow
tensor type 
Tensor type mismatch.
Tensor type mismatch. 
Tensor types should have been handled already
tensor with rank equal to or smaller than the first tensor), or having its
Tensor with shape information must be 1 dimensional.
tensor(
tensor(bfloat16)
tensor(bool)
tensor(complex128)
tensor(complex64)
tensor(complext128)
tensor(complext64)
tensor(double)
tensor(float)
tensor(float16)
tensor(int16)
tensor(int32)
tensor(int64)
tensor(int8)
tensor(string)
tensor(string) expected as input
tensor(uint16)
tensor(uint32)
tensor(uint64)
tensor(uint8)
tensor_a_zero_point == nullptr || IsScalarOr1ElementVector(tensor_a_zero_point)
tensor_b_zero_point == nullptr || IsScalarOr1ElementVector(tensor_b_zero_point)
tensor_c_zero_point == nullptr || IsScalarOr1ElementVector(tensor_c_zero_point)
tensor_type
tensor_x_zero_point == nullptr || IsScalarOr1ElementVector(tensor_x_zero_point)
tensor_y_zero_point == nullptr || IsScalarOr1ElementVector(tensor_y_zero_point)
TensorProto ( tensor name: 
TensorProto (tensor name: 
TensorProto external data size mismatch. 
TensorProto external data size mismatch. Computed size: 
TensorrtExecutionProvider
TENSORS
ter!u
TerminateProcess
TerminateThread
text file busy
text size: 
tF;;r
tf_crop_and_resize
tf_half_pixel_for_nn
tFh(n:
tFhDr=
TFIDF
TfIdfVectorizer
tGj=h
t'h(#@
tH;;r
tH;>r
t'h8E@
Thaana
than the operator set version 
The Alpha value in Celu formula which control the shape of the unit. The default value is 1.0.
The axis along which same quantization parameters are applied. It's optional.If it's not specified, it means per-tensor quantization and input 'x_scale' and 'x_zero_point' must be scalars.If it's specified, it means per 'axis' quantization and input 'x_scale' and 'x_zero_point' must be 1-D tensors.
The axis in which to compute the arg indices.
The axis in which to compute the arg indices. Accepted range is [-r, r-1] where r = rank(data).
The axis on which to apply normalization, -1 mean last axis.
The bias (or mask) as Tensor.
The bias input data that is a 1D tensor.
The bias value added to output. Default is 0.
The broadcasted dimensions of the inputs are incompatible
The buffer planner is not consistent with tensor buffer size, expected 
The coefficient 'a' used in cubic interpolation. Two common choice are -0.5 (in some cases of TensorFlow) and -0.75 (in PyTorch). Check out Equation (4) in https://ieeexplore.ieee.org/document/1163711 for the details. This attribute is valid only if "mode" is "cubic".
The coordinate of each dimension is transformed individually. Let's describe a case using axis x as an example.
The data type for the elements of the output tensor. Default is TensorProto::FLOAT.
The data type for the elements of the output tensor. If not specified, default is TensorProto::FLOAT.
The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto
The dimension with value zero exceeds the dimension size of the input tensor.
The distance metric to use. If a string, the distance function can be "braycurtis", "canberra", "chebyshev", "cityblock", "correlation", "cosine", "dice", "euclidean", "hamming", "jaccard", "jensenshannon", "kulsinski", "mahalanobis", "matching", "minkowski", "rogerstanimoto", "russellrao", "seuclidean", "sokalmichener", "sokalsneath", "sqeuclidean", "wminkowski", "yule".
The environment variable contained the value: 
The epsilon value to use to avoid division by zero, default is 1e-5f.
The epsilon value to use to avoid division by zero.
the expected transformation of a stochastic regularizer which randomly applies
The exponent.
The filled tensor
The first input of CDist kernel has wrong shape: 
The first input of Range should be a constant with value 0.
The first normalization dimension: normalization will be performed along dimensions axis : rank(inputs).
The fused convolution operator schema is the same as Conv besides it includes an attribute
The FusedGemm operator schema is the same as Gemm besides it includes attributes
The given version [%u] is not supported, only version 1 to %u is supported in this build.
The graph run each iteration. It has 2+N inputs: (iteration_num, condition, loop carried dependencies...). It has 1+N+K outputs: (condition, loop carried dependencies..., scan_outputs...). Each scan_output is created by concatenating the value of the specified output value at the end of each iteration of the loop. It is an error if the dimensions or data type of these scan_outputs change across loop iterations.
The graph run each iteration. It has N+M inputs: (loop state variables..., scan_input_elts...). It has N+K outputs: (loop state variables..., scan_output_elts...). Each scan_output is created by concatenating the value of the specified scan_output_elt value at the end of each iteration of the loop. It is an error if the dimensions of these values change across loop iterations.
The id of the tree that each node is in.
The id of the tree that this node is in.
the identity or zero map to a neuron's input. The GELU nonlinearity weights
The index of the class list that each weight is for.
The index of the target that each weight is for
The inner-most 2 dimensions must have the same size (mat_w:
The innermost dims should have the same dim value to parse the diagonal elements
The input data as Tensor.
The input is not evenly splittable
The input must be a map from strings or integers to either strings or a numeric type. The key and value types cannot be the same.
The input must be a tensor of a numeric type or string. The output will be of the same tensor type.
The input must be a tensor of a numeric type, and of of shape [N,C] or [C]. In the latter case, it will be treated as [1,C]
The input must be a tensor of a numeric type, either [C] or [N,C].
The input must be a tensor of a numeric type.
The input must be a tensor of a numeric type. The output will be of the same tensor type.
The input must be a tensor of strings or integers, either [N,C] or [C].
The input must be an integer map to either string or float.
The input shape override's size does not match the input tensor's shape size
The input tensor cannot be reshaped to the requested shape. Input shape:
The input type is a tensor of any shape.
The input type must be a tensor of a numeric type, either [C] or [N,C].
The input type must be a tensor of a numeric type, either [N,C] or [C]. The output type will be of the same tensor type and shape.
The input type must be a tensor of a numeric type.
The input type must be a tensor of integers or strings, of any shape.
The integers of the map. This sequence must be the same length as the 'cats_strings' sequence.
The kernel type, one of 'LINEAR,' 'POLY,' 'RBF,' 'SIGMOID'.
The keys when using int keys.<br>One and only one of the 'classlabels_*' attributes must be defined.
The keys when using string keys.<br>One and only one of the 'classlabels_*' attributes must be defined.
The lambd value for the Shrink formulation. Default is 0.5.
The mean of the normal distribution.
The model contains a 16-bit input (
The model has input '
The Model Proto has already been checked for the ORT config json.
The Model Proto hasn't been checked for the ORT config json.
The most inner dimension in boxes must have 4 data.
The new GRU hidden state calculated by this op.
The node id of each weight
The node is not placed on any Execution Provider. 
The node kind, that is, the comparison to make at the node. There is no comparison to make at a leaf node.<br>One of 'BRANCH_LEQ', 'BRANCH_LT', 'BRANCH_GTE', 'BRANCH_GT', 'BRANCH_EQ', 'BRANCH_NEQ', 'LEAF'
The normal input data.
The number of batch dimensions. The gather of indexing starts from dimension of data[batch_dims:]
The number of channels to sum over
The number of support vectors.
The only subscript labels allowed are lower-cased letters (a-z) and upper-cased letters (A-Z)
The only supported values for the environment variable 
The order of the normalization, only 1 or 2 are supported.
The ORT format model version [
The output is a 1-D tensor of string, float, or integer.
The output is a tensor of strings or integers. Its shape will be the same as the input shape.
The output type will be a tensor of strings or integers, and will have the same shape as the input.
The output type will be a tensor of strings or integers, depending on which of the the classlabels_* attributes is used.
The output type will be a tensor of strings or integers, depending on which of the the classlabels_* attributes is used. Its size will match the bactch size of the input.
The output will be a sequence of string or integer maps to float.
The output will be a tensor of strings or integers.
The output will be a tensor of the value type of the input map. It's shape will be [1,C], where C is the length of the input dictionary.
The output.
The override dims are not compatible with given tensor's shape. 
The pads attribute cannot be used simultaneously with auto_pad attribute
The parent of shape nodes are expected to be input_ids.
The parent of two shape nodes are expected to be input_ids.
The pooling method. Two modes are supported: 'avg' and 'max'. Default is 'avg'.
The pooling method. Two modes are supported: 'bilinear' and 'nearest'. Default is 'bilinear'.
The previous GRU hidden state.
The rank of input tensor must be >= axis
The rank of the input must match permutation size for Transpose
The ratio of random dropout
the same ordering as the image pixel format.
The scale along height dimension. It takes value greater than or equal to 1.
The scale along width dimension. It takes value greater than or equal to 1.
The scale array along each dimension. It takes value greater than or equal to 1. The number of elements of 'scales' should be the same as the rank of input 'X'.
The scale to apply.
The scaled hyperbolic tangent values of the input tensor computed element-wise
The second input of CDist kernel has wrong shape: 
The sequence output for the hidden is optional if 0. Default 0.
The shape of filled tensor
The shape of the convolution kernel. If not present, should be inferred from input W.
The shape of the convolution kernel. If not present, should be inferred from input 'w'.
The shape of the output can be explicitly set which will cause pads values to be auto generated. If output_shape is specified pads values are ignored. See doc for details for equations to generate pads
The shape of the output tensor.
The size of each input in the input list
The size of the kernel along each axis.
The standard deviation of the normal distribution.
The starting indexes of 1-grams, 2-grams, and so on in pool. It is useful when determining the boundary between two consecutive collections of n-grams. For example, if ngram_counts is [0, 17, 36], the first index (zero-based) of 1-gram/2-gram/3-gram in pool are 0/17/36. This format is essentially identical to CSR (or CSC) sparse matrix format, and we choose to use this due to its popularity.
The storage order of the tensor. 0 is row major, and 1 is column major.
The string used to pad output tensors when the tokens extracted doesn't match the maximum number of tokens found. If start/end markers are needed, padding will appear outside the markers.
The strings of the map. This sequence must be the same length as the 'cats_int64s' sequence
The subgraph in 'body' requires 
the tensor elementwise.
the tensor to be tiled using Tile OP must be atleast 1 dimensional
The third input of Range should be a constant with value 1.
The timestep for this operation.
The total number of regression targets, 1 if not defined.
The total number of targets.
The type of the output tensor is integer.
The underlying implementation is MurmurHash3_x86_32 generating low latency 32bits hash suitable for implementing lookup tables, Bloom filters, count min sketch or feature hashing.
The value for the elements of the output tensor in sparse format.
The value for the elements of the output tensor.
The value for the sole element for the scalar, float32, output tensor.
The value for the sole element for the scalar, int64, output tensor.
The value for the sole element for the scalar, UTF-8 string, output tensor.
The values for the elements for the 1D, float32, output tensor.
The values for the elements for the 1D, int64, output tensor.
The values for the elements for the 1D, UTF-8 string, output tensor.
The weight for each target
The weight for the class in class_id.
The weighting criteria. It can be one of "TF" (term frequency), "IDF" (inverse document frequency), and "TFIDF" (the combination of TF and IDF)
The WordConvEmbedding takes in a batch of sequence words and embed each word to a vector.
The zero-padding added to one side of the output. This is also called adjs/adjustment in some frameworks.
then optionally start the crop offset by the left/top border amounts.
then_branch
then_branch and else_branch produce different number of outputs. 
then_feeds_fetches_manager_ && else_feeds_fetches_manager_
there are multiple cases for the number of outputs, which we list below:
There must be one (and only one) dynamic typed input to the custom op. Its type info at runtime will be used to infer the type info of this dynamic typed output which is required for the success of the model loading step. More than one dynamic typed inputs are currently not supported as differing types at runtime means the output type cannot be inferred without which model loading cannot proceed.
There was a problem acquiring temporary memory allocator in Einsum op
There's no data transfer registered for copying tensors from 
this API does not support strings
This attribute describes how to transform the coordinate in the resized tensor to the coordinate in the original tensor. <br/>
This is an invalid model. At top level graph without matching NodeArg that subgraph consumes. Name=
This is an invalid model. Error in Node:
This is an invalid model. Error: Duplicate definition of name (
This is an invalid model. Error: the graph is not acyclic.
This is an invalid model. Error: two nodes with same node name (
This is an invalid model. Failed to find NodeArg in all parent graphs. Name=
This is an invalid model. Graph output (
This is an invalid model. Model input (
This is an invalid model. Node (
This is an invalid model. Tensor does not have type information.
This is an invalid model. The sum of input arg count is not equal to size of input defs in node (
This is an invalid model. Type Error: Type '
This may prevent some of the graph optimizations, like const folding. 
This operator applies convolution to word from left to right with window equal to conv_window_size and stride to 1.Take word 'example' for example, with conv_window_size equal to 2, conv is applied to [ex],[xa], [am], [mp]...If not provide, use the first dimension of conv kernal shape.
This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.
This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).
This optimizer does not support external data for unidirectional mask right now
This session already contains a loaded model.
This session has already been initialized.
This session will use the allocator registered with the environment.
This transformer is already registered 
This ZeroCopyOutputStream doesn't support aliasing. Reaching here usually means a ZeroCopyOutputStream implementation bug.
this->base_values_.size() == predictions.size()
thisProto->value_case() == TypeProto::ValueCase::kMapType
thisProto->value_case() == TypeProto::ValueCase::kSequenceType
thisProto->value_case() == TypeProto::ValueCase::kSparseTensorType
thisProto->value_case() == TypeProto::ValueCase::kTensorType
Three interpolation modes: nearest (default), linear and cubic. The "linear" mode includes linear interpolation for 1D tensor and N-linear interpolation for N-D tensor (for example, bilinear interpolation for 2D tensor). The "cubic" mode includes cubic interpolation for 1D tensor and N-cubic interpolation for N-D tensor (for example, bicubic interpolation for 2D tensor).
Three modes: `constant`(default) - pads with a given constant value, `reflect` - pads with the reflection of the vector mirrored on the first and last values of the vector along each axis, `edge` - pads with the edge values of array
Three modes: constant(default), reflect, edge
threshold
Threshold
Threshold value
ThresholdedRelu
thresholdedrelu
Thresholds to do the splitting on for each node.
Tibetan
Tifinagh
tIjEh
Tile doesn't have an implementation yet for the type: 
Tile doesn't support string type yet
tiles
time_axis
time_axis < 2
time_axis and batch_axis must have different values but both are 
timed out
Tirhuta
tJ<_t<<$t8<<t4<>t0<-t,<a|
tJ=k(
t'jDh
t'jFh
tJj2h
tJjtSV
t'jOh
tKj*h
tMhDr=
tmp_cats_int64s.empty() || tmp_cats_strings.empty()
to flatten the input shape to (N x C*D1*D2 ..*Dn) before a BatchNormalization Op.
TO_FLOAT
TO_INT64
TO_STRING
tokenexp
Tokenized strings
Tokenizer
too many files open
too many files open in system
too many links
too many symbolic link levels
Total allocated bytes: 
Total fused Attention node count: 
Total fused reshape node count: 
Total Gelu Approximation (FastGelu) node count: 
totalRunDuration
totalRuns
tPVVVWS
tqhh4=
tR<0|
TraceAllocation for ort_value_idx=
TraceFree for ort_value_idx=
trailing \
training_mode
training_mode of Dropout must be a scalar.
TrainingStepTensor
TransA
transA
TransB
transB
transform_targets
Translation
Transpose
Transpose not implemented for empty tensors.
Transpose of element size not supported in this build. Size=
transposed
TransposeMatMul
Tree id for each node.
TreeEnsembleClassifier
TreeEnsembleRegressor
tried creating tensor with negative value in shape
tried to allocate 0 bytes
Tried to allocate without valid type information, ort_value index=
Trilu
true literal
TryAcquireSRWLockExclusive
Trying to allocate memory for unused optional inputs/outputs
Trying to get a SparseTensor, but got: 
Trying to get a Tensor, but got: 
Trying to get a TensorSeq, but got: 
Trying to register schema with name 
tv;;s5
tVjJh
tVShp
tVWVRS
Two interpolation modes: nearest (default), and linear (including bilinear, trilinear, etc)
Two interpolation modes: nearest(default), bilinear
two paths share the same shape
tX;7s)
tY=333
type 
type != nullptr
type == dtype_
type case mismatch. existing=
type case unsupported. existing=
Type Error: Data in initializer '
Type Error: Shape of initializer 
Type Error: Type (
Type Error: Type parameter (
type field and data field mismatch in attribute 
Type mismatch. Current=
type must be number, but is 
Type of reduction to apply to loss: none, sum, mean (default). 'none': the output is the loss for each sample. 'sum': the output will be summed. 'mean': the sum of the output will be divided by the sum of applied weights.
Type of reduction to apply to loss: none, sum, mean(default). 'none': no reduction will be applied, 'sum': the output will be summed. 'mean': the sum of the output will be divided by the number of elements in the output.
type used for stash mean/inv_std_var
Type:
type_error
type_id_counter == 1
type_proto is not of type map!
type_proto is not of type sequence!
TypeAndShapeInferenceFunction implementation incomplete: this line should never be reached.
u Vjd
u VWS
u WSV
u#WSV
u$VRQ
u$WPQ
u%8\$
u%PPW
u(VVWSV
u)8O<t
u*9{$u%Q
u,9u0|F
u/h0V=
u:;T$
u';D$
u;F(r
u;j*hTz=
u;jCh
u?Qh<
u+j(j
u<+]8
U<+E8
u>h$9<
U0]0m0t0
u3j1h
u5h0V=
u8j=h
uD9}@
Ugaritic
ui_^[
uint16
uint32
uint64
uint64_data
uint8
UjAX;
ujh\q>
uJhh4=
uLjih IA
uMhh4=
uMj[h
Unable to find compiled kernel hash for node '
Unable to find node 
Unable to get an allocator
Unable to serialize model as it contains compiled nodes. Please disable any execution providers which generate compiled nodes.
Unactivated gate outputs from forget, update, and output gates, pre-activation.
UNDEFINED
Undefined tensor type!
unexpected 
Unexpected CAST_TO value of 
Unexpected CBLAS_TRANSPOSE for TransA of 
Unexpected data type for Clip input of 
Unexpected data type for Clip 'min' input of 
Unexpected element size of 
unexpected error
unexpected failure
Unexpected input data type. Actual: (
Unexpected mode of 
Unexpected mode:
Unexpected NORMALIZE value of 
Unexpected op in Regexp::Equal: 
Unexpected opcode in IsMatch: 
Unexpected opcode in short circuit: 
Unexpected opcode: 
Unexpected special state in RunStateOnByte
Unexpected value for 'add_second_class' of 
Unhandled 
unhandled 
unhandled opcode: 
UnhandledExceptionFilter
unidir mask is not constant
unidir mask shape not expected
unidirectional
unimplemented activation: 
union 
Unique
uNj*h
UNKNOWN
unknown
Unknown aggregation function in TreeEnsemble.
Unknown AutoPadType String
Unknown Category and zeros = 0.
Unknown encoding 
unknown error
Unknown error during EndProfiling()
Unknown exception
Unknown exception in Load()
Unknown exception was caught by catch-all handler.
unknown kernel type
Unknown model file format version.
unknown round: 
Unknown tensor type of 
unknown token
unknown_dim == -1
Unloading DSO 
unordered_map/set too long
UnpackTensor: the pre-allocate size does not match the size in proto
UnpackTensor: the pre-allocated size does not match the raw data size, expected 
Unrecognized attribute: 
Unrecognized data_type (tensor name: 
Unrecognized type value case (value_info name: 
unsigned 
Unsqueeze
unsqueeze_after_gather axes value not expected
UnsqueezeElimination
UnsqueezeElimination cannot remove node 
UnsqueezeElimination_
Unsupported attribute value type of 
Unsupported AutoPad Type.
Unsupported data type of 
Unsupported data type: 
unsupported data type: 
Unsupported 'dtype' value: 
Unsupported execution_mode value in ORT config: 
Unsupported graph_optimization_level value in ORT config: 
Unsupported input data type of 
Unsupported input element type of 
Unsupported input type
Unsupported level
Unsupported level 
Unsupported non-raw-data data type!
Unsupported output datatype with size: 
Unsupported output type of 
Unsupported pooling size : 
Unsupported pooling size.
Unsupported sparse tensor data type of 
Unsupported tensor element type in the input: 
Unsupported tensor type of 
Unsupported type:
Unsupported type: 
Unsupported value attribute datatype: 
Unsupported value for enable_profiling option: 
Unsupported value for inter_op_num_threads: 
Unsupported value for intra_op_num_threads: 
Unsupported version '
Unsupported X type: 
Unsupported Y type: 
unused
uOj<3
uOPPV
updates
updates shape: 
updates tensor should have shape equal to indices.shape[:-1] + data.shape[indices.shape[-1]:]. 
UpdatesTensor
UpdateTypeShapeInference is not intended to be used with control flow nodes containing subgraphs
uPf9_
uPh\q>
upper
UPPER
Upper boundary of the output values.
Upsample
Upsample operator
Upsample: input shape needs to be at least a single dimension.
Upsample: input tensor's dimension does not match the scales.
Upsample: input/output value is nullptr
Upsample: input/output value's dimension mismatch
Upsample: unexpected mode
u'Qh 
UQPXY]Y[
uR9|$$uL
URPQQh
use_approximation
UseClipThreshold
usefp16
Using an input in multiple nodes on different devices is not supported currently. Input:
Using global/env threadpools since use_per_session_threads_ is false
Using user supplied initializer with name (
UTCReplace_AppSessionGuid
utils::HasDataType(t_proto)
utils::HasElemType(thisProto->sequence_type())
utils::HasElemType(thisProto->sparse_tensor_type())
utils::HasElemType(thisProto->tensor_type())
utils::HasKeyType(thisProto->map_type())
utils::HasName(sparse_tensor)
utils::IsPrimitiveDataType<T>(dtype_)
utQQ3
uzh\q>
v ;D$
v >= 0 && static_cast<uint64_t>(v) <= std::numeric_limits<size_t>::max()
v PjoY
v$;|$
V$_^[
V;8u"
v_final_and_scan_outputs
v_initial
v_reshape initializer value is not expected
v4VWRP
vAj/h
VALID
valid
ValidateUnidirMask returns false for mask_slice
Validating no unexpected access using an invalid node_index. Got:
value
Value
value at X[t][n] >= seqLengths[n].
Value of alpha
Value of alpha default to 0.2
Value of alpha.
Value of attribute 
Value of beta
Value of beta default to 0.5
Value of beta.
value of k must not be negative
Value tensor should be a 1D tensor of size 1 with the same type as that of the input tensor
value too large
Value type is not supported yet: 
Value used for extrapolation, when applicable. Default is 0.0f. 
Value(s) to change to
Value(s) to change to.
value_float
value_floats
value_info
value_int
value_ints
value_proto != nullptr
value_string
value_strings
value_tensor->DataType() == data_type && value_tensor->Shape().Size() == 1
value_type
value_type != nullptr
ValueDataType
ValueDelta
values
Values
Values greater than this are mapped to 1, others to 0.
values in 'axes' are beyond the bounds of the computed output shape
values is 
values of data_type '
values.size() == static_cast<size_t>(attr->floats_size())
values.size() == static_cast<size_t>(attr->ints_size())
values_floats
values_int64s
values_strings
ValueStart
ValuesTensor
VarFileInfo
variadic_output
Variance
VarianceTensor
Vd^[3
vector<bool> too long
vector<T> too long
vectors_per_class
vgj~hp
Vh ;>
Vh 1=
Vh(7=
Vh,V>
Vh\f=
vH+Fh
Vh<c=
Vh0o=
Vh0x>
Vh8>>
Vh8f=
Vhd@=
Vhh{>
VhHE>
VhHg=
VhL}>
VhL<>
Vhp*=
VhP;=
VhP3>
VhtX>
Vhty=
VhX(=
VhX)=
VhX.=
VhXT=
Violation of the requirment that all input tensors must have the same data type.
virtual 
VirtualAlloc
VirtualFree
VitisAIExecutionProvider
VIWEF
VjdRP
Vl[_^
void 
volatile
volatile 
VPRQQQVQ
VPSWj
VQh|#?
VQPQW
VQQQRQhPs<
VQQRh
VQQVRQh
VQVVQSQh
VQWQSQ
VRRRVSQh
VRRVPQh
VRRVSQhd
VS_VERSION_INFO
VSPQQ
VSSVhL
VSVVSPQh|#?
VSWQQ
VSWSh
VVQQQRQhp
VVRRRh
VVRRVQQh
VVRVj
VVSPQhl
VVVVV
VWjp+
w_^[]
W_scale
w_scale
W_zero_point
w_zero_point
W0o0y0
W8+W4
W9^Lt
WaitForSingleObject
WaitForSingleObjectEx
WakeAllConditionVariable
WakeConditionVariable
Walk NULL
Warang_Citi
WARNING
Warning: Checker does not support models with experimental ops: 
Warning: Shape inference does not support
Warning: Unsupported operator 
'was added but does not exist. 
wchar_t
wcsnlen
WCSSSQQh
WCSSSRQh8
WCSSWQQh
WCSSWRQh
WCSSWRQh$
Wd_^]
We do not expect duplicate registration of types for: 
We do not support type [
We don't expect custom allocators for non-tensor types, so a shape is mandatory here.
weight
weight and zero_point pair is expected to have same type.
Weight buffer for initializer '
weight must be a scalar or 1D tensor of size 1
Weight point must be constant
Weight rank must be 1.
weight zero point must be a scalar or 1D tensor of size 1.
Weight zero point must be zero
weight_gather
weight_gather_sum
weight_gather_temp
weight_gather_temp_1
weight_scale
weight_zero_point
weights
Weights of the intercepts, if used.
Weights of the model(s).
weights.quant_para_
WeightTensor
Wh(yL
Wh0"=
Wh06@
When computing the output of the hidden gate, apply the linear transformation before multiplying by the output of the reset gate.
When coordinate_transformation_mode is "tf_crop_and_resize" and x_original is outside the range [0, length_original - 1], this value is used as the corresponding output value. Default is 0.0f.
When the session is not configured to use per session threadpools, the env must be created with the the CreateEnvWithGlobalThreadPools API.
Where
where const not matched.
Whether A should be transposed
Whether A should be transposed on the last two dimensions before doing multiplication
Whether B should be transposed
Whether B should be transposed on the last two dimensions before doing multiplication
Whether C should be broadcasted
Whether every token can only attend to previous tokens. Default value is 0.
Whether include pad pixels when calculating values for the edges. Default is 0, doesn't count include pad.
Whether the operator should behave like fmod (default=0 meaning it will do integer mods); Set this to 1 to force fmod treatment
Whether to return the elements in sorted order.
Whether to return the top-K largest or smallest elements.
Whether to select the last index or the first index if the {name} appears in multiple indices, default is False (first index).
Whether to use ceil or floor (default) to compute the output shape.
Whh=@
Which axis to concat on
Which axis to concat on.  Default value is 1.
Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs)..
Which axis to concat on. Accepted range in `[-r, r - 1]`, where `r` is the rank of input tensors. When `new_axis` is 1, accepted range is `[-r - 1, r]`. 
Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1]
Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
Which axis to scatter on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1]
Which axis to scatter on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
Which axis to split on
Which axis to split on. 
Which axis to split on. A negative value means counting dimensions from the back. Accepted range is [-rank, rank-1] where r = rank(input).
Which axis to split on. A negative value means counting dimensions from the back. Accepted range is [-rank, rank-1].
which does not equal the specified override of 
while parsing 
Whlx@
WhP'?
WhPs<
wHPSV
Whxs<
WhXs<
Whxs<
WhXs<
Whxs<
Wi]C:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorGather.cpp
WideCharToMultiByte
width_scale
WilError_03
window
Windows.Foundation.Collections.IIterator`1<Windows.Foundation.Collections.IKeyValuePair`2<String, UInt32>>
Windows.Foundation.Collections.IKeyValuePair`2<String, UInt32>
Windows.Foundation.Collections.IMap`2<String, UInt32>
Windows::AI::MachineLearning::Adapter::SessionRegisterCustomRegistry
WindowsCreateString
WindowsDeleteString
WindowsDeleteStringBuffer
WindowsDuplicateString
WindowsGetStringRawBuffer
WindowSize
WindowsPreallocateStringBuffer
WindowsPromoteStringBuffer
WinmlRuleTransformer
with a fixed dimension size 
with activation 
Wj<Y+
WjpZjoY
Word embedding shape not expected.
word_embedding
word_embedding and position_embedding shall have same dimension 1
word_embedding and segment_embedding shall have same dimension 1
word_embedding is expected to have 2 dimensions, got 
word_embedding should have 2 dimensions and dimension size is known.
WordConvEmbedding
WQh(j?
WQh|#?
WQPQQV
WQQh0
WQQhti?
WQQSQ
WQQWRQh
Writing profiler data to file 
wrong protocol type
WRQQj
WRRWQQh
WRWWRQQh|#?
WShL_@
WSj?Z
WSSSQQh
WSSSRQ
WSSSRQh
WSSSRQPSWSSWRQh
WSSSWQQh
WSSSWRQh
WSSWh 
WSSWhL
WSSWQQh
WSSWQQh0
WSSWRQh
WSSWRQh 
WSSWRQPj
wstr != wconv_error
WSWWSQQh
WVhx9I
WVSQQ
WVWVP
WWQQQRQh
WWRRj
WWRRWQQh
WWSSj
WWSSSQQh
WWSSSQQh<
WWSSSRQh
WWSSSRQhx
WWSSWh,
WWSSWhL
WWSSWQQh
WWSSWRQh
WWSSWRQQj
wX;wXtR
X != nullptr
X and mask should have the same shape
X C;}
X dims is empty.
X input is required!
X num_dims does not match W num_dims.
x#;N(}
X[_^]
X_alpha
X_Div
X_Exp
X_Log
X_Max
X_Max_Adjusted
X_Min
X_Min_Adjusted
x_original = (x_resized + 0.5) / scale - 0.5, <br/>
x_original = (x_resized + 0.5) / scale, <br/>
x_original = length_resized > 1 ? (x_resized + 0.5) / scale - 0.5 : 0, <br/>
x_original = length_resized > 1 ? start_x * (length_original - 1) + x_resized * (end_x - start_x) * (length_original - 1) / (length_resized - 1) : 0.5 * (start_x + end_x) * (length_original - 1).
x_original = x_resized * (length_original - 1) / (length_resized - 1), <br/>
x_original = x_resized / scale, <br/>
x_ptr != nullptr
X_Range
X_ReduceMax
X_ReduceSum
X_scale
x_scale
X_squared
X_Sub
X_variance
X_zero_point
x_zero_point
x_zero_point must be null or 1D tensor with size 
x_zero_point must be null or a scalar or 1D tensor or size 1.
X|+Xx
x|+xx
X->Shape().GetDims().size() == output_dims.size()
X->Shape().NumDimensions() == 4
XPh\?>
Y != nullptr
Y = softmax(scores + bias)) with simple broadcast on bias. Intended to specialize softmax(scores + additive_mask) commonly found in transformer models.
y*;7r
Y_^[]
Y__^[
y_scale
Y_scale
y_zero_point
Y_zero_point
y|+yx
Y0c1}1
Y8^ t
yC:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMaxUnpool.cpp
YD$4f
yet this opset 
yGF;u
Yj h@
Yj!h0
YPh I<
YPh(c<
YPh(i<
YPh(N<
YPh@A<
YPh\}<
YPh`=<
YPh`j<
YPh<}<
YPh0o<
YPh8i<
YPh8N<
YPhh]<
YPhX]<
YPhxc<
YPhXZ<
YQRRRh,
YSh,1>
YSQWQ
YVWRP
YY_^[
YY_^]
YY9~,t
YY90t
YY98u
YY98uS
YYh Q3
YYPSj
YYPVj
YYSC3
YYVQP
YYW;^
Z9T$,W
Zanabazar_Square
zero_point == nullptr || std::all_of(zero_point, zero_point + x_zero_point->Shape().Size(), [](int32_t zp) { return zp == 0; })
zero_point_ptr == nullptr || (zero_point_ptr->Shape().NumDimensions() == 1 && zero_point_ptr->Shape()[0] == broadcast_dim)
zero_point_ptr == nullptr || IsScalarOr1ElementVector(zero_point_ptr)
Zeropoint
ZeroPointTensor
zeros
ZipMap
Zipmap does not support empty dim count
Zipmap only supports 1D or 2D input tensors
ZRQFVVQhL
ZRSPPSh
ZRSQQQVhx9?
ZvC:\apilot\agent\_work\5\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphKernelHelper.cpp
